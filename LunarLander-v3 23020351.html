<html>
<head>
<title>lunar_lander_starter.ipynb</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #a8afbe;}
.s1 { color: #a8afbe;}
.s2 { color: #2ea9aa;}
.s3 { color: #54806d;}
.s4 { color: #acac58;}
.s5 { color: #ccb86c;}
.s6 { color: #bb8f73;}
.s7 { color: #aab77c; font-weight: bold;}
.s8 { color: #4c836d; font-style: italic;}
.ls0 { height: 1px; border-width: 0; color: #4d4d4d; background-color:#4d4d4d}
.ln { color: #8d9296; font-weight: normal; font-style: normal; }
</style>
</head>
<body bgcolor="#132623">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
lunar_lander_starter.ipynb</font>
</center></td></tr></table>
<pre><a name="l1"><span class="ln">1    </span></a><span class="s0">#%% md 
<a name="l2"><span class="ln">2    </span></a># FINAL TERM REPORT 
<a name="l3"><span class="ln">3    </span></a>## Course: Foundation of Artificial Intelligence 
<a name="l4"><span class="ln">4    </span></a>--- 
<a name="l5"><span class="ln">5    </span></a>**Student Name**: Vũ Nguyên Đan 
<a name="l6"><span class="ln">6    </span></a> 
<a name="l7"><span class="ln">7    </span></a>**Student ID**: 23020351 
<a name="l8"><span class="ln">8    </span></a> 
<a name="l9"><span class="ln">9    </span></a>**Class**: K68A-AI1 <hr class="ls0"><a name="l10"><span class="ln">10   </span></a>#%% md 
<a name="l11"><span class="ln">11   </span></a># 1. Introduction - What is LunarLander? 
<a name="l12"><span class="ln">12   </span></a>**Overview** 
<a name="l13"><span class="ln">13   </span></a> 
<a name="l14"><span class="ln">14   </span></a>The LunarLander-v3 environment, part of the Gymnasium library, simulates the task of controlling a lunar lander to achieve a safe descent and landing between two designated flags on uneven terrain. The environment is widely used in reinforcement learning research as a benchmark for discrete control and trajectory optimization problems. 
<a name="l15"><span class="ln">15   </span></a> 
<a name="l16"><span class="ln">16   </span></a>**State (Observation) Space** 
<a name="l17"><span class="ln">17   </span></a> 
<a name="l18"><span class="ln">18   </span></a>The state of the environment is represented by an 8-dimensional vector, capturing the key physical attributes of the lander at each timestep: 
<a name="l19"><span class="ln">19   </span></a> 
<a name="l20"><span class="ln">20   </span></a>- Position: (x, y) coordinates of the lander relative to the landing pad 
<a name="l21"><span class="ln">21   </span></a>- Velocity: (vx, vy) linear velocities along the x and y axes 
<a name="l22"><span class="ln">22   </span></a>- Angle: Orientation of the lander (radians) 
<a name="l23"><span class="ln">23   </span></a>- Angular velocity: Rate of change of the lander's angle 
<a name="l24"><span class="ln">24   </span></a>- Leg contact indicators: Two boolean values (leg_left_contact, leg_right_contact) indicating whether each of the lander's legs is in contact with the ground 
<a name="l25"><span class="ln">25   </span></a> 
<a name="l26"><span class="ln">26   </span></a>**Action Space** 
<a name="l27"><span class="ln">27   </span></a> 
<a name="l28"><span class="ln">28   </span></a>The agent interacts with the environment via a discrete action space of four possible actions: 
<a name="l29"><span class="ln">29   </span></a> 
<a name="l30"><span class="ln">30   </span></a>- 0: Do nothing 
<a name="l31"><span class="ln">31   </span></a>- 1: Fire left orientation engine (rotates lander right) 
<a name="l32"><span class="ln">32   </span></a>- 2: Fire main engine (provides upward thrust) 
<a name="l33"><span class="ln">33   </span></a>- 3: Fire right orientation engine (rotates lander left) 
<a name="l34"><span class="ln">34   </span></a> 
<a name="l35"><span class="ln">35   </span></a>**Reward Structure** 
<a name="l36"><span class="ln">36   </span></a> 
<a name="l37"><span class="ln">37   </span></a>The reward function is carefully designed to incentivize safe, efficient, and precise landings: 
<a name="l38"><span class="ln">38   </span></a> 
<a name="l39"><span class="ln">39   </span></a>- Successful landing between the flags with low speed: +100 to +140 
<a name="l40"><span class="ln">40   </span></a>- Crash landing: -100 
<a name="l41"><span class="ln">41   </span></a>- Moving out of bounds: -100 
<a name="l42"><span class="ln">42   </span></a>- Each timestep: -0.03 (to encourage faster completion) 
<a name="l43"><span class="ln">43   </span></a>- Firing main engine: -0.3 per frame (penalizes fuel usage) 
<a name="l44"><span class="ln">44   </span></a>- Firing side engines: -0.03 per frame 
<a name="l45"><span class="ln">45   </span></a>- Each leg in contact with ground: +10 per leg 
<a name="l46"><span class="ln">46   </span></a> 
<a name="l47"><span class="ln">47   </span></a>The total episode reward is the sum of per-step rewards, with significant bonuses or penalties for terminal events. An episode is considered solved if the agent achieves a reward of at least 200 points. 
<a name="l48"><span class="ln">48   </span></a> 
<a name="l49"><span class="ln">49   </span></a>**Training Process** 
<a name="l50"><span class="ln">50   </span></a> 
<a name="l51"><span class="ln">51   </span></a>During training, the agent interacts with the environment over many episodes. At each timestep, it observes the current state, selects an action, receives a reward, and transitions to the next state. The collected experience is used to update the agent's policy, typically via reinforcement learning algorithms such as DQN or PPO. 
<a name="l52"><span class="ln">52   </span></a> 
<a name="l53"><span class="ln">53   </span></a>**Performance Monitoring** 
<a name="l54"><span class="ln">54   </span></a> 
<a name="l55"><span class="ln">55   </span></a>Throughout training, the cumulative reward per episode is recorded to track the agent's learning progress. This metric provides insight into whether the agent is improving its landing strategy over time. 
<a name="l56"><span class="ln">56   </span></a> 
<a name="l57"><span class="ln">57   </span></a>**Episode Termination** 
<a name="l58"><span class="ln">58   </span></a> 
<a name="l59"><span class="ln">59   </span></a>An episode ends when one of the following conditions is met: 
<a name="l60"><span class="ln">60   </span></a> 
<a name="l61"><span class="ln">61   </span></a>- The lander crashes (body contacts the terrain outside the legs) 
<a name="l62"><span class="ln">62   </span></a>- The lander moves out of the visible area 
<a name="l63"><span class="ln">63   </span></a>- The lander comes to rest (no movement or collisions) 
<a name="l64"><span class="ln">64   </span></a> 
<a name="l65"><span class="ln">65   </span></a>**Environment Customization** 
<a name="l66"><span class="ln">66   </span></a> 
<a name="l67"><span class="ln">67   </span></a>LunarLander-v3 supports optional parameters such as gravity, wind, and turbulence, allowing for varied difficulty and dynamics. The default configuration uses standard lunar gravity and no wind. 
<a name="l68"><span class="ln">68   </span></a> 
<a name="l69"><span class="ln">69   </span></a>This environment serves as a rigorous testbed for discrete control, reward shaping, and policy learning in reinforcement learning research. 
<a name="l70"><span class="ln">70   </span></a> <hr class="ls0"><a name="l71"><span class="ln">71   </span></a>#%% md 
<a name="l72"><span class="ln">72   </span></a># 2. Prerequisites 
<a name="l73"><span class="ln">73   </span></a>### basically just a lot of imports and installs 
<a name="l74"><span class="ln">74   </span></a> 
<a name="l75"><span class="ln">75   </span></a>### Note: You must have SWIG installed and added to env var (in PATH), otherwise this error will show up: 
<a name="l76"><span class="ln">76   </span></a>``` 
<a name="l77"><span class="ln">77   </span></a> ERROR: Failed building wheel for box2d-py 
<a name="l78"><span class="ln">78   </span></a> ERROR: Failed to build installable wheels for some pyproject.toml based projects (box2d-py) 
<a name="l79"><span class="ln">79   </span></a>``` <hr class="ls0"><a name="l80"><span class="ln">80   </span></a>#%% 
<a name="l81"><span class="ln">81   </span></a></span><span class="s1">!</span><span class="s0">pip install gymnasium[box2d] torch stable-baselines3 matplotlib pygame opencv-python pandas py-cpuinfo</span>
<a name="l82"><span class="ln">82   </span></a><span class="s1">!</span><span class="s0">pip install tqdm</span>
<a name="l83"><span class="ln">83   </span></a><span class="s1">!</span><span class="s0">pip install tqdm ipywidgets</span>
<a name="l84"><span class="ln">84   </span></a><span class="s1">!</span><span class="s0">pip install stable-baselines3[extra]</span><hr class="ls0"><a name="l85"><span class="ln">85   </span></a><span class="s0">#%% 
<a name="l86"><span class="ln">86   </span></a></span><span class="s2">import </span><span class="s0">torch</span>
<a name="l87"><span class="ln">87   </span></a><span class="s2">import </span><span class="s0">os</span>
<a name="l88"><span class="ln">88   </span></a><span class="s2">import </span><span class="s0">torch.nn </span><span class="s2">as </span><span class="s0">nn</span>
<a name="l89"><span class="ln">89   </span></a><span class="s2">import </span><span class="s0">torch.optim </span><span class="s2">as </span><span class="s0">optim</span>
<a name="l90"><span class="ln">90   </span></a><span class="s2">import </span><span class="s0">numpy </span><span class="s2">as </span><span class="s0">np</span>
<a name="l91"><span class="ln">91   </span></a><span class="s2">import </span><span class="s0">random</span>
<a name="l92"><span class="ln">92   </span></a><span class="s2">import </span><span class="s0">platform</span>
<a name="l93"><span class="ln">93   </span></a><span class="s2">import </span><span class="s0">cpuinfo</span>
<a name="l94"><span class="ln">94   </span></a><span class="s2">from </span><span class="s0">collections </span><span class="s2">import </span><span class="s0">deque</span>
<a name="l95"><span class="ln">95   </span></a><span class="s2">import </span><span class="s0">matplotlib.pyplot </span><span class="s2">as </span><span class="s0">plt</span>
<a name="l96"><span class="ln">96   </span></a><span class="s2">import </span><span class="s0">pandas </span><span class="s2">as </span><span class="s0">pd</span>
<a name="l97"><span class="ln">97   </span></a><span class="s2">from </span><span class="s0">tqdm </span><span class="s2">import </span><span class="s0">trange</span>
<a name="l98"><span class="ln">98   </span></a><span class="s2">from </span><span class="s0">stable_baselines3.common.callbacks </span><span class="s2">import </span><span class="s0">EvalCallback</span>
<a name="l99"><span class="ln">99   </span></a><span class="s2">import </span><span class="s0">gymnasium </span><span class="s2">as </span><span class="s0">gym</span>
<a name="l100"><span class="ln">100  </span></a><span class="s3">#</span>
<a name="l101"><span class="ln">101  </span></a><span class="s2">from </span><span class="s0">stable_baselines3.common.env_util </span><span class="s2">import </span><span class="s0">make_vec_env</span>
<a name="l102"><span class="ln">102  </span></a><span class="s2">from </span><span class="s0">stable_baselines3 </span><span class="s2">import </span><span class="s0">PPO</span>
<a name="l103"><span class="ln">103  </span></a><span class="s2">from </span><span class="s0">stable_baselines3.common.evaluation </span><span class="s2">import </span><span class="s0">evaluate_policy</span>
<a name="l104"><span class="ln">104  </span></a><span class="s2">from </span><span class="s0">stable_baselines3.common.monitor </span><span class="s2">import </span><span class="s0">Monitor</span>
<a name="l105"><span class="ln">105  </span></a><span class="s3">#</span>
<a name="l106"><span class="ln">106  </span></a><span class="s2">import </span><span class="s0">matplotlib.animation </span><span class="s2">as </span><span class="s0">animation</span>
<a name="l107"><span class="ln">107  </span></a><span class="s2">from </span><span class="s0">IPython.display </span><span class="s2">import </span><span class="s0">HTML</span><hr class="ls0"><a name="l108"><span class="ln">108  </span></a><span class="s0">#%% md 
<a name="l109"><span class="ln">109  </span></a>## Hardware checks 
<a name="l110"><span class="ln">110  </span></a>This step was added in hindsight, I actually spent 4 days training the DQN Agent using my (very old) CPU instead of GPU. 
<a name="l111"><span class="ln">111  </span></a> 
<a name="l112"><span class="ln">112  </span></a>My system's Python interpreter was Python 3.10, but PyCharm used a different interpreter instead (a virtual environment Python 3.12), which lead to the CUDA support not being detected. <hr class="ls0"><a name="l113"><span class="ln">113  </span></a>#%% 
<a name="l114"><span class="ln">114  </span></a>print(</span><span class="s4">&quot;CUDA available:&quot;</span><span class="s5">, </span><span class="s0">torch.cuda.is_available())</span>
<a name="l115"><span class="ln">115  </span></a><span class="s2">if </span><span class="s0">torch.cuda.is_available():</span>
<a name="l116"><span class="ln">116  </span></a>    <span class="s0">print(</span><span class="s4">&quot;Device name:&quot;</span><span class="s5">, </span><span class="s0">torch.cuda.get_device_name(</span><span class="s6">0</span><span class="s0">))</span>
<a name="l117"><span class="ln">117  </span></a>    <span class="s0">print(</span><span class="s4">&quot;Current device index:&quot;</span><span class="s5">, </span><span class="s0">torch.cuda.current_device())</span>
<a name="l118"><span class="ln">118  </span></a>    <span class="s0">print(</span><span class="s4">&quot;Device properties:&quot;</span><span class="s5">, </span><span class="s0">torch.cuda.get_device_properties(</span><span class="s6">0</span><span class="s0">))</span>
<a name="l119"><span class="ln">119  </span></a>
<a name="l120"><span class="ln">120  </span></a><span class="s3"># MOVE ALL TENSORS AND MODELS TO GPU - DO NOT USE CPU FOR DQN</span>
<a name="l121"><span class="ln">121  </span></a><span class="s0">device = torch.device(</span><span class="s4">&quot;cuda&quot; </span><span class="s2">if </span><span class="s0">torch.cuda.is_available() </span><span class="s2">else </span><span class="s4">&quot;cpu&quot;</span><span class="s0">)</span>
<a name="l122"><span class="ln">122  </span></a>
<a name="l123"><span class="ln">123  </span></a><span class="s3"># CPU check</span>
<a name="l124"><span class="ln">124  </span></a><span class="s0">print(</span><span class="s4">&quot;</span><span class="s7">\n</span><span class="s4">CPU info&quot;</span><span class="s0">)</span>
<a name="l125"><span class="ln">125  </span></a><span class="s0">info = cpuinfo.get_cpu_info()</span>
<a name="l126"><span class="ln">126  </span></a><span class="s0">print(</span><span class="s4">f&quot;Processor: </span><span class="s7">{</span><span class="s0">info[</span><span class="s4">'brand_raw'</span><span class="s0">]</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l127"><span class="ln">127  </span></a><span class="s0">print(</span><span class="s4">f&quot;Machine: </span><span class="s7">{</span><span class="s0">platform.machine()</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l128"><span class="ln">128  </span></a><span class="s0">print(</span><span class="s4">f&quot;Platform: </span><span class="s7">{</span><span class="s0">platform.platform()</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l129"><span class="ln">129  </span></a><span class="s0">print(</span><span class="s4">f&quot;CPU cores: </span><span class="s7">{</span><span class="s0">os.cpu_count()</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span><hr class="ls0"><a name="l130"><span class="ln">130  </span></a><span class="s0">#%% md 
<a name="l131"><span class="ln">131  </span></a># 3. Reinforcement Learning Agent Deployment and Algorithm Selection 
<a name="l132"><span class="ln">132  </span></a>Two main reinforcement learning algorithms were selected and implemented to solve the LunarLander problem: Double Deep Q-Network (Double DQN, or DDQN) and Proximal Policy Optimization (PPO). The choice of these two algorithms, which belong to different approaches, allows for a comprehensive evaluation. DQN is a value-based, off-policy method, while PPO is a policy-based, actor-critic approach that operates on-policy. This setup enables a comparative analysis of their learning effectiveness and unique characteristics when applied to the same problem. 
<a name="l133"><span class="ln">133  </span></a> 
<a name="l134"><span class="ln">134  </span></a>### Quick comparison between DDQN and PPO 
<a name="l135"><span class="ln">135  </span></a> 
<a name="l136"><span class="ln">136  </span></a>| Aspect | Double DQN | PPO | 
<a name="l137"><span class="ln">137  </span></a>|--------|------------|-----| 
<a name="l138"><span class="ln">138  </span></a>| **Type** | Value-based, Off-policy | Policy-based, On-policy | 
<a name="l139"><span class="ln">139  </span></a>| **Action Space** | Discrete only | Discrete + Continuous | 
<a name="l140"><span class="ln">140  </span></a>| **Sample Efficiency** | High (replay buffer) | Moderate (multiple epochs) | 
<a name="l141"><span class="ln">141  </span></a>| **Stability** | Moderate (target network) | High (clipped updates) | 
<a name="l142"><span class="ln">142  </span></a>| **Exploration** | ε-greedy | Stochastic policy | 
<a name="l143"><span class="ln">143  </span></a>| **Memory Requirements** | High (replay buffer) | Low (batch only) | 
<a name="l144"><span class="ln">144  </span></a> 
<a name="l145"><span class="ln">145  </span></a>## 3.1 Code Structure and Components 
<a name="l146"><span class="ln">146  </span></a> 
<a name="l147"><span class="ln">147  </span></a>### 3.1.1 DQN Network Architecture 
<a name="l148"><span class="ln">148  </span></a> 
<a name="l149"><span class="ln">149  </span></a>The **DQN** class implements a deep Q-network with a carefully designed architecture optimized for LunarLander's 8-dimensional state space. The network uses a progressive dimensionality reduction pattern (256 -&gt; 128 -&gt; 64) with layer normalization after each linear transformation to stabilize training. Key design features include: 
<a name="l150"><span class="ln">150  </span></a> 
<a name="l151"><span class="ln">151  </span></a>- **Input Processing**: Maps 8D state vector (position, velocity, angle, leg contacts) to 256 features 
<a name="l152"><span class="ln">152  </span></a>- **Feature Hierarchy**: Progressive reduction creates hierarchical learning from low-level state combinations to high-level decision features 
<a name="l153"><span class="ln">153  </span></a>- **Stabilization**: Layer normalization reduces internal covariate shift and enables higher learning rates 
<a name="l154"><span class="ln">154  </span></a>- **Conservative Initialization**: Xavier normal with gain=0.1 and zero bias initialization prevents gradient instabilities common in RL 
<a name="l155"><span class="ln">155  </span></a> 
<a name="l156"><span class="ln">156  </span></a>### 3.1.2 DDQN Agent Implementation 
<a name="l157"><span class="ln">157  </span></a> 
<a name="l158"><span class="ln">158  </span></a>The **DoubleDQNAgent** class encapsulates the Double DQN algorithm with several stabilization enhancements: 
<a name="l159"><span class="ln">159  </span></a> 
<a name="l160"><span class="ln">160  </span></a>- **Dual Network Structure**: Online and target networks with soft updates (τ=0.001) 
<a name="l161"><span class="ln">161  </span></a>- **Overestimation Mitigation**: Decouples action selection (online network) from evaluation (target network) 
<a name="l162"><span class="ln">162  </span></a>- **Exploration Strategy**: $\epsilon$-greedy with decay (1.0 -&gt; 0.1) and extended warmup phase 
<a name="l163"><span class="ln">163  </span></a>- **Training Stabilization**: Reward normalization, gradient clipping (max norm 1.0), and conservative hyperparameters 
<a name="l164"><span class="ln">164  </span></a> 
<a name="l165"><span class="ln">165  </span></a>### 3.1.3 Actor-Critic Network (PPO) 
<a name="l166"><span class="ln">166  </span></a> 
<a name="l167"><span class="ln">167  </span></a>The **PPO** implementation uses Stable-Baselines3's MlpPolicy with separate actor and critic networks. The architecture features: 
<a name="l168"><span class="ln">168  </span></a> 
<a name="l169"><span class="ln">169  </span></a>- **Actor Network**:  neurons for policy function π(a|s) mapping states to action probabilities 
<a name="l170"><span class="ln">170  </span></a>- **Critic Network**:  neurons for value function V(s) estimating state values 
<a name="l171"><span class="ln">171  </span></a>- **Shared Features**: Lower layers shared between networks for parameter efficiency 
<a name="l172"><span class="ln">172  </span></a>- **Vectorized Training**: 4 parallel environments for improved sample collection and reduced gradient variance 
<a name="l173"><span class="ln">173  </span></a> 
<a name="l174"><span class="ln">174  </span></a>### 3.1.4 PPO Agent Configuration 
<a name="l175"><span class="ln">175  </span></a> 
<a name="l176"><span class="ln">176  </span></a>The PPO agent leverages **policy gradient methods** with clipped surrogate objectives: 
<a name="l177"><span class="ln">177  </span></a> 
<a name="l178"><span class="ln">178  </span></a>- **On-Policy Learning**: Collects trajectories with current policy for 2048 steps across 4 environments 
<a name="l179"><span class="ln">179  </span></a>- **Clipped Updates**: 0.2 clip range prevents destructive policy changes 
<a name="l180"><span class="ln">180  </span></a>- **GAE Implementation**: Generalized Advantage Estimation (λ=0.95) for bias-variance balance 
<a name="l181"><span class="ln">181  </span></a>- **Multi-Epoch Training**: Reuses collected data for multiple gradient steps per update cycle 
<a name="l182"><span class="ln">182  </span></a> 
<a name="l183"><span class="ln">183  </span></a>### 3.1.5 Buffer Mechanisms 
<a name="l184"><span class="ln">184  </span></a> 
<a name="l185"><span class="ln">185  </span></a>Multiple replay buffer implementations support different learning paradigms: 
<a name="l186"><span class="ln">186  </span></a> 
<a name="l187"><span class="ln">187  </span></a>- **Standard Replay Buffer**: Deque-based storage (100000 capacity) for DDQN with uniform sampling 
<a name="l188"><span class="ln">188  </span></a>- **N-Step Returns**: 3-step lookahead implementation for improved temporal credit assignment 
<a name="l189"><span class="ln">189  </span></a>- **Prioritized Experience Replay**: Attempted but discarded due to 44x training slowdown (1s -&gt; 44s per iteration) 
<a name="l190"><span class="ln">190  </span></a> 
<a name="l191"><span class="ln">191  </span></a>### 3.1.6 Training Functions 
<a name="l192"><span class="ln">192  </span></a> 
<a name="l193"><span class="ln">193  </span></a>Comprehensive training pipelines with robust monitoring and control mechanisms: 
<a name="l194"><span class="ln">194  </span></a> 
<a name="l195"><span class="ln">195  </span></a>- **Early Stopping**: 50 consecutive episodes above 200 reward threshold prevents overtraining 
<a name="l196"><span class="ln">196  </span></a>- **Warmup Strategy**: 300 episodes of forced exploration followed by accelerated epsilon decay 
<a name="l197"><span class="ln">197  </span></a>- **Checkpointing**: Regular model saves every 100 episodes with best performance tracking 
<a name="l198"><span class="ln">198  </span></a>- **Performance Monitoring**: Real-time logging of loss, gradient norms, Q-values, and exploration rates 
<a name="l199"><span class="ln">199  </span></a> 
<a name="l200"><span class="ln">200  </span></a>### 3.1.7 Evaluation and Visualization 
<a name="l201"><span class="ln">201  </span></a> 
<a name="l202"><span class="ln">202  </span></a>Multi-faceted evaluation framework for comprehensive performance analysis: 
<a name="l203"><span class="ln">203  </span></a> 
<a name="l204"><span class="ln">204  </span></a>- **Deterministic Testing**: Epsilon=0.0 for DQN, deterministic=True for PPO eliminates exploration noise 
<a name="l205"><span class="ln">205  </span></a>- **Performance Metrics**: Success rate, fuel efficiency, landing precision, and crash detection 
<a name="l206"><span class="ln">206  </span></a>- **Statistical Analysis**: Multiple evaluation sessions (10 runs x 100 episodes) for variance assessment 
<a name="l207"><span class="ln">207  </span></a>- **Video Generation**: RGB array capture with matplotlib animation for visual policy inspection 
<a name="l208"><span class="ln">208  </span></a>- **Comparative Analytics**: Side-by-side algorithm comparison with comprehensive plotting utilities 
<a name="l209"><span class="ln">209  </span></a> 
<a name="l210"><span class="ln">210  </span></a>The modular architecture enables independent testing and modification of components while maintaining clear separation of concerns between value-based (DDQN) and policy-based (PPO) approaches. <hr class="ls0"><a name="l211"><span class="ln">211  </span></a>#%% md 
<a name="l212"><span class="ln">212  </span></a># 4. Double DQN (Value-based approach) 
<a name="l213"><span class="ln">213  </span></a>## 4.1 What is Double DQN? 
<a name="l214"><span class="ln">214  </span></a> 
<a name="l215"><span class="ln">215  </span></a>### Standard DQN Problem 
<a name="l216"><span class="ln">216  </span></a>Standard DQN suffers from overestimation bias due to the max operator: 
<a name="l217"><span class="ln">217  </span></a> 
<a name="l218"><span class="ln">218  </span></a>$$Q_{\text{target}}^{\text{DQN}} = r + \gamma \max_{a'} Q(s', a'; \theta^-)$$ 
<a name="l219"><span class="ln">219  </span></a> 
<a name="l220"><span class="ln">220  </span></a>This leads to systematic overestimation because: 
<a name="l221"><span class="ln">221  </span></a>$$\mathbb{E}[\max(X_1, X_2, ..., X_n)] \geq \max(\mathbb{E}[X_1], \mathbb{E}[X_2], ..., \mathbb{E}[X_n])$$ 
<a name="l222"><span class="ln">222  </span></a> 
<a name="l223"><span class="ln">223  </span></a>### Double DQN Solution 
<a name="l224"><span class="ln">224  </span></a>Double DQN decouples action selection and evaluation: 
<a name="l225"><span class="ln">225  </span></a> 
<a name="l226"><span class="ln">226  </span></a>1. **Action Selection** (using online network): 
<a name="l227"><span class="ln">227  </span></a>$$a^* = \arg\max_{a'} Q(s', a'; \theta)$$ 
<a name="l228"><span class="ln">228  </span></a> 
<a name="l229"><span class="ln">229  </span></a>2. **Action Evaluation** (using target network): 
<a name="l230"><span class="ln">230  </span></a>$$Q_{\text{target}}^{\text{DDQN}} = r + \gamma Q(s', a^*; \theta^-)$$ 
<a name="l231"><span class="ln">231  </span></a> 
<a name="l232"><span class="ln">232  </span></a>3. **Loss Function**: 
<a name="l233"><span class="ln">233  </span></a>$$L(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \left[ \left( Q(s,a;\theta) - Q_{\text{target}}^{\text{DDQN}} \right)^2 \right]$$ 
<a name="l234"><span class="ln">234  </span></a> 
<a name="l235"><span class="ln">235  </span></a>## 4.2 Why did I choose DDQN for LunarLander? 
<a name="l236"><span class="ln">236  </span></a>####  Because: 
<a name="l237"><span class="ln">237  </span></a>- **Discrete action space**: LunarLander has 4 discrete actions, perfect for Q-learning 
<a name="l238"><span class="ln">238  </span></a>- **Overestimation mitigation**: Critical for precise landing control 
<a name="l239"><span class="ln">239  </span></a>- **Sample efficiency**: Better than vanilla DQN with same computational cost 
<a name="l240"><span class="ln">240  </span></a> <hr class="ls0"><a name="l241"><span class="ln">241  </span></a>#%% md 
<a name="l242"><span class="ln">242  </span></a>## 4.3 Network Architecture 
<a name="l243"><span class="ln">243  </span></a> 
<a name="l244"><span class="ln">244  </span></a>### Architecture Design Rationale 
<a name="l245"><span class="ln">245  </span></a> 
<a name="l246"><span class="ln">246  </span></a>The DQN network architecture is specifically designed for the LunarLander-v3 environment's 8-dimensional continuous state space and 4-action discrete action space. 
<a name="l247"><span class="ln">247  </span></a> 
<a name="l248"><span class="ln">248  </span></a>### Layer-by-Layer Analysis 
<a name="l249"><span class="ln">249  </span></a> 
<a name="l250"><span class="ln">250  </span></a>**Input layer (8 -&gt; 256)** 
<a name="l251"><span class="ln">251  </span></a>- Maps the 8-dimensional state vector to 256 features 
<a name="l252"><span class="ln">252  </span></a>- Expansion from compact state representation to rich feature space 
<a name="l253"><span class="ln">253  </span></a>- Input includes: position (x, y), velocity (vx, vy), angle, angular velocity, leg contacts 
<a name="l254"><span class="ln">254  </span></a> 
<a name="l255"><span class="ln">255  </span></a>**Hidden layers (256 -&gt; 128 -&gt; 64)** 
<a name="l256"><span class="ln">256  </span></a>- Progressive dimensionality reduction creates hierarchical feature learning 
<a name="l257"><span class="ln">257  </span></a>- 256 units: Capture low-level state combinations and feature extraction 
<a name="l258"><span class="ln">258  </span></a>- 128 units: Learn intermediate behavioral patterns and state abstractions 
<a name="l259"><span class="ln">259  </span></a>- 64 units: Extract high-level decision features for action selection 
<a name="l260"><span class="ln">260  </span></a> 
<a name="l261"><span class="ln">261  </span></a>**Layer normalization** 
<a name="l262"><span class="ln">262  </span></a>- Applied after each linear transformation (before ReLU activation) 
<a name="l263"><span class="ln">263  </span></a>- Stabilizes training by normalizing activations across the feature dimension 
<a name="l264"><span class="ln">264  </span></a>- Reduces internal covariate shift, enabling higher learning rates 
<a name="l265"><span class="ln">265  </span></a>- Critical for deep RL where gradient magnitudes can vary significantly between episodes 
<a name="l266"><span class="ln">266  </span></a> 
<a name="l267"><span class="ln">267  </span></a>**ReLU activation** 
<a name="l268"><span class="ln">268  </span></a>- Applied after each LayerNorm (except final layer) 
<a name="l269"><span class="ln">269  </span></a>- Introduces non-linearity while maintaining computational efficiency 
<a name="l270"><span class="ln">270  </span></a>- Prevents vanishing gradient problem in deep networks 
<a name="l271"><span class="ln">271  </span></a>- Enables learning of complex, non-linear Q-value functions 
<a name="l272"><span class="ln">272  </span></a> 
<a name="l273"><span class="ln">273  </span></a>**Output layer (64 -&gt; 4)** 
<a name="l274"><span class="ln">274  </span></a>- Produces Q-values for each of the 4 discrete actions 
<a name="l275"><span class="ln">275  </span></a>- No activation function - raw Q-values can be positive or negative 
<a name="l276"><span class="ln">276  </span></a>- Direct mapping from learned features to action values 
<a name="l277"><span class="ln">277  </span></a> 
<a name="l278"><span class="ln">278  </span></a>### Weight Initialization Strategy 
<a name="l279"><span class="ln">279  </span></a>#### Conservative initialization 
<a name="l280"><span class="ln">280  </span></a> 
<a name="l281"><span class="ln">281  </span></a>``` 
<a name="l282"><span class="ln">282  </span></a>for layer in self.net: 
<a name="l283"><span class="ln">283  </span></a>    if isinstance(layer, nn.Linear): 
<a name="l284"><span class="ln">284  </span></a>        nn.init.xavier_normal_(layer.weight, gain=0.1) 
<a name="l285"><span class="ln">285  </span></a>        nn.init.constant_(layer.bias, 0.0) 
<a name="l286"><span class="ln">286  </span></a>``` 
<a name="l287"><span class="ln">287  </span></a> 
<a name="l288"><span class="ln">288  </span></a>**Xavier Normal Initialization** 
<a name="l289"><span class="ln">289  </span></a>- Maintains variance of activations across layers 
<a name="l290"><span class="ln">290  </span></a>- Prevents exploding/vanishing gradients during early training 
<a name="l291"><span class="ln">291  </span></a>- Gain=0.1: Conservative scaling reduces initial Q-value magnitudes 
<a name="l292"><span class="ln">292  </span></a> 
<a name="l293"><span class="ln">293  </span></a>**Zero Bias Initialization** 
<a name="l294"><span class="ln">294  </span></a>- Ensures symmetric initial behavior 
<a name="l295"><span class="ln">295  </span></a>- Prevents systematic bias toward specific actions 
<a name="l296"><span class="ln">296  </span></a>- Allows exploration to drive initial learning 
<a name="l297"><span class="ln">297  </span></a> 
<a name="l298"><span class="ln">298  </span></a>### Architecture Benefits for LunarLander 
<a name="l299"><span class="ln">299  </span></a> 
<a name="l300"><span class="ln">300  </span></a>**Sufficient Capacity**: 256 -&gt; 128 -&gt; 64 provides approximately 45K parameters to learn complex landing strategies without overfitting 
<a name="l301"><span class="ln">301  </span></a> 
<a name="l302"><span class="ln">302  </span></a>**Stability**: Layer normalization after each linear transformation and conservative initialization prevent training instabilities common in RL 
<a name="l303"><span class="ln">303  </span></a> 
<a name="l304"><span class="ln">304  </span></a>**Efficiency**: Moderate size enables fast forward passes for real-time decision making 
<a name="l305"><span class="ln">305  </span></a> 
<a name="l306"><span class="ln">306  </span></a>**Generalization**: Architecture can handle varying landing scenarios and environmental conditions through hierarchical feature extraction <hr class="ls0"><a name="l307"><span class="ln">307  </span></a>#%% md 
<a name="l308"><span class="ln">308  </span></a>## DQN v2 network - Canceled due to inability to converge after 1000 eps, while  v1 converged after just 850 eps with similar hyperparameters 
<a name="l309"><span class="ln">309  </span></a>### Possible reasons why this v2 network failed: 
<a name="l310"><span class="ln">310  </span></a> 
<a name="l311"><span class="ln">311  </span></a>#### 1. Over-parameterization for LunarLander 
<a name="l312"><span class="ln">312  </span></a> 
<a name="l313"><span class="ln">313  </span></a>v1: 256 -&gt; 128 -&gt; 64 = ~67K parameters 
<a name="l314"><span class="ln">314  </span></a> 
<a name="l315"><span class="ln">315  </span></a>v2: 512 -&gt; 256 -&gt; 128 = ~200K parameters 
<a name="l316"><span class="ln">316  </span></a> 
<a name="l317"><span class="ln">317  </span></a>LunarLander's 8D state space surely doesn't need this level of complexity 
<a name="l318"><span class="ln">318  </span></a> 
<a name="l319"><span class="ln">319  </span></a>#### 2. Initialization Issues 
<a name="l320"><span class="ln">320  </span></a> 
<a name="l321"><span class="ln">321  </span></a>Kaiming initialization with large networks can cause gradient instability 
<a name="l322"><span class="ln">322  </span></a> 
<a name="l323"><span class="ln">323  </span></a>0.01 bias instead of 0.0 can introduce unwanted bias in Q-values 
<a name="l324"><span class="ln">324  </span></a> 
<a name="l325"><span class="ln">325  </span></a>Conservative Xavier + gain=0.1 of the v1 was probably perfect for this problem 
<a name="l326"><span class="ln">326  </span></a> 
<a name="l327"><span class="ln">327  </span></a>#### 3. Capacity vs. Sample Efficiency Trade-off 
<a name="l328"><span class="ln">328  </span></a> 
<a name="l329"><span class="ln">329  </span></a>Larger networks need more data to train effectively 
<a name="l330"><span class="ln">330  </span></a> 
<a name="l331"><span class="ln">331  </span></a>1000 episode budget isn't enough for the 512 -&gt; 256 -&gt; 128 network (probably gonna require 3000 eps or sth) 
<a name="l332"><span class="ln">332  </span></a> 
<a name="l333"><span class="ln">333  </span></a>So, the old network reaches good performance faster... 
<a name="l334"><span class="ln">334  </span></a> 
<a name="l335"><span class="ln">335  </span></a>#### Code of DQN v2 Network 
<a name="l336"><span class="ln">336  </span></a> 
<a name="l337"><span class="ln">337  </span></a>``` 
<a name="l338"><span class="ln">338  </span></a>class DQN(nn.Module): 
<a name="l339"><span class="ln">339  </span></a>    def __init__(self, input_dim, output_dim): 
<a name="l340"><span class="ln">340  </span></a>        super().__init__() 
<a name="l341"><span class="ln">341  </span></a>        self.net = nn.Sequential( 
<a name="l342"><span class="ln">342  </span></a>            nn.Linear(input_dim, 512), 
<a name="l343"><span class="ln">343  </span></a>            nn.LayerNorm(512), 
<a name="l344"><span class="ln">344  </span></a>            nn.ReLU(), 
<a name="l345"><span class="ln">345  </span></a>            nn.Linear(512, 256), 
<a name="l346"><span class="ln">346  </span></a>            nn.LayerNorm(256), 
<a name="l347"><span class="ln">347  </span></a>            nn.ReLU(), 
<a name="l348"><span class="ln">348  </span></a>            nn.Linear(256, 128), 
<a name="l349"><span class="ln">349  </span></a>            nn.LayerNorm(128), 
<a name="l350"><span class="ln">350  </span></a>            nn.ReLU(), 
<a name="l351"><span class="ln">351  </span></a>            nn.Linear(128, output_dim) 
<a name="l352"><span class="ln">352  </span></a>        ) 
<a name="l353"><span class="ln">353  </span></a>        for layer in self.net: 
<a name="l354"><span class="ln">354  </span></a>            if isinstance(layer, nn.Linear): 
<a name="l355"><span class="ln">355  </span></a>                nn.init.kaiming_normal_(layer.weight, nonlinearity='relu') 
<a name="l356"><span class="ln">356  </span></a>                nn.init.constant_(layer.bias, 0.01) 
<a name="l357"><span class="ln">357  </span></a> 
<a name="l358"><span class="ln">358  </span></a>    def forward(self, x): 
<a name="l359"><span class="ln">359  </span></a>        return self.net(x) 
<a name="l360"><span class="ln">360  </span></a> ``` <hr class="ls0"><a name="l361"><span class="ln">361  </span></a>#%% 
<a name="l362"><span class="ln">362  </span></a></span><span class="s3"># DQN v1 Network</span>
<a name="l363"><span class="ln">363  </span></a><span class="s2">class </span><span class="s0">DQN(nn.Module):</span>
<a name="l364"><span class="ln">364  </span></a>    <span class="s2">def </span><span class="s0">__init__(self</span><span class="s5">, </span><span class="s0">input_dim</span><span class="s5">, </span><span class="s0">output_dim):</span>
<a name="l365"><span class="ln">365  </span></a>        <span class="s0">super().__init__()</span>
<a name="l366"><span class="ln">366  </span></a>        <span class="s0">self.net = nn.Sequential(</span>
<a name="l367"><span class="ln">367  </span></a>            <span class="s0">nn.Linear(input_dim</span><span class="s5">, </span><span class="s6">256</span><span class="s0">)</span><span class="s5">,</span>
<a name="l368"><span class="ln">368  </span></a>            <span class="s0">nn.LayerNorm(</span><span class="s6">256</span><span class="s0">)</span><span class="s5">,</span>
<a name="l369"><span class="ln">369  </span></a>            <span class="s0">nn.ReLU()</span><span class="s5">,</span>
<a name="l370"><span class="ln">370  </span></a>            <span class="s0">nn.Linear(</span><span class="s6">256</span><span class="s5">, </span><span class="s6">128</span><span class="s0">)</span><span class="s5">,</span>
<a name="l371"><span class="ln">371  </span></a>            <span class="s0">nn.LayerNorm(</span><span class="s6">128</span><span class="s0">)</span><span class="s5">,</span>
<a name="l372"><span class="ln">372  </span></a>            <span class="s0">nn.ReLU()</span><span class="s5">,</span>
<a name="l373"><span class="ln">373  </span></a>            <span class="s0">nn.Linear(</span><span class="s6">128</span><span class="s5">, </span><span class="s6">64</span><span class="s0">)</span><span class="s5">,</span>
<a name="l374"><span class="ln">374  </span></a>            <span class="s0">nn.LayerNorm(</span><span class="s6">64</span><span class="s0">)</span><span class="s5">,</span>
<a name="l375"><span class="ln">375  </span></a>            <span class="s0">nn.ReLU()</span><span class="s5">,</span>
<a name="l376"><span class="ln">376  </span></a>            <span class="s0">nn.Linear(</span><span class="s6">64</span><span class="s5">, </span><span class="s0">output_dim)</span>
<a name="l377"><span class="ln">377  </span></a>        <span class="s0">)</span>
<a name="l378"><span class="ln">378  </span></a>        <span class="s3"># initialization (quite conservative)</span>
<a name="l379"><span class="ln">379  </span></a>        <span class="s2">for </span><span class="s0">layer </span><span class="s2">in </span><span class="s0">self.net:</span>
<a name="l380"><span class="ln">380  </span></a>            <span class="s2">if </span><span class="s0">isinstance(layer</span><span class="s5">, </span><span class="s0">nn.Linear):</span>
<a name="l381"><span class="ln">381  </span></a>                <span class="s0">nn.init.xavier_normal_(layer.weight</span><span class="s5">, </span><span class="s0">gain=</span><span class="s6">0.1</span><span class="s0">)</span>
<a name="l382"><span class="ln">382  </span></a>                <span class="s0">nn.init.constant_(layer.bias</span><span class="s5">, </span><span class="s6">0.0</span><span class="s0">)</span>
<a name="l383"><span class="ln">383  </span></a>
<a name="l384"><span class="ln">384  </span></a>    <span class="s2">def </span><span class="s0">forward(self</span><span class="s5">, </span><span class="s0">x):</span>
<a name="l385"><span class="ln">385  </span></a>        <span class="s2">return </span><span class="s0">self.net(x)</span><hr class="ls0"><a name="l386"><span class="ln">386  </span></a><span class="s0">#%% md 
<a name="l387"><span class="ln">387  </span></a>## 4.4 Double DQN Agent Implementation 
<a name="l388"><span class="ln">388  </span></a> 
<a name="l389"><span class="ln">389  </span></a>### 4.4.1 Core Agent Architecture 
<a name="l390"><span class="ln">390  </span></a> 
<a name="l391"><span class="ln">391  </span></a>The `DoubleDQNAgent` v4 implements the Double DQN algorithm with several key enhancements for stable learning in the LunarLander environment, including **N-step returns** for improved temporal credit assignment. 
<a name="l392"><span class="ln">392  </span></a> 
<a name="l393"><span class="ln">393  </span></a>### 4.4.2 Key Hyperparameters and Their Roles 
<a name="l394"><span class="ln">394  </span></a> 
<a name="l395"><span class="ln">395  </span></a>**Learning Rate (lr=0.0001)** 
<a name="l396"><span class="ln">396  </span></a>- Conservative rate prevents destructive policy updates 
<a name="l397"><span class="ln">397  </span></a>- Balances learning speed with stability 
<a name="l398"><span class="ln">398  </span></a>- Critical for continuous state spaces where small changes can have large effects 
<a name="l399"><span class="ln">399  </span></a> 
<a name="l400"><span class="ln">400  </span></a>**Batch Size (256)** 
<a name="l401"><span class="ln">401  </span></a>- Large enough to provide stable gradient estimates 
<a name="l402"><span class="ln">402  </span></a>- Reduces variance in Q-value updates 
<a name="l403"><span class="ln">403  </span></a>- Computational efficiency on modern GPUs 
<a name="l404"><span class="ln">404  </span></a>- Optimal balance between memory usage and training stability 
<a name="l405"><span class="ln">405  </span></a> 
<a name="l406"><span class="ln">406  </span></a>**Discount Factor (γ=0.99)** 
<a name="l407"><span class="ln">407  </span></a>- High value emphasizes long-term planning 
<a name="l408"><span class="ln">408  </span></a>- Essential for landing tasks requiring multi-step coordination 
<a name="l409"><span class="ln">409  </span></a>- Balances immediate fuel costs with landing success rewards 
<a name="l410"><span class="ln">410  </span></a>- Used both for standard discounting and N-step return calculations 
<a name="l411"><span class="ln">411  </span></a> 
<a name="l412"><span class="ln">412  </span></a>**Target Network Update (τ=0.001)** 
<a name="l413"><span class="ln">413  </span></a>- Soft update rate for target network stability 
<a name="l414"><span class="ln">414  </span></a>- Gradual parameter transfer prevents oscillations 
<a name="l415"><span class="ln">415  </span></a>- Formula: `θ_target = τ * θ_online + (1-τ) * θ_target` 
<a name="l416"><span class="ln">416  </span></a> 
<a name="l417"><span class="ln">417  </span></a>**Experience Replay Buffer (100,000)** 
<a name="l418"><span class="ln">418  </span></a>- Standard deque-based storage for N-step enhanced experiences 
<a name="l419"><span class="ln">419  </span></a>- Breaks temporal correlations in training data 
<a name="l420"><span class="ln">420  </span></a>- Enables multiple learning updates from single experiences 
<a name="l421"><span class="ln">421  </span></a>- Size balances memory efficiency with experience diversity 
<a name="l422"><span class="ln">422  </span></a> 
<a name="l423"><span class="ln">423  </span></a>### 4.4.3 N-Step Returns Enhancement 
<a name="l424"><span class="ln">424  </span></a> 
<a name="l425"><span class="ln">425  </span></a>**N-Step Learning (n=3)** 
<a name="l426"><span class="ln">426  </span></a> 
<a name="l427"><span class="ln">427  </span></a>```python 
<a name="l428"><span class="ln">428  </span></a>self.n_step = 3 
<a name="l429"><span class="ln">429  </span></a>self.n_step_buffer = deque(maxlen=self.n_step) 
<a name="l430"><span class="ln">430  </span></a>``` 
<a name="l431"><span class="ln">431  </span></a> 
<a name="l432"><span class="ln">432  </span></a>**N-Step Return Calculation** 
<a name="l433"><span class="ln">433  </span></a>```python 
<a name="l434"><span class="ln">434  </span></a>n_step_reward = sum(self.gamma**i * reward for i, (_, _, reward, _, done) in enumerate(self.n_step_buffer)) 
<a name="l435"><span class="ln">435  </span></a>target_q = rewards + (1 - dones) * (self.gamma ** self.n_step) * next_q.squeeze() 
<a name="l436"><span class="ln">436  </span></a>``` 
<a name="l437"><span class="ln">437  </span></a> 
<a name="l438"><span class="ln">438  </span></a>**Benefits for LunarLander** 
<a name="l439"><span class="ln">439  </span></a>- **Faster Credit Assignment**: Rewards propagate more quickly through multi-step sequences 
<a name="l440"><span class="ln">440  </span></a>- **Improved Sample Efficiency**: Each experience update incorporates 3-step lookahead 
<a name="l441"><span class="ln">441  </span></a>- **Better Trajectory Learning**: Landing sequences benefit from extended temporal context 
<a name="l442"><span class="ln">442  </span></a>- **Reduced Bias**: Multi-step returns provide more accurate value estimates than 1-step 
<a name="l443"><span class="ln">443  </span></a> 
<a name="l444"><span class="ln">444  </span></a>### 4.4.4 Exploration Strategy 
<a name="l445"><span class="ln">445  </span></a> 
<a name="l446"><span class="ln">446  </span></a>**Epsilon-Greedy with Decay** 
<a name="l447"><span class="ln">447  </span></a> 
<a name="l448"><span class="ln">448  </span></a>```python 
<a name="l449"><span class="ln">449  </span></a>self.epsilon = 1.0 # Start with full exploration 
<a name="l450"><span class="ln">450  </span></a>self.epsilon_decay = 0.998 # Gradual reduction 
<a name="l451"><span class="ln">451  </span></a>self.epsilon_min = 0.1 # Maintain 10% exploration 
<a name="l452"><span class="ln">452  </span></a>``` 
<a name="l453"><span class="ln">453  </span></a> 
<a name="l454"><span class="ln">454  </span></a>- **Initial Exploration (ε=1.0)**: Random action selection for environment discovery 
<a name="l455"><span class="ln">455  </span></a>- **Gradual Decay (0.998)**: Smooth transition from exploration to exploitation 
<a name="l456"><span class="ln">456  </span></a>- **Minimum Exploration (0.1)**: Prevents complete exploitation, maintains adaptability 
<a name="l457"><span class="ln">457  </span></a> 
<a name="l458"><span class="ln">458  </span></a>### 4.4.5 Double DQN Update Mechanism 
<a name="l459"><span class="ln">459  </span></a> 
<a name="l460"><span class="ln">460  </span></a>**Action Selection vs. Evaluation Decoupling** 
<a name="l461"><span class="ln">461  </span></a> 
<a name="l462"><span class="ln">462  </span></a>Standard DQN (prone to overestimation): 
<a name="l463"><span class="ln">463  </span></a>`target = r + γ * max_a Q_target(s', a)` 
<a name="l464"><span class="ln">464  </span></a> 
<a name="l465"><span class="ln">465  </span></a>Double DQN with N-step (reduces overestimation): 
<a name="l466"><span class="ln">466  </span></a>```python 
<a name="l467"><span class="ln">467  </span></a>next_actions = self.q_net(next_states).argmax(dim=1, keepdim=True) # Online network selects 
<a name="l468"><span class="ln">468  </span></a>next_q = self.target_net(next_states).gather(1, next_actions) # Target network evaluates 
<a name="l469"><span class="ln">469  </span></a>target_q = rewards + (1 - dones) * (self.gamma ** self.n_step) * next_q.squeeze() 
<a name="l470"><span class="ln">470  </span></a>``` 
<a name="l471"><span class="ln">471  </span></a> 
<a name="l472"><span class="ln">472  </span></a>**How does this affect LunarLander?** 
<a name="l473"><span class="ln">473  </span></a>- Landing requires precise control where overestimated Q-values can lead to overconfident, risky actions 
<a name="l474"><span class="ln">474  </span></a>- Decoupling prevents systematic overoptimism in action selection 
<a name="l475"><span class="ln">475  </span></a>- N-step returns accelerate learning of landing sequences 
<a name="l476"><span class="ln">476  </span></a>- Results in more conservative, stable landing strategies with faster convergence 
<a name="l477"><span class="ln">477  </span></a> 
<a name="l478"><span class="ln">478  </span></a>### 4.4.6 Training Stabilization Techniques 
<a name="l479"><span class="ln">479  </span></a> 
<a name="l480"><span class="ln">480  </span></a>**Reward Normalization** 
<a name="l481"><span class="ln">481  </span></a> 
<a name="l482"><span class="ln">482  </span></a>```python 
<a name="l483"><span class="ln">483  </span></a>rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7) 
<a name="l484"><span class="ln">484  </span></a>``` 
<a name="l485"><span class="ln">485  </span></a> 
<a name="l486"><span class="ln">486  </span></a>- Normalizes reward scale across batches 
<a name="l487"><span class="ln">487  </span></a>- Prevents large reward variations from destabilizing learning 
<a name="l488"><span class="ln">488  </span></a>- Particularly important given LunarLander's wide reward range (-100 to +300) 
<a name="l489"><span class="ln">489  </span></a>- Applied to N-step accumulated rewards 
<a name="l490"><span class="ln">490  </span></a> 
<a name="l491"><span class="ln">491  </span></a>**Gradient Clipping** 
<a name="l492"><span class="ln">492  </span></a> 
<a name="l493"><span class="ln">493  </span></a>```python 
<a name="l494"><span class="ln">494  </span></a>grad_norm = torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), self.grad_clip) 
<a name="l495"><span class="ln">495  </span></a>``` 
<a name="l496"><span class="ln">496  </span></a> 
<a name="l497"><span class="ln">497  </span></a>- Prevents exploding gradients that can destroy learned policies 
<a name="l498"><span class="ln">498  </span></a>- Max norm of 1.0 maintains learning progress while ensuring stability 
<a name="l499"><span class="ln">499  </span></a>- Critical for N-step learning where gradient magnitudes can be amplified 
<a name="l500"><span class="ln">500  </span></a> 
<a name="l501"><span class="ln">501  </span></a>### 4.4.7 Memory Management Optimization 
<a name="l502"><span class="ln">502  </span></a> 
<a name="l503"><span class="ln">503  </span></a>**Efficient N-Step Buffer** 
<a name="l504"><span class="ln">504  </span></a>- Circular buffer automatically handles N-step sequence management 
<a name="l505"><span class="ln">505  </span></a>- Only stores complete N-step transitions in replay buffer 
<a name="l506"><span class="ln">506  </span></a>- Reduces memory overhead compared to storing all individual steps 
<a name="l507"><span class="ln">507  </span></a> 
<a name="l508"><span class="ln">508  </span></a>**Fast Tensor Creation** 
<a name="l509"><span class="ln">509  </span></a>```python 
<a name="l510"><span class="ln">510  </span></a>states = torch.FloatTensor(np.array(states)).to(device) 
<a name="l511"><span class="ln">511  </span></a>``` 
<a name="l512"><span class="ln">512  </span></a>- Uses numpy array batching before tensor conversion for performance 
<a name="l513"><span class="ln">513  </span></a>- Prevents the catastrophic slowdown from individual tensor creation 
<a name="l514"><span class="ln">514  </span></a>- Essential optimization that reduced training time from 90 minutes to normal speeds 
<a name="l515"><span class="ln">515  </span></a> 
<a name="l516"><span class="ln">516  </span></a>## DDQN Setups from v1 to v4 are shown below <hr class="ls0"><a name="l517"><span class="ln">517  </span></a>#%% md 
<a name="l518"><span class="ln">518  </span></a># DDQN v1 Setup 
<a name="l519"><span class="ln">519  </span></a>- Reached a 72-83% success rate, 77.8% mean success rate in 5 evaluation sessions, 100 episodes per session 
<a name="l520"><span class="ln">520  </span></a>- Just 850 episodes needed to reach 230 reward 
<a name="l521"><span class="ln">521  </span></a>- Was somewhat simple to train 
<a name="l522"><span class="ln">522  </span></a> 
<a name="l523"><span class="ln">523  </span></a>``` 
<a name="l524"><span class="ln">524  </span></a># very paranoid about pycharm not using gpu so keep this line even if it is redundant (it is) 
<a name="l525"><span class="ln">525  </span></a>device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) 
<a name="l526"><span class="ln">526  </span></a>print(f&quot;Using device: {device}&quot;) 
<a name="l527"><span class="ln">527  </span></a> 
<a name="l528"><span class="ln">528  </span></a>class DoubleDQNAgent: 
<a name="l529"><span class="ln">529  </span></a>    def __init__(self, env): 
<a name="l530"><span class="ln">530  </span></a>        self.env = env 
<a name="l531"><span class="ln">531  </span></a>        self.q_net = DQN(env.observation_space.shape[0], env.action_space.n).to(device) 
<a name="l532"><span class="ln">532  </span></a>        self.target_net = DQN(env.observation_space.shape[0], env.action_space.n).to(device) 
<a name="l533"><span class="ln">533  </span></a>        self.target_net.load_state_dict(self.q_net.state_dict()) 
<a name="l534"><span class="ln">534  </span></a> 
<a name="l535"><span class="ln">535  </span></a>        self.optimizer = optim.Adam(self.q_net.parameters(), lr=0.0005, weight_decay=1e-5) # standard LR, used by many implementations 
<a name="l536"><span class="ln">536  </span></a>        self.memory = deque(maxlen=100000) 
<a name="l537"><span class="ln">537  </span></a> 
<a name="l538"><span class="ln">538  </span></a>        # Hyperparameters 
<a name="l539"><span class="ln">539  </span></a>        self.batch_size = 128 
<a name="l540"><span class="ln">540  </span></a>        self.gamma = 0.995 # universal across all successful implementations 
<a name="l541"><span class="ln">541  </span></a>        self.epsilon = 1.0 
<a name="l542"><span class="ln">542  </span></a>        self.epsilon_decay = 0.9995 
<a name="l543"><span class="ln">543  </span></a>        self.epsilon_min = 0.01 
<a name="l544"><span class="ln">544  </span></a>        self.tau = 0.005 
<a name="l545"><span class="ln">545  </span></a>        self.grad_clip = 1.0  # more aggressive gradient clipping since loss exploded 
<a name="l546"><span class="ln">546  </span></a> 
<a name="l547"><span class="ln">547  </span></a>    def act(self, state): 
<a name="l548"><span class="ln">548  </span></a>        if random.random() &lt; self.epsilon: 
<a name="l549"><span class="ln">549  </span></a>            return self.env.action_space.sample() 
<a name="l550"><span class="ln">550  </span></a>        state = torch.FloatTensor(state).to(device) 
<a name="l551"><span class="ln">551  </span></a>        with torch.no_grad(): 
<a name="l552"><span class="ln">552  </span></a>            return torch.argmax(self.q_net(state)).item() 
<a name="l553"><span class="ln">553  </span></a> 
<a name="l554"><span class="ln">554  </span></a>    def remember(self, state, action, reward, next_state, done): 
<a name="l555"><span class="ln">555  </span></a>        self.memory.append((state, action, reward, next_state, done)) 
<a name="l556"><span class="ln">556  </span></a> 
<a name="l557"><span class="ln">557  </span></a>    def replay(self): 
<a name="l558"><span class="ln">558  </span></a>        if len(self.memory) &lt; self.batch_size: 
<a name="l559"><span class="ln">559  </span></a>            return None, None, None 
<a name="l560"><span class="ln">560  </span></a> 
<a name="l561"><span class="ln">561  </span></a>        batch = random.sample(self.memory, self.batch_size) 
<a name="l562"><span class="ln">562  </span></a>        states, actions, rewards, next_states, dones = zip(*batch) 
<a name="l563"><span class="ln">563  </span></a> 
<a name="l564"><span class="ln">564  </span></a>        # Convert and normalize 
<a name="l565"><span class="ln">565  </span></a>        states = torch.FloatTensor(np.array(states)).to(device) 
<a name="l566"><span class="ln">566  </span></a>        next_states = torch.FloatTensor(np.array(next_states)).to(device) 
<a name="l567"><span class="ln">567  </span></a>        actions = torch.LongTensor(np.array(actions)).to(device) 
<a name="l568"><span class="ln">568  </span></a>        dones = torch.FloatTensor(np.array(dones)).to(device) 
<a name="l569"><span class="ln">569  </span></a> 
<a name="l570"><span class="ln">570  </span></a>        # (Kinda) robust reward normalization 
<a name="l571"><span class="ln">571  </span></a>        rewards = torch.FloatTensor(np.array(rewards)).to(device) 
<a name="l572"><span class="ln">572  </span></a>        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7) 
<a name="l573"><span class="ln">573  </span></a> 
<a name="l574"><span class="ln">574  </span></a>        # Double DQN update 
<a name="l575"><span class="ln">575  </span></a>        current_q = self.q_net(states).gather(1, actions.unsqueeze(1)) 
<a name="l576"><span class="ln">576  </span></a> 
<a name="l577"><span class="ln">577  </span></a>        with torch.no_grad(): 
<a name="l578"><span class="ln">578  </span></a>            next_actions = self.q_net(next_states).argmax(dim=1, keepdim=True) 
<a name="l579"><span class="ln">579  </span></a>            next_q = self.target_net(next_states).gather(1, next_actions) 
<a name="l580"><span class="ln">580  </span></a>            target_q = rewards + (1 - dones) * self.gamma * next_q.squeeze() 
<a name="l581"><span class="ln">581  </span></a> 
<a name="l582"><span class="ln">582  </span></a>        loss = nn.SmoothL1Loss()(current_q.squeeze(), target_q.detach()) 
<a name="l583"><span class="ln">583  </span></a> 
<a name="l584"><span class="ln">584  </span></a>        self.optimizer.zero_grad() 
<a name="l585"><span class="ln">585  </span></a>        loss.backward() 
<a name="l586"><span class="ln">586  </span></a>        grad_norm = torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), self.grad_clip) 
<a name="l587"><span class="ln">587  </span></a>        self.optimizer.step() 
<a name="l588"><span class="ln">588  </span></a> 
<a name="l589"><span class="ln">589  </span></a>        # Soft target network update 
<a name="l590"><span class="ln">590  </span></a>        for target_param, param in zip(self.target_net.parameters(), self.q_net.parameters()): 
<a name="l591"><span class="ln">591  </span></a>            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data) 
<a name="l592"><span class="ln">592  </span></a> 
<a name="l593"><span class="ln">593  </span></a>        # Epsilon decay 
<a name="l594"><span class="ln">594  </span></a>        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay) 
<a name="l595"><span class="ln">595  </span></a> 
<a name="l596"><span class="ln">596  </span></a>        return loss.item(), grad_norm.item(), current_q.detach() 
<a name="l597"><span class="ln">597  </span></a>``` <hr class="ls0"><a name="l598"><span class="ln">598  </span></a>#%% md 
<a name="l599"><span class="ln">599  </span></a>### DDQN v2 used a different DQN network, combined with a different set of hyperparameters that solely focused on speed, but it failed quite miserably so I'm definitely not gonna go into detail about it here. 
<a name="l600"><span class="ln">600  </span></a>#### It converged very quickly at just 550 episodes, reaching a reward of 210, but when evaluated it had a success rate of ~55% and used twice as much fuel units as the DDQN v1 (sometimes up to 470 units!). 
<a name="l601"><span class="ln">601  </span></a>That's just more than enough reason to discard it. <hr class="ls0"><a name="l602"><span class="ln">602  </span></a>#%% md 
<a name="l603"><span class="ln">603  </span></a>### DDQN v3 Setup - Canceled because PER and N-step severely slowed down the training!!! It went from taking 1.5 s/it to 44 s/it, expected time to train 2000 episodes was nearly 28 hours! 
<a name="l604"><span class="ln">604  </span></a>### The slowdown problem lies more with PER, so I'mma get rid of it in v4. 
<a name="l605"><span class="ln">605  </span></a>#### Definitely gonna try this setup again once I get my new PC with Dual Xeon 
<a name="l606"><span class="ln">606  </span></a> 
<a name="l607"><span class="ln">607  </span></a>``` 
<a name="l608"><span class="ln">608  </span></a># DDQN v3 Setup with PER and N-step 
<a name="l609"><span class="ln">609  </span></a>import heapq 
<a name="l610"><span class="ln">610  </span></a>from collections import namedtuple 
<a name="l611"><span class="ln">611  </span></a> 
<a name="l612"><span class="ln">612  </span></a># Named tuple for storing experiences with priorities 
<a name="l613"><span class="ln">613  </span></a>Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done', 'priority']) 
<a name="l614"><span class="ln">614  </span></a> 
<a name="l615"><span class="ln">615  </span></a>class PrioritizedReplayBuffer: 
<a name="l616"><span class="ln">616  </span></a>    def __init__(self, capacity, alpha=0.6): 
<a name="l617"><span class="ln">617  </span></a>        self.capacity = capacity 
<a name="l618"><span class="ln">618  </span></a>        self.alpha = alpha  # How much prioritization to use (0 = uniform, 1 = full prioritization) 
<a name="l619"><span class="ln">619  </span></a>        self.buffer = {} 
<a name="l620"><span class="ln">620  </span></a>        self.priorities = {} 
<a name="l621"><span class="ln">621  </span></a>        self.pos = 0 
<a name="l622"><span class="ln">622  </span></a>        self.max_priority = 1.0 
<a name="l623"><span class="ln">623  </span></a> 
<a name="l624"><span class="ln">624  </span></a>    def add(self, state, action, reward, next_state, done, td_error=None): 
<a name="l625"><span class="ln">625  </span></a>        # Calculate priority from TD error 
<a name="l626"><span class="ln">626  </span></a>        priority = (abs(td_error) + 1e-6) ** self.alpha if td_error is not None else self.max_priority 
<a name="l627"><span class="ln">627  </span></a> 
<a name="l628"><span class="ln">628  </span></a>        # Store experience 
<a name="l629"><span class="ln">629  </span></a>        experience = Experience(state, action, reward, next_state, done, priority) 
<a name="l630"><span class="ln">630  </span></a> 
<a name="l631"><span class="ln">631  </span></a>        if len(self.buffer) &lt; self.capacity: 
<a name="l632"><span class="ln">632  </span></a>            self.buffer[self.pos] = experience 
<a name="l633"><span class="ln">633  </span></a>            self.priorities[self.pos] = priority 
<a name="l634"><span class="ln">634  </span></a>        else: 
<a name="l635"><span class="ln">635  </span></a>            # Replace oldest experience 
<a name="l636"><span class="ln">636  </span></a>            self.buffer[self.pos] = experience 
<a name="l637"><span class="ln">637  </span></a>            self.priorities[self.pos] = priority 
<a name="l638"><span class="ln">638  </span></a> 
<a name="l639"><span class="ln">639  </span></a>        self.max_priority = max(self.max_priority, priority) 
<a name="l640"><span class="ln">640  </span></a>        self.pos = (self.pos + 1) % self.capacity 
<a name="l641"><span class="ln">641  </span></a> 
<a name="l642"><span class="ln">642  </span></a>    def sample(self, batch_size, beta=0.4): 
<a name="l643"><span class="ln">643  </span></a>        if len(self.buffer) &lt; batch_size: 
<a name="l644"><span class="ln">644  </span></a>            return None, None, None 
<a name="l645"><span class="ln">645  </span></a> 
<a name="l646"><span class="ln">646  </span></a>        # Calculate sampling probabilities 
<a name="l647"><span class="ln">647  </span></a>        priorities = np.array(list(self.priorities.values())) 
<a name="l648"><span class="ln">648  </span></a>        probs = priorities / priorities.sum() 
<a name="l649"><span class="ln">649  </span></a> 
<a name="l650"><span class="ln">650  </span></a>        # Sample indices based on priorities 
<a name="l651"><span class="ln">651  </span></a>        indices = np.random.choice(len(self.buffer), batch_size, p=probs) 
<a name="l652"><span class="ln">652  </span></a> 
<a name="l653"><span class="ln">653  </span></a>        # Get experiences 
<a name="l654"><span class="ln">654  </span></a>        experiences = [self.buffer[idx] for idx in indices] 
<a name="l655"><span class="ln">655  </span></a> 
<a name="l656"><span class="ln">656  </span></a>        # Calculate importance sampling weights 
<a name="l657"><span class="ln">657  </span></a>        total = len(self.buffer) 
<a name="l658"><span class="ln">658  </span></a>        weights = (total * probs[indices]) ** (-beta) 
<a name="l659"><span class="ln">659  </span></a>        weights /= weights.max()  # Normalize for stability 
<a name="l660"><span class="ln">660  </span></a> 
<a name="l661"><span class="ln">661  </span></a>        return experiences, indices, weights 
<a name="l662"><span class="ln">662  </span></a> 
<a name="l663"><span class="ln">663  </span></a>    def update_priorities(self, indices, td_errors): 
<a name="l664"><span class="ln">664  </span></a>        for idx, td_error in zip(indices, td_errors): 
<a name="l665"><span class="ln">665  </span></a>            priority = (abs(td_error) + 1e-6) ** self.alpha 
<a name="l666"><span class="ln">666  </span></a>            self.priorities[idx] = priority 
<a name="l667"><span class="ln">667  </span></a>            self.max_priority = max(self.max_priority, priority) 
<a name="l668"><span class="ln">668  </span></a> 
<a name="l669"><span class="ln">669  </span></a>    def __len__(self): 
<a name="l670"><span class="ln">670  </span></a>        return len(self.buffer) 
<a name="l671"><span class="ln">671  </span></a> 
<a name="l672"><span class="ln">672  </span></a># v3 uses PER and N-step 
<a name="l673"><span class="ln">673  </span></a>class DoubleDQNAgent: 
<a name="l674"><span class="ln">674  </span></a>    def __init__(self, env): 
<a name="l675"><span class="ln">675  </span></a>        self.env = env 
<a name="l676"><span class="ln">676  </span></a>        self.q_net = DQN(env.observation_space.shape[0], env.action_space.n).to(device) 
<a name="l677"><span class="ln">677  </span></a>        self.target_net = DQN(env.observation_space.shape[0], env.action_space.n).to(device) 
<a name="l678"><span class="ln">678  </span></a>        self.target_net.load_state_dict(self.q_net.state_dict()) 
<a name="l679"><span class="ln">679  </span></a> 
<a name="l680"><span class="ln">680  </span></a>        self.optimizer = optim.Adam(self.q_net.parameters(), lr=0.0005, weight_decay=1e-5) 
<a name="l681"><span class="ln">681  </span></a>        self.memory = PrioritizedReplayBuffer(100000, alpha=0.6)  # PER buffer 
<a name="l682"><span class="ln">682  </span></a> 
<a name="l683"><span class="ln">683  </span></a>        # Hyperparameters 
<a name="l684"><span class="ln">684  </span></a>        self.batch_size = 128 
<a name="l685"><span class="ln">685  </span></a>        self.gamma = 0.995 
<a name="l686"><span class="ln">686  </span></a>        self.epsilon = 1.0 
<a name="l687"><span class="ln">687  </span></a>        self.epsilon_decay = 0.9995 
<a name="l688"><span class="ln">688  </span></a>        self.epsilon_min = 0.01 
<a name="l689"><span class="ln">689  </span></a>        self.tau = 0.005 
<a name="l690"><span class="ln">690  </span></a>        self.grad_clip = 1.0 
<a name="l691"><span class="ln">691  </span></a> 
<a name="l692"><span class="ln">692  </span></a>        # PER parameters 
<a name="l693"><span class="ln">693  </span></a>        self.beta = 0.4 
<a name="l694"><span class="ln">694  </span></a>        self.beta_increment = 0.001 
<a name="l695"><span class="ln">695  </span></a> 
<a name="l696"><span class="ln">696  </span></a>        # N-step parameters 
<a name="l697"><span class="ln">697  </span></a>        self.n_step = 3 
<a name="l698"><span class="ln">698  </span></a>        self.n_step_buffer = deque(maxlen=self.n_step) 
<a name="l699"><span class="ln">699  </span></a> 
<a name="l700"><span class="ln">700  </span></a>    def act(self, state): 
<a name="l701"><span class="ln">701  </span></a>        if random.random() &lt; self.epsilon: 
<a name="l702"><span class="ln">702  </span></a>            return self.env.action_space.sample() 
<a name="l703"><span class="ln">703  </span></a>        state = torch.FloatTensor(state).to(device) 
<a name="l704"><span class="ln">704  </span></a>        with torch.no_grad(): 
<a name="l705"><span class="ln">705  </span></a>            return torch.argmax(self.q_net(state)).item() 
<a name="l706"><span class="ln">706  </span></a> 
<a name="l707"><span class="ln">707  </span></a>    def remember(self, state, action, reward, next_state, done): 
<a name="l708"><span class="ln">708  </span></a>        # Add to n-step buffer 
<a name="l709"><span class="ln">709  </span></a>        self.n_step_buffer.append((state, action, reward, next_state, done)) 
<a name="l710"><span class="ln">710  </span></a> 
<a name="l711"><span class="ln">711  </span></a>        if len(self.n_step_buffer) == self.n_step: 
<a name="l712"><span class="ln">712  </span></a>            # Calculate n-step return 
<a name="l713"><span class="ln">713  </span></a>            n_step_state, n_step_action, n_step_reward, n_step_next_state, n_step_done = self.calculate_n_step_return() 
<a name="l714"><span class="ln">714  </span></a> 
<a name="l715"><span class="ln">715  </span></a>            # Calculate initial TD error for prioritization 
<a name="l716"><span class="ln">716  </span></a>            with torch.no_grad(): 
<a name="l717"><span class="ln">717  </span></a>                current_q = self.q_net(torch.FloatTensor(n_step_state).to(device)) 
<a name="l718"><span class="ln">718  </span></a>                current_q_value = current_q[n_step_action].item() 
<a name="l719"><span class="ln">719  </span></a> 
<a name="l720"><span class="ln">720  </span></a>                next_q = self.target_net(torch.FloatTensor(n_step_next_state).to(device)) 
<a name="l721"><span class="ln">721  </span></a>                target_q_value = n_step_reward + (self.gamma ** self.n_step) * next_q.max().item() * (1 - n_step_done) 
<a name="l722"><span class="ln">722  </span></a> 
<a name="l723"><span class="ln">723  </span></a>                td_error = abs(target_q_value - current_q_value) 
<a name="l724"><span class="ln">724  </span></a> 
<a name="l725"><span class="ln">725  </span></a>            # Store in prioritized buffer 
<a name="l726"><span class="ln">726  </span></a>            self.memory.add(n_step_state, n_step_action, n_step_reward, n_step_next_state, n_step_done, td_error) 
<a name="l727"><span class="ln">727  </span></a> 
<a name="l728"><span class="ln">728  </span></a>    def calculate_n_step_return(self): 
<a name="l729"><span class="ln">729  </span></a>        &quot;&quot;&quot;Calculate n-step return from buffer&quot;&quot;&quot; 
<a name="l730"><span class="ln">730  </span></a>        n_step_reward = 0 
<a name="l731"><span class="ln">731  </span></a>        for i, (_, _, reward, _, done) in enumerate(self.n_step_buffer): 
<a name="l732"><span class="ln">732  </span></a>            n_step_reward += (self.gamma ** i) * reward 
<a name="l733"><span class="ln">733  </span></a>            if done: 
<a name="l734"><span class="ln">734  </span></a>                break 
<a name="l735"><span class="ln">735  </span></a> 
<a name="l736"><span class="ln">736  </span></a>        # Get first and last states 
<a name="l737"><span class="ln">737  </span></a>        first_state, first_action, _, _, _ = self.n_step_buffer[0] 
<a name="l738"><span class="ln">738  </span></a>        _, _, _, last_next_state, last_done = self.n_step_buffer[-1] 
<a name="l739"><span class="ln">739  </span></a> 
<a name="l740"><span class="ln">740  </span></a>        return first_state, first_action, n_step_reward, last_next_state, last_done 
<a name="l741"><span class="ln">741  </span></a> 
<a name="l742"><span class="ln">742  </span></a>    def replay(self): 
<a name="l743"><span class="ln">743  </span></a>        # Sample from prioritized buffer 
<a name="l744"><span class="ln">744  </span></a>        batch_data = self.memory.sample(self.batch_size, self.beta) 
<a name="l745"><span class="ln">745  </span></a>        if batch_data[0] is None: 
<a name="l746"><span class="ln">746  </span></a>            return None, None, None 
<a name="l747"><span class="ln">747  </span></a> 
<a name="l748"><span class="ln">748  </span></a>        experiences, indices, weights = batch_data 
<a name="l749"><span class="ln">749  </span></a> 
<a name="l750"><span class="ln">750  </span></a>        # Extract batch data 
<a name="l751"><span class="ln">751  </span></a>        states = torch.FloatTensor(np.array([e.state for e in experiences])).to(device) 
<a name="l752"><span class="ln">752  </span></a>        actions = torch.LongTensor(np.array([e.action for e in experiences])).to(device) 
<a name="l753"><span class="ln">753  </span></a>        rewards = torch.FloatTensor(np.array([e.reward for e in experiences])).to(device) 
<a name="l754"><span class="ln">754  </span></a>        next_states = torch.FloatTensor(np.array([e.next_state for e in experiences])).to(device) 
<a name="l755"><span class="ln">755  </span></a>        dones = torch.FloatTensor(np.array([e.done for e in experiences])).to(device) 
<a name="l756"><span class="ln">756  </span></a>        weights = torch.FloatTensor(weights).to(device) 
<a name="l757"><span class="ln">757  </span></a> 
<a name="l758"><span class="ln">758  </span></a>        # Normalize rewards 
<a name="l759"><span class="ln">759  </span></a>        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7) 
<a name="l760"><span class="ln">760  </span></a> 
<a name="l761"><span class="ln">761  </span></a>        # Double DQN update 
<a name="l762"><span class="ln">762  </span></a>        current_q = self.q_net(states).gather(1, actions.unsqueeze(1)) 
<a name="l763"><span class="ln">763  </span></a> 
<a name="l764"><span class="ln">764  </span></a>        with torch.no_grad(): 
<a name="l765"><span class="ln">765  </span></a>            next_actions = self.q_net(next_states).argmax(dim=1, keepdim=True) 
<a name="l766"><span class="ln">766  </span></a>            next_q = self.target_net(next_states).gather(1, next_actions) 
<a name="l767"><span class="ln">767  </span></a>            target_q = rewards + (1 - dones) * (self.gamma ** self.n_step) * next_q.squeeze() 
<a name="l768"><span class="ln">768  </span></a> 
<a name="l769"><span class="ln">769  </span></a>        # Calculate TD errors for priority update 
<a name="l770"><span class="ln">770  </span></a>        td_errors = (current_q.squeeze() - target_q).detach().cpu().numpy() 
<a name="l771"><span class="ln">771  </span></a> 
<a name="l772"><span class="ln">772  </span></a>        # Weighted loss for importance sampling 
<a name="l773"><span class="ln">773  </span></a>        loss = (weights * nn.SmoothL1Loss(reduction='none')(current_q.squeeze(), target_q.detach())).mean() 
<a name="l774"><span class="ln">774  </span></a> 
<a name="l775"><span class="ln">775  </span></a>        self.optimizer.zero_grad() 
<a name="l776"><span class="ln">776  </span></a>        loss.backward() 
<a name="l777"><span class="ln">777  </span></a>        grad_norm = torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), self.grad_clip) 
<a name="l778"><span class="ln">778  </span></a>        self.optimizer.step() 
<a name="l779"><span class="ln">779  </span></a> 
<a name="l780"><span class="ln">780  </span></a>        # Update priorities in buffer 
<a name="l781"><span class="ln">781  </span></a>        self.memory.update_priorities(indices, td_errors) 
<a name="l782"><span class="ln">782  </span></a> 
<a name="l783"><span class="ln">783  </span></a>        # Update beta for importance sampling 
<a name="l784"><span class="ln">784  </span></a>        self.beta = min(1.0, self.beta + self.beta_increment) 
<a name="l785"><span class="ln">785  </span></a> 
<a name="l786"><span class="ln">786  </span></a>        # Soft target network update 
<a name="l787"><span class="ln">787  </span></a>        for target_param, param in zip(self.target_net.parameters(), self.q_net.parameters()): 
<a name="l788"><span class="ln">788  </span></a>            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data) 
<a name="l789"><span class="ln">789  </span></a> 
<a name="l790"><span class="ln">790  </span></a>        # Epsilon decay 
<a name="l791"><span class="ln">791  </span></a>        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay) 
<a name="l792"><span class="ln">792  </span></a> 
<a name="l793"><span class="ln">793  </span></a>        return loss.item(), grad_norm.item(), current_q.detach() 
<a name="l794"><span class="ln">794  </span></a>``` 
<a name="l795"><span class="ln">795  </span></a> <hr class="ls0"><a name="l796"><span class="ln">796  </span></a>#%% md 
<a name="l797"><span class="ln">797  </span></a>### And finally, DDQN v4: <hr class="ls0"><a name="l798"><span class="ln">798  </span></a>#%% 
<a name="l799"><span class="ln">799  </span></a></span><span class="s3"># DDQN v4 Setup - Kept N-step and discarded PER</span>
<a name="l800"><span class="ln">800  </span></a><span class="s2">from </span><span class="s0">collections </span><span class="s2">import </span><span class="s0">deque</span>
<a name="l801"><span class="ln">801  </span></a><span class="s2">class </span><span class="s0">DoubleDQNAgent:</span>
<a name="l802"><span class="ln">802  </span></a>    <span class="s2">def </span><span class="s0">__init__(self</span><span class="s5">, </span><span class="s0">env):</span>
<a name="l803"><span class="ln">803  </span></a>        <span class="s0">self.env = env</span>
<a name="l804"><span class="ln">804  </span></a>        <span class="s0">self.q_net = DQN(env.observation_space.shape[</span><span class="s6">0</span><span class="s0">]</span><span class="s5">, </span><span class="s0">env.action_space.n).to(device)</span>
<a name="l805"><span class="ln">805  </span></a>        <span class="s0">self.target_net = DQN(env.observation_space.shape[</span><span class="s6">0</span><span class="s0">]</span><span class="s5">, </span><span class="s0">env.action_space.n).to(device)</span>
<a name="l806"><span class="ln">806  </span></a>        <span class="s0">self.target_net.load_state_dict(self.q_net.state_dict())</span>
<a name="l807"><span class="ln">807  </span></a>
<a name="l808"><span class="ln">808  </span></a>        <span class="s0">self.optimizer = optim.Adam(self.q_net.parameters()</span><span class="s5">, </span><span class="s0">lr=</span><span class="s6">0.0001</span><span class="s5">, </span><span class="s0">weight_decay=</span><span class="s6">1e-5</span><span class="s0">)</span>
<a name="l809"><span class="ln">809  </span></a>        <span class="s0">self.memory = deque(maxlen=</span><span class="s6">100000</span><span class="s0">)</span>
<a name="l810"><span class="ln">810  </span></a>
<a name="l811"><span class="ln">811  </span></a>        <span class="s3"># Battle-tested Proven Hyperparameters &lt;3</span>
<a name="l812"><span class="ln">812  </span></a>        <span class="s0">self.batch_size = </span><span class="s6">256</span>
<a name="l813"><span class="ln">813  </span></a>        <span class="s0">self.gamma = </span><span class="s6">0.99</span>
<a name="l814"><span class="ln">814  </span></a>        <span class="s0">self.epsilon = </span><span class="s6">1.0</span>
<a name="l815"><span class="ln">815  </span></a>        <span class="s0">self.epsilon_decay = </span><span class="s6">0.998</span>
<a name="l816"><span class="ln">816  </span></a>        <span class="s0">self.epsilon_min = </span><span class="s6">0.1</span>
<a name="l817"><span class="ln">817  </span></a>        <span class="s0">self.tau = </span><span class="s6">0.001</span>
<a name="l818"><span class="ln">818  </span></a>        <span class="s0">self.grad_clip = </span><span class="s6">1.0</span>
<a name="l819"><span class="ln">819  </span></a>
<a name="l820"><span class="ln">820  </span></a>        <span class="s3"># N-step wasn't too time-consuming, so it was kept</span>
<a name="l821"><span class="ln">821  </span></a>        <span class="s0">self.n_step = </span><span class="s6">3</span>
<a name="l822"><span class="ln">822  </span></a>        <span class="s0">self.n_step_buffer = deque(maxlen=self.n_step)</span>
<a name="l823"><span class="ln">823  </span></a>
<a name="l824"><span class="ln">824  </span></a>    <span class="s2">def </span><span class="s0">act(self</span><span class="s5">, </span><span class="s0">state):</span>
<a name="l825"><span class="ln">825  </span></a>        <span class="s2">if </span><span class="s0">random.random() &lt; self.epsilon:</span>
<a name="l826"><span class="ln">826  </span></a>            <span class="s2">return </span><span class="s0">self.env.action_space.sample()</span>
<a name="l827"><span class="ln">827  </span></a>        <span class="s0">state = torch.FloatTensor(state).to(device)</span>
<a name="l828"><span class="ln">828  </span></a>        <span class="s2">with </span><span class="s0">torch.no_grad():</span>
<a name="l829"><span class="ln">829  </span></a>            <span class="s2">return </span><span class="s0">torch.argmax(self.q_net(state)).item()</span>
<a name="l830"><span class="ln">830  </span></a>
<a name="l831"><span class="ln">831  </span></a>    <span class="s2">def </span><span class="s0">remember(self</span><span class="s5">, </span><span class="s0">state</span><span class="s5">, </span><span class="s0">action</span><span class="s5">, </span><span class="s0">reward</span><span class="s5">, </span><span class="s0">next_state</span><span class="s5">, </span><span class="s0">done):</span>
<a name="l832"><span class="ln">832  </span></a>        <span class="s0">self.n_step_buffer.append((state</span><span class="s5">, </span><span class="s0">action</span><span class="s5">, </span><span class="s0">reward</span><span class="s5">, </span><span class="s0">next_state</span><span class="s5">, </span><span class="s0">done))</span>
<a name="l833"><span class="ln">833  </span></a>
<a name="l834"><span class="ln">834  </span></a>        <span class="s2">if </span><span class="s0">len(self.n_step_buffer) == self.n_step:</span>
<a name="l835"><span class="ln">835  </span></a>            <span class="s3"># Calculate n-step return</span>
<a name="l836"><span class="ln">836  </span></a>            <span class="s0">n_step_state</span><span class="s5">, </span><span class="s0">n_step_action</span><span class="s5">, </span><span class="s0">n_step_reward</span><span class="s5">, </span><span class="s0">n_step_next_state</span><span class="s5">, </span><span class="s0">n_step_done = self.calculate_n_step_return()</span>
<a name="l837"><span class="ln">837  </span></a>            <span class="s0">self.memory.append((n_step_state</span><span class="s5">, </span><span class="s0">n_step_action</span><span class="s5">, </span><span class="s0">n_step_reward</span><span class="s5">, </span><span class="s0">n_step_next_state</span><span class="s5">, </span><span class="s0">n_step_done))</span>
<a name="l838"><span class="ln">838  </span></a>
<a name="l839"><span class="ln">839  </span></a>    <span class="s2">def </span><span class="s0">calculate_n_step_return(self):</span>
<a name="l840"><span class="ln">840  </span></a>        <span class="s8">&quot;&quot;&quot;Calculate n-step return from buffer&quot;&quot;&quot;</span>
<a name="l841"><span class="ln">841  </span></a>        <span class="s0">n_step_reward = </span><span class="s6">0</span>
<a name="l842"><span class="ln">842  </span></a>        <span class="s2">for </span><span class="s0">i</span><span class="s5">, </span><span class="s0">(_</span><span class="s5">, </span><span class="s0">_</span><span class="s5">, </span><span class="s0">reward</span><span class="s5">, </span><span class="s0">_</span><span class="s5">, </span><span class="s0">done) </span><span class="s2">in </span><span class="s0">enumerate(self.n_step_buffer):</span>
<a name="l843"><span class="ln">843  </span></a>            <span class="s0">n_step_reward += (self.gamma ** i) * reward</span>
<a name="l844"><span class="ln">844  </span></a>            <span class="s2">if </span><span class="s0">done:</span>
<a name="l845"><span class="ln">845  </span></a>                <span class="s2">break</span>
<a name="l846"><span class="ln">846  </span></a>
<a name="l847"><span class="ln">847  </span></a>        <span class="s3"># Get first and last states</span>
<a name="l848"><span class="ln">848  </span></a>        <span class="s0">first_state</span><span class="s5">, </span><span class="s0">first_action</span><span class="s5">, </span><span class="s0">_</span><span class="s5">, </span><span class="s0">_</span><span class="s5">, </span><span class="s0">_ = self.n_step_buffer[</span><span class="s6">0</span><span class="s0">]</span>
<a name="l849"><span class="ln">849  </span></a>        <span class="s0">_</span><span class="s5">, </span><span class="s0">_</span><span class="s5">, </span><span class="s0">_</span><span class="s5">, </span><span class="s0">last_next_state</span><span class="s5">, </span><span class="s0">last_done = self.n_step_buffer[-</span><span class="s6">1</span><span class="s0">]</span>
<a name="l850"><span class="ln">850  </span></a>
<a name="l851"><span class="ln">851  </span></a>        <span class="s2">return </span><span class="s0">first_state</span><span class="s5">, </span><span class="s0">first_action</span><span class="s5">, </span><span class="s0">n_step_reward</span><span class="s5">, </span><span class="s0">last_next_state</span><span class="s5">, </span><span class="s0">last_done</span>
<a name="l852"><span class="ln">852  </span></a>
<a name="l853"><span class="ln">853  </span></a>    <span class="s2">def </span><span class="s0">replay(self):</span>
<a name="l854"><span class="ln">854  </span></a>        <span class="s2">if </span><span class="s0">len(self.memory) &lt; self.batch_size:</span>
<a name="l855"><span class="ln">855  </span></a>            <span class="s2">return None</span><span class="s5">, </span><span class="s2">None</span><span class="s5">, </span><span class="s2">None</span>
<a name="l856"><span class="ln">856  </span></a>
<a name="l857"><span class="ln">857  </span></a>        <span class="s0">batch = random.sample(self.memory</span><span class="s5">, </span><span class="s0">self.batch_size)</span>
<a name="l858"><span class="ln">858  </span></a>        <span class="s0">states</span><span class="s5">, </span><span class="s0">actions</span><span class="s5">, </span><span class="s0">rewards</span><span class="s5">, </span><span class="s0">next_states</span><span class="s5">, </span><span class="s0">dones = zip(*batch)</span>
<a name="l859"><span class="ln">859  </span></a>
<a name="l860"><span class="ln">860  </span></a>        <span class="s3"># Fast tensor creation</span>
<a name="l861"><span class="ln">861  </span></a>        <span class="s0">states = torch.FloatTensor(np.array(states)).to(device)</span>
<a name="l862"><span class="ln">862  </span></a>        <span class="s0">next_states = torch.FloatTensor(np.array(next_states)).to(device)</span>
<a name="l863"><span class="ln">863  </span></a>        <span class="s0">actions = torch.LongTensor(np.array(actions)).to(device)</span>
<a name="l864"><span class="ln">864  </span></a>        <span class="s0">dones = torch.FloatTensor(np.array(dones)).to(device)</span>
<a name="l865"><span class="ln">865  </span></a>        <span class="s0">rewards = torch.FloatTensor(np.array(rewards)).to(device)</span>
<a name="l866"><span class="ln">866  </span></a>
<a name="l867"><span class="ln">867  </span></a>        <span class="s3"># Normalize rewards</span>
<a name="l868"><span class="ln">868  </span></a>        <span class="s0">rewards = (rewards - rewards.mean()) / (rewards.std() + </span><span class="s6">1e-7</span><span class="s0">)</span>
<a name="l869"><span class="ln">869  </span></a>
<a name="l870"><span class="ln">870  </span></a>        <span class="s3"># Boom boom, DDQN with n-step returns</span>
<a name="l871"><span class="ln">871  </span></a>        <span class="s0">current_q = self.q_net(states).gather(</span><span class="s6">1</span><span class="s5">, </span><span class="s0">actions.unsqueeze(</span><span class="s6">1</span><span class="s0">))</span>
<a name="l872"><span class="ln">872  </span></a>
<a name="l873"><span class="ln">873  </span></a>        <span class="s2">with </span><span class="s0">torch.no_grad():</span>
<a name="l874"><span class="ln">874  </span></a>            <span class="s0">next_actions = self.q_net(next_states).argmax(dim=</span><span class="s6">1</span><span class="s5">, </span><span class="s0">keepdim=</span><span class="s2">True</span><span class="s0">)</span>
<a name="l875"><span class="ln">875  </span></a>            <span class="s0">next_q = self.target_net(next_states).gather(</span><span class="s6">1</span><span class="s5">, </span><span class="s0">next_actions)</span>
<a name="l876"><span class="ln">876  </span></a>            <span class="s0">target_q = rewards + (</span><span class="s6">1 </span><span class="s0">- dones) * (self.gamma ** self.n_step) * next_q.squeeze()</span>
<a name="l877"><span class="ln">877  </span></a>
<a name="l878"><span class="ln">878  </span></a>        <span class="s3"># Simple loss - no importance sampling overhead (i hate you PER)</span>
<a name="l879"><span class="ln">879  </span></a>        <span class="s0">loss = nn.SmoothL1Loss()(current_q.squeeze()</span><span class="s5">, </span><span class="s0">target_q.detach())</span>
<a name="l880"><span class="ln">880  </span></a>
<a name="l881"><span class="ln">881  </span></a>        <span class="s0">self.optimizer.zero_grad()</span>
<a name="l882"><span class="ln">882  </span></a>        <span class="s0">loss.backward()</span>
<a name="l883"><span class="ln">883  </span></a>        <span class="s0">grad_norm = torch.nn.utils.clip_grad_norm_(self.q_net.parameters()</span><span class="s5">, </span><span class="s0">self.grad_clip)</span>
<a name="l884"><span class="ln">884  </span></a>        <span class="s0">self.optimizer.step()</span>
<a name="l885"><span class="ln">885  </span></a>
<a name="l886"><span class="ln">886  </span></a>        <span class="s3"># Soft target network update</span>
<a name="l887"><span class="ln">887  </span></a>        <span class="s2">for </span><span class="s0">target_param</span><span class="s5">, </span><span class="s0">param </span><span class="s2">in </span><span class="s0">zip(self.target_net.parameters()</span><span class="s5">, </span><span class="s0">self.q_net.parameters()):</span>
<a name="l888"><span class="ln">888  </span></a>            <span class="s0">target_param.data.copy_(self.tau * param.data + (</span><span class="s6">1 </span><span class="s0">- self.tau) * target_param.data)</span>
<a name="l889"><span class="ln">889  </span></a>
<a name="l890"><span class="ln">890  </span></a>        <span class="s3"># Epsilon decay</span>
<a name="l891"><span class="ln">891  </span></a>        <span class="s0">self.epsilon = max(self.epsilon_min</span><span class="s5">, </span><span class="s0">self.epsilon * self.epsilon_decay)</span>
<a name="l892"><span class="ln">892  </span></a>        <span class="s2">return </span><span class="s0">loss.item()</span><span class="s5">, </span><span class="s0">grad_norm.item()</span><span class="s5">, </span><span class="s0">current_q.detach()</span><hr class="ls0"><a name="l893"><span class="ln">893  </span></a><span class="s0">#%% md 
<a name="l894"><span class="ln">894  </span></a>### 4.5 Training Loop 
<a name="l895"><span class="ln">895  </span></a> 
<a name="l896"><span class="ln">896  </span></a>### 4.5.1 Warmup Phase Strategy 
<a name="l897"><span class="ln">897  </span></a> 
<a name="l898"><span class="ln">898  </span></a>**Extended Exploration Period** 
<a name="l899"><span class="ln">899  </span></a> 
<a name="l900"><span class="ln">900  </span></a>```python 
<a name="l901"><span class="ln">901  </span></a>if ep &lt; 300: 
<a name="l902"><span class="ln">902  </span></a>    agent.epsilon = 1.0 # Force exploration 
<a name="l903"><span class="ln">903  </span></a>elif ep == 300: 
<a name="l904"><span class="ln">904  </span></a>    agent.epsilon = 0.95 # Decay starts faster 
<a name="l905"><span class="ln">905  </span></a>``` 
<a name="l906"><span class="ln">906  </span></a> 
<a name="l907"><span class="ln">907  </span></a>**Benefits** 
<a name="l908"><span class="ln">908  </span></a> 
<a name="l909"><span class="ln">909  </span></a>- **Episodes 0-300**: Pure exploration to populate replay buffer with diverse experiences 
<a name="l910"><span class="ln">910  </span></a>- **Episode 300**: Reset exploration with accelerated decay to transition to exploitation 
<a name="l911"><span class="ln">911  </span></a>- **Post-300**: Normal epsilon decay (0.998) for exploitation transition 
<a name="l912"><span class="ln">912  </span></a> 
<a name="l913"><span class="ln">913  </span></a>### 4.5.2 Per-Step Learning Implementation 
<a name="l914"><span class="ln">914  </span></a> 
<a name="l915"><span class="ln">915  </span></a>**Continuous Learning Loop** 
<a name="l916"><span class="ln">916  </span></a> 
<a name="l917"><span class="ln">917  </span></a>```python 
<a name="l918"><span class="ln">918  </span></a>while not done: 
<a name="l919"><span class="ln">919  </span></a>    action = agent.act(state) 
<a name="l920"><span class="ln">920  </span></a>    next_state, reward, terminated, truncated, _ = env.step(action) 
<a name="l921"><span class="ln">921  </span></a>    done = terminated or truncated 
<a name="l922"><span class="ln">922  </span></a> 
<a name="l923"><span class="ln">923  </span></a>    agent.remember(state, action, reward, next_state, done) 
<a name="l924"><span class="ln">924  </span></a>    loss, grad_norm, current_q = agent.replay() # replay every step 
<a name="l925"><span class="ln">925  </span></a> 
<a name="l926"><span class="ln">926  </span></a>    state = next_state 
<a name="l927"><span class="ln">927  </span></a>    total_reward += reward 
<a name="l928"><span class="ln">928  </span></a>``` 
<a name="l929"><span class="ln">929  </span></a> 
<a name="l930"><span class="ln">930  </span></a>**Key Features** 
<a name="l931"><span class="ln">931  </span></a> 
<a name="l932"><span class="ln">932  </span></a>- **Step-by-Step Learning**: Network updates after every environment step, not episode end 
<a name="l933"><span class="ln">933  </span></a>- **N-Step Integration**: agent.remember() handles N-step buffer management automatically 
<a name="l934"><span class="ln">934  </span></a>- **Real-Time Monitoring**: Loss, gradient norm, and Q-values tracked per step 
<a name="l935"><span class="ln">935  </span></a> 
<a name="l936"><span class="ln">936  </span></a> 
<a name="l937"><span class="ln">937  </span></a>### 4.5.3 Early Stopping Mechanism 
<a name="l938"><span class="ln">938  </span></a> 
<a name="l939"><span class="ln">939  </span></a>**Success Criteria** 
<a name="l940"><span class="ln">940  </span></a> 
<a name="l941"><span class="ln">941  </span></a>``` 
<a name="l942"><span class="ln">942  </span></a>        if avg_reward &gt; 200: 
<a name="l943"><span class="ln">943  </span></a>            success_streak += 1 
<a name="l944"><span class="ln">944  </span></a>            if success_streak &gt;= 50: 
<a name="l945"><span class="ln">945  </span></a>                print(f&quot;\nEarly stopping at episode {ep+1} (avg: {avg_reward:.1f})!&quot;) 
<a name="l946"><span class="ln">946  </span></a>                torch.save(agent.q_net.state_dict(), &quot;dqn_success.pth&quot;) 
<a name="l947"><span class="ln">947  </span></a>                break 
<a name="l948"><span class="ln">948  </span></a>        else: 
<a name="l949"><span class="ln">949  </span></a>            success_streak = 0 
<a name="l950"><span class="ln">950  </span></a>``` 
<a name="l951"><span class="ln">951  </span></a> 
<a name="l952"><span class="ln">952  </span></a>**Design Logic** 
<a name="l953"><span class="ln">953  </span></a> 
<a name="l954"><span class="ln">954  </span></a>- **Threshold (200)**: LunarLander &quot;solved&quot; benchmark 
<a name="l955"><span class="ln">955  </span></a>- **Streak Requirement (50)**: Ensures consistent performance, not lucky episodes 
<a name="l956"><span class="ln">956  </span></a>- **Automatic Saving**: Best model saved immediately upon early stopping 
<a name="l957"><span class="ln">957  </span></a>- **Prevents Overtraining**: Stops before potential performance degradation 
<a name="l958"><span class="ln">958  </span></a> 
<a name="l959"><span class="ln">959  </span></a> 
<a name="l960"><span class="ln">960  </span></a>### 4.5.4 Performance Monitoring 
<a name="l961"><span class="ln">961  </span></a> 
<a name="l962"><span class="ln">962  </span></a>**Multi-Level Monitoring System** 
<a name="l963"><span class="ln">963  </span></a> 
<a name="l964"><span class="ln">964  </span></a>**Rolling Average Tracking** 
<a name="l965"><span class="ln">965  </span></a> 
<a name="l966"><span class="ln">966  </span></a>- 100-episode rolling average provides stable performance estimates 
<a name="l967"><span class="ln">967  </span></a>- Reduces noise from individual episode variance 
<a name="l968"><span class="ln">968  </span></a>- Enables reliable convergence detection 
<a name="l969"><span class="ln">969  </span></a> 
<a name="l970"><span class="ln">970  </span></a>**Detailed Progress Monitoring (Every 50 Episodes)** 
<a name="l971"><span class="ln">971  </span></a> 
<a name="l972"><span class="ln">972  </span></a>```python 
<a name="l973"><span class="ln">973  </span></a>if (ep + 1) % 50 == 0 and loss is not None and current_q is not None: 
<a name="l974"><span class="ln">974  </span></a>    print(f&quot;\nEpisode {ep+1}:&quot;) 
<a name="l975"><span class="ln">975  </span></a>    print(f&quot;Avg Reward (last 100): {avg_reward:.1f}&quot;) 
<a name="l976"><span class="ln">976  </span></a>    print(f&quot;Loss: {loss:.4f} | Grad Norm: {grad_norm:.2f}&quot;) 
<a name="l977"><span class="ln">977  </span></a>    print(f&quot;Q-values: {current_q.mean().item():.2f} ± {current_q.std().item():.2f}&quot;) 
<a name="l978"><span class="ln">978  </span></a>    print(f&quot;Epsilon: {agent.epsilon:.3f}&quot;) 
<a name="l979"><span class="ln">979  </span></a>``` 
<a name="l980"><span class="ln">980  </span></a> 
<a name="l981"><span class="ln">981  </span></a>**Real-Time Progress Bar** 
<a name="l982"><span class="ln">982  </span></a> 
<a name="l983"><span class="ln">983  </span></a>```python 
<a name="l984"><span class="ln">984  </span></a>progress_bar.set_postfix({ 
<a name="l985"><span class="ln">985  </span></a>    'reward': f&quot;{total_reward:.1f}&quot;, 
<a name="l986"><span class="ln">986  </span></a>    'avg': f&quot;{avg_reward:.1f}&quot;, 
<a name="l987"><span class="ln">987  </span></a>    'ε': f&quot;{agent.epsilon:.3f}&quot;, 
<a name="l988"><span class="ln">988  </span></a>    'grad': f&quot;{grad_norm:.2f}&quot; if grad_norm else &quot;N/A&quot; 
<a name="l989"><span class="ln">989  </span></a>}) 
<a name="l990"><span class="ln">990  </span></a>``` 
<a name="l991"><span class="ln">991  </span></a> 
<a name="l992"><span class="ln">992  </span></a> 
<a name="l993"><span class="ln">993  </span></a>### 4.5.5 Checkpointing Strategy 
<a name="l994"><span class="ln">994  </span></a> 
<a name="l995"><span class="ln">995  </span></a>**Post-Warmup Checkpointing** 
<a name="l996"><span class="ln">996  </span></a> 
<a name="l997"><span class="ln">997  </span></a>```python 
<a name="l998"><span class="ln">998  </span></a>if (ep + 1) % 100 == 0 and ep &gt;= 300: 
<a name="l999"><span class="ln">999  </span></a>    torch.save(agent.q_net.state_dict(), f&quot;dqn_ep{ep+1}.pth&quot;) 
<a name="l1000"><span class="ln">1000 </span></a>    if avg_reward &gt; best_avg: 
<a name="l1001"><span class="ln">1001 </span></a>        best_avg = avg_reward 
<a name="l1002"><span class="ln">1002 </span></a>        torch.save(agent.q_net.state_dict(), &quot;dqn_best.pth&quot;) 
<a name="l1003"><span class="ln">1003 </span></a>``` 
<a name="l1004"><span class="ln">1004 </span></a> 
<a name="l1005"><span class="ln">1005 </span></a>**Strategic Design** 
<a name="l1006"><span class="ln">1006 </span></a> 
<a name="l1007"><span class="ln">1007 </span></a>- **Delayed Checkpointing**: Only after warmup phase (episode 300+) to avoid saving untrained models 
<a name="l1008"><span class="ln">1008 </span></a>- **Regular Intervals**: Every 100 episodes for training recovery 
<a name="l1009"><span class="ln">1009 </span></a>- **Best Model Tracking**: Continuous monitoring and saving of peak performance 
<a name="l1010"><span class="ln">1010 </span></a>- **Failure Recovery**: Enables resuming from stable training points 
<a name="l1011"><span class="ln">1011 </span></a> 
<a name="l1012"><span class="ln">1012 </span></a> 
<a name="l1013"><span class="ln">1013 </span></a>### 4.5.6 Training Efficiency Features 
<a name="l1014"><span class="ln">1014 </span></a> 
<a name="l1015"><span class="ln">1015 </span></a>**GPU Verification** 
<a name="l1016"><span class="ln">1016 </span></a> 
<a name="l1017"><span class="ln">1017 </span></a>- Model placement verification at training start 
<a name="l1018"><span class="ln">1018 </span></a>- Ensures CUDA acceleration is properly utilized 
<a name="l1019"><span class="ln">1019 </span></a> 
<a name="l1020"><span class="ln">1020 </span></a>**Progress Tracking** 
<a name="l1021"><span class="ln">1021 </span></a> 
<a name="l1022"><span class="ln">1022 </span></a>- trange progress bar for visual training monitoring 
<a name="l1023"><span class="ln">1023 </span></a>- Real-time metrics display for immediate feedback 
<a name="l1024"><span class="ln">1024 </span></a>- Episode completion status and performance indicators 
<a name="l1025"><span class="ln">1025 </span></a> 
<a name="l1026"><span class="ln">1026 </span></a>**Memory Management** 
<a name="l1027"><span class="ln">1027 </span></a> 
<a name="l1028"><span class="ln">1028 </span></a>- Environment properly closed after training completion 
<a name="l1029"><span class="ln">1029 </span></a>- Returns both reward history and trained agent for analysis <hr class="ls0"><a name="l1030"><span class="ln">1030 </span></a>#%% 
<a name="l1031"><span class="ln">1031 </span></a></span><span class="s2">def </span><span class="s0">train_agent():</span>
<a name="l1032"><span class="ln">1032 </span></a>    <span class="s0">env = gym.make(</span><span class="s4">&quot;LunarLander-v3&quot;</span><span class="s0">)</span>
<a name="l1033"><span class="ln">1033 </span></a>    <span class="s0">agent = DoubleDQNAgent(env)</span>
<a name="l1034"><span class="ln">1034 </span></a>    <span class="s0">print(</span><span class="s4">&quot;Model on GPU?&quot;</span><span class="s5">, </span><span class="s0">next(agent.q_net.parameters()).is_cuda)</span>
<a name="l1035"><span class="ln">1035 </span></a>
<a name="l1036"><span class="ln">1036 </span></a>    <span class="s0">episodes = </span><span class="s6">2000 </span><span class="s3"># whoever suggested 4000 should be executed :)</span>
<a name="l1037"><span class="ln">1037 </span></a>    <span class="s0">rewards = []</span>
<a name="l1038"><span class="ln">1038 </span></a>    <span class="s0">best_avg = -np.inf</span>
<a name="l1039"><span class="ln">1039 </span></a>    <span class="s0">success_streak = </span><span class="s6">0</span>
<a name="l1040"><span class="ln">1040 </span></a>
<a name="l1041"><span class="ln">1041 </span></a>    <span class="s0">progress_bar = trange(episodes</span><span class="s5">, </span><span class="s0">desc=</span><span class="s4">&quot;Training&quot;</span><span class="s0">)</span>
<a name="l1042"><span class="ln">1042 </span></a>
<a name="l1043"><span class="ln">1043 </span></a>    <span class="s2">for </span><span class="s0">ep </span><span class="s2">in </span><span class="s0">progress_bar:</span>
<a name="l1044"><span class="ln">1044 </span></a>        <span class="s3"># Warmup phase with FULLLLL exploration</span>
<a name="l1045"><span class="ln">1045 </span></a>        <span class="s2">if </span><span class="s0">ep &lt; </span><span class="s6">300</span><span class="s0">:</span>
<a name="l1046"><span class="ln">1046 </span></a>            <span class="s0">agent.epsilon = </span><span class="s6">1.0</span>
<a name="l1047"><span class="ln">1047 </span></a>        <span class="s2">elif </span><span class="s0">ep == </span><span class="s6">300</span><span class="s0">:</span>
<a name="l1048"><span class="ln">1048 </span></a>            <span class="s0">agent.epsilon = </span><span class="s6">0.95  </span><span class="s3"># Decay starts faster</span>
<a name="l1049"><span class="ln">1049 </span></a>
<a name="l1050"><span class="ln">1050 </span></a>        <span class="s0">state</span><span class="s5">, </span><span class="s0">_ = env.reset()</span>
<a name="l1051"><span class="ln">1051 </span></a>        <span class="s0">total_reward = </span><span class="s6">0</span>
<a name="l1052"><span class="ln">1052 </span></a>        <span class="s0">done = </span><span class="s2">False</span>
<a name="l1053"><span class="ln">1053 </span></a>
<a name="l1054"><span class="ln">1054 </span></a>        <span class="s2">while not </span><span class="s0">done:</span>
<a name="l1055"><span class="ln">1055 </span></a>            <span class="s0">action = agent.act(state)</span>
<a name="l1056"><span class="ln">1056 </span></a>            <span class="s0">next_state</span><span class="s5">, </span><span class="s0">reward</span><span class="s5">, </span><span class="s0">terminated</span><span class="s5">, </span><span class="s0">truncated</span><span class="s5">, </span><span class="s0">_ = env.step(action)</span>
<a name="l1057"><span class="ln">1057 </span></a>            <span class="s0">done = terminated </span><span class="s2">or </span><span class="s0">truncated</span>
<a name="l1058"><span class="ln">1058 </span></a>
<a name="l1059"><span class="ln">1059 </span></a>            <span class="s0">agent.remember(state</span><span class="s5">, </span><span class="s0">action</span><span class="s5">, </span><span class="s0">reward</span><span class="s5">, </span><span class="s0">next_state</span><span class="s5">, </span><span class="s0">done)</span>
<a name="l1060"><span class="ln">1060 </span></a>            <span class="s0">loss</span><span class="s5">, </span><span class="s0">grad_norm</span><span class="s5">, </span><span class="s0">current_q = agent.replay()  </span><span class="s3"># replay every step</span>
<a name="l1061"><span class="ln">1061 </span></a>
<a name="l1062"><span class="ln">1062 </span></a>            <span class="s0">state = next_state</span>
<a name="l1063"><span class="ln">1063 </span></a>            <span class="s0">total_reward += reward</span>
<a name="l1064"><span class="ln">1064 </span></a>
<a name="l1065"><span class="ln">1065 </span></a>        <span class="s0">rewards.append(total_reward)</span>
<a name="l1066"><span class="ln">1066 </span></a>        <span class="s0">avg_reward = np.mean(rewards[-</span><span class="s6">100</span><span class="s0">:])</span>
<a name="l1067"><span class="ln">1067 </span></a>
<a name="l1068"><span class="ln">1068 </span></a>        <span class="s3"># Early stopping</span>
<a name="l1069"><span class="ln">1069 </span></a>        <span class="s2">if </span><span class="s0">avg_reward &gt; </span><span class="s6">200</span><span class="s0">:</span>
<a name="l1070"><span class="ln">1070 </span></a>            <span class="s0">success_streak += </span><span class="s6">1</span>
<a name="l1071"><span class="ln">1071 </span></a>            <span class="s2">if </span><span class="s0">success_streak &gt;= </span><span class="s6">50</span><span class="s0">:</span>
<a name="l1072"><span class="ln">1072 </span></a>                <span class="s0">print(</span><span class="s4">f&quot;</span><span class="s7">\n</span><span class="s4">Early stopping at episode </span><span class="s7">{</span><span class="s0">ep+</span><span class="s6">1</span><span class="s7">} </span><span class="s4">(avg: </span><span class="s7">{</span><span class="s0">avg_reward</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">)!&quot;</span><span class="s0">)</span>
<a name="l1073"><span class="ln">1073 </span></a>                <span class="s0">torch.save(agent.q_net.state_dict()</span><span class="s5">, </span><span class="s4">&quot;dqn_success.pth&quot;</span><span class="s0">)</span>
<a name="l1074"><span class="ln">1074 </span></a>                <span class="s2">break</span>
<a name="l1075"><span class="ln">1075 </span></a>        <span class="s2">else</span><span class="s0">:</span>
<a name="l1076"><span class="ln">1076 </span></a>            <span class="s0">success_streak = </span><span class="s6">0</span>
<a name="l1077"><span class="ln">1077 </span></a>
<a name="l1078"><span class="ln">1078 </span></a>        <span class="s3"># Checkpoints only after warmup (300 eps)</span>
<a name="l1079"><span class="ln">1079 </span></a>        <span class="s2">if </span><span class="s0">(ep + </span><span class="s6">1</span><span class="s0">) % </span><span class="s6">100 </span><span class="s0">== </span><span class="s6">0 </span><span class="s2">and </span><span class="s0">ep &gt;= </span><span class="s6">300</span><span class="s0">:</span>
<a name="l1080"><span class="ln">1080 </span></a>            <span class="s0">torch.save(agent.q_net.state_dict()</span><span class="s5">, </span><span class="s4">f&quot;dqn_ep</span><span class="s7">{</span><span class="s0">ep+</span><span class="s6">1</span><span class="s7">}</span><span class="s4">.pth&quot;</span><span class="s0">)</span>
<a name="l1081"><span class="ln">1081 </span></a>            <span class="s2">if </span><span class="s0">avg_reward &gt; best_avg:</span>
<a name="l1082"><span class="ln">1082 </span></a>                <span class="s0">best_avg = avg_reward</span>
<a name="l1083"><span class="ln">1083 </span></a>                <span class="s0">torch.save(agent.q_net.state_dict()</span><span class="s5">, </span><span class="s4">&quot;dqn_best.pth&quot;</span><span class="s0">)</span>
<a name="l1084"><span class="ln">1084 </span></a>
<a name="l1085"><span class="ln">1085 </span></a>        <span class="s3"># Monitoring check</span>
<a name="l1086"><span class="ln">1086 </span></a>        <span class="s2">if </span><span class="s0">(ep + </span><span class="s6">1</span><span class="s0">) % </span><span class="s6">50 </span><span class="s0">== </span><span class="s6">0 </span><span class="s2">and </span><span class="s0">loss </span><span class="s2">is not None and </span><span class="s0">current_q </span><span class="s2">is not None</span><span class="s0">:</span>
<a name="l1087"><span class="ln">1087 </span></a>            <span class="s0">print(</span><span class="s4">f&quot;</span><span class="s7">\n</span><span class="s4">Episode </span><span class="s7">{</span><span class="s0">ep+</span><span class="s6">1</span><span class="s7">}</span><span class="s4">:&quot;</span><span class="s0">)</span>
<a name="l1088"><span class="ln">1088 </span></a>            <span class="s0">print(</span><span class="s4">f&quot;Avg Reward (last 100): </span><span class="s7">{</span><span class="s0">avg_reward</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l1089"><span class="ln">1089 </span></a>            <span class="s0">print(</span><span class="s4">f&quot;Loss: </span><span class="s7">{</span><span class="s0">loss</span><span class="s7">:</span><span class="s4">.4f</span><span class="s7">} </span><span class="s4">| Grad Norm: </span><span class="s7">{</span><span class="s0">grad_norm</span><span class="s7">:</span><span class="s4">.2f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l1090"><span class="ln">1090 </span></a>            <span class="s0">print(</span><span class="s4">f&quot;Q-values: </span><span class="s7">{</span><span class="s0">current_q.mean().item()</span><span class="s7">:</span><span class="s4">.2f</span><span class="s7">} </span><span class="s4">± </span><span class="s7">{</span><span class="s0">current_q.std().item()</span><span class="s7">:</span><span class="s4">.2f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l1091"><span class="ln">1091 </span></a>            <span class="s0">print(</span><span class="s4">f&quot;Epsilon: </span><span class="s7">{</span><span class="s0">agent.epsilon</span><span class="s7">:</span><span class="s4">.3f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l1092"><span class="ln">1092 </span></a>
<a name="l1093"><span class="ln">1093 </span></a>        <span class="s0">progress_bar.set_postfix({</span>
<a name="l1094"><span class="ln">1094 </span></a>            <span class="s4">'reward'</span><span class="s0">: </span><span class="s4">f&quot;</span><span class="s7">{</span><span class="s0">total_reward</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s5">,</span>
<a name="l1095"><span class="ln">1095 </span></a>            <span class="s4">'avg'</span><span class="s0">: </span><span class="s4">f&quot;</span><span class="s7">{</span><span class="s0">avg_reward</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s5">,</span>
<a name="l1096"><span class="ln">1096 </span></a>            <span class="s4">'ε'</span><span class="s0">: </span><span class="s4">f&quot;</span><span class="s7">{</span><span class="s0">agent.epsilon</span><span class="s7">:</span><span class="s4">.3f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s5">,</span>
<a name="l1097"><span class="ln">1097 </span></a>            <span class="s4">'grad'</span><span class="s0">: </span><span class="s4">f&quot;</span><span class="s7">{</span><span class="s0">grad_norm</span><span class="s7">:</span><span class="s4">.2f</span><span class="s7">}</span><span class="s4">&quot; </span><span class="s2">if </span><span class="s0">grad_norm </span><span class="s2">else </span><span class="s4">&quot;N/A&quot;</span>
<a name="l1098"><span class="ln">1098 </span></a>        <span class="s0">})</span>
<a name="l1099"><span class="ln">1099 </span></a>
<a name="l1100"><span class="ln">1100 </span></a>    <span class="s0">env.close()</span>
<a name="l1101"><span class="ln">1101 </span></a>    <span class="s2">return </span><span class="s0">rewards</span><span class="s5">, </span><span class="s0">agent</span>
<a name="l1102"><span class="ln">1102 </span></a>
<a name="l1103"><span class="ln">1103 </span></a><span class="s3"># Run training</span>
<a name="l1104"><span class="ln">1104 </span></a><span class="s0">rewards</span><span class="s5">, </span><span class="s0">agent = train_agent()</span>
<a name="l1105"><span class="ln">1105 </span></a>
<a name="l1106"><span class="ln">1106 </span></a><span class="s3"># Plot results</span>
<a name="l1107"><span class="ln">1107 </span></a><span class="s0">plt.figure(figsize=(</span><span class="s6">12</span><span class="s5">, </span><span class="s6">6</span><span class="s0">))</span>
<a name="l1108"><span class="ln">1108 </span></a><span class="s0">plt.plot(rewards</span><span class="s5">, </span><span class="s0">alpha=</span><span class="s6">0.6</span><span class="s0">)</span>
<a name="l1109"><span class="ln">1109 </span></a><span class="s0">plt.plot(pd.Series(rewards).rolling(</span><span class="s6">100</span><span class="s0">).mean()</span><span class="s5">, </span><span class="s0">color=</span><span class="s4">'red'</span><span class="s0">)</span>
<a name="l1110"><span class="ln">1110 </span></a><span class="s0">plt.title(</span><span class="s4">&quot;Double DQN Training Progress - v4&quot;</span><span class="s0">)</span>
<a name="l1111"><span class="ln">1111 </span></a><span class="s0">plt.xlabel(</span><span class="s4">&quot;Episode&quot;</span><span class="s0">)</span>
<a name="l1112"><span class="ln">1112 </span></a><span class="s0">plt.ylabel(</span><span class="s4">&quot;Total Reward&quot;</span><span class="s0">)</span>
<a name="l1113"><span class="ln">1113 </span></a><span class="s0">plt.grid(</span><span class="s2">True</span><span class="s0">)</span>
<a name="l1114"><span class="ln">1114 </span></a><span class="s0">plt.show()</span><hr class="ls0"><a name="l1115"><span class="ln">1115 </span></a><span class="s0">#%% md 
<a name="l1116"><span class="ln">1116 </span></a>### This part is written in hindsight: 
<a name="l1117"><span class="ln">1117 </span></a> 
<a name="l1118"><span class="ln">1118 </span></a>I created tensors from lists of numpy arrays, which is slower than my grandma (she's dead). 
<a name="l1119"><span class="ln">1119 </span></a>``` 
<a name="l1120"><span class="ln">1120 </span></a>states = torch.FloatTensor([e.state for e in experiences]).to(device) 
<a name="l1121"><span class="ln">1121 </span></a>actions = torch.LongTensor([e.action for e in experiences]).to(device) 
<a name="l1122"><span class="ln">1122 </span></a>rewards = torch.FloatTensor([e.reward for e in experiences]).to(device) 
<a name="l1123"><span class="ln">1123 </span></a>next_states = torch.FloatTensor([e.next_state for e in experiences]).to(device) 
<a name="l1124"><span class="ln">1124 </span></a>dones = torch.FloatTensor([e.done for e in experiences]).to(device) 
<a name="l1125"><span class="ln">1125 </span></a>``` 
<a name="l1126"><span class="ln">1126 </span></a> 
<a name="l1127"><span class="ln">1127 </span></a>This resulted in **catastrophically lethargic training**, with 500 episodes taking up to 90 minutes to train. 
<a name="l1128"><span class="ln">1128 </span></a> 
<a name="l1129"><span class="ln">1129 </span></a>The fix was simple, just use np.array() first to create a single numpy array, then convert it to a tensor: 
<a name="l1130"><span class="ln">1130 </span></a>``` 
<a name="l1131"><span class="ln">1131 </span></a>states = torch.FloatTensor(np.array([e.state for e in experiences])).to(device) 
<a name="l1132"><span class="ln">1132 </span></a>actions = torch.LongTensor(np.array([e.action for e in experiences])).to(device) 
<a name="l1133"><span class="ln">1133 </span></a>rewards = torch.FloatTensor(np.array([e.reward for e in experiences])).to(device) 
<a name="l1134"><span class="ln">1134 </span></a>next_states = torch.FloatTensor(np.array([e.next_state for e in experiences])).to(device) 
<a name="l1135"><span class="ln">1135 </span></a>dones = torch.FloatTensor(np.array([e.done for e in experiences])).to(device) 
<a name="l1136"><span class="ln">1136 </span></a>weights = torch.FloatTensor(weights).to(device) 
<a name="l1137"><span class="ln">1137 </span></a>``` 
<a name="l1138"><span class="ln">1138 </span></a> 
<a name="l1139"><span class="ln">1139 </span></a>#### Why did the fix work? 
<a name="l1140"><span class="ln">1140 </span></a>Creating a tensor from a list of numpy arrays forces PyTorch to convert each numpy array individually, so it's like individually microwaving every grain of rice for dinner. 
<a name="l1141"><span class="ln">1141 </span></a> 
<a name="l1142"><span class="ln">1142 </span></a>Instead, wrapping the list in np.array() batches them together into one numpy array, and PyTorch can create the tensor in one *smooooth* operation. 
<a name="l1143"><span class="ln">1143 </span></a> 
<a name="l1144"><span class="ln">1144 </span></a>Plus, for some reason (probably because I've lost my sanity) I did not notice this until it was too late. 
<a name="l1145"><span class="ln">1145 </span></a> 
<a name="l1146"><span class="ln">1146 </span></a>Only after I modified my code for the n-th time trying to fix this severely time-dilating model of mine, this fateful error finally showed up: 
<a name="l1147"><span class="ln">1147 </span></a>``` 
<a name="l1148"><span class="ln">1148 </span></a>UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. 
<a name="l1149"><span class="ln">1149 </span></a>Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. 
<a name="l1150"><span class="ln">1150 </span></a>``` 
<a name="l1151"><span class="ln">1151 </span></a> 
<a name="l1152"><span class="ln">1152 </span></a>#### In short: 
<a name="l1153"><span class="ln">1153 </span></a> 
<a name="l1154"><span class="ln">1154 </span></a>**Problem**: torch.FloatTensor([list_of_numpy_arrays]) is inefficient because PyTorch has to convert each numpy array individually. 
<a name="l1155"><span class="ln">1155 </span></a> 
<a name="l1156"><span class="ln">1156 </span></a>**Solution**: torch.FloatTensor(np.array([list])) first creates a single numpy array, then converts to tensor in one operation. 
<a name="l1157"><span class="ln">1157 </span></a> <hr class="ls0"><a name="l1158"><span class="ln">1158 </span></a>#%% md 
<a name="l1159"><span class="ln">1159 </span></a># 5. PPO Implementation (Policy-based approach) 
<a name="l1160"><span class="ln">1160 </span></a> 
<a name="l1161"><span class="ln">1161 </span></a>## 5.1 Algorithm Overview 
<a name="l1162"><span class="ln">1162 </span></a> 
<a name="l1163"><span class="ln">1163 </span></a>Proximal Policy Optimization (PPO) represents a fundamentally different approach to reinforcement learning compared to value-based methods like DQN. While DQN learns to estimate action values, PPO directly optimizes the policy function that maps states to action probabilities. 
<a name="l1164"><span class="ln">1164 </span></a> 
<a name="l1165"><span class="ln">1165 </span></a>## 5.2 Mathematical Foundation 
<a name="l1166"><span class="ln">1166 </span></a> 
<a name="l1167"><span class="ln">1167 </span></a>### 5.2.1 Policy Gradient Theorem 
<a name="l1168"><span class="ln">1168 </span></a> 
<a name="l1169"><span class="ln">1169 </span></a>PPO builds on the policy gradient theorem: 
<a name="l1170"><span class="ln">1170 </span></a>$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a|s) A^{\pi_\theta}(s,a) \right]$$ 
<a name="l1171"><span class="ln">1171 </span></a> 
<a name="l1172"><span class="ln">1172 </span></a>Where: 
<a name="l1173"><span class="ln">1173 </span></a>- $J(\theta)$ is the expected cumulative reward 
<a name="l1174"><span class="ln">1174 </span></a>- $\pi_\theta(a|s)$ is the policy function 
<a name="l1175"><span class="ln">1175 </span></a>- $A^{\pi_\theta}(s,a)$ is the advantage function 
<a name="l1176"><span class="ln">1176 </span></a> 
<a name="l1177"><span class="ln">1177 </span></a>### 5.2.2 Clipped Surrogate Objective 
<a name="l1178"><span class="ln">1178 </span></a> 
<a name="l1179"><span class="ln">1179 </span></a>PPO's key innovation is the clipped objective function: 
<a name="l1180"><span class="ln">1180 </span></a>$$L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right]$$ 
<a name="l1181"><span class="ln">1181 </span></a> 
<a name="l1182"><span class="ln">1182 </span></a>Where $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ is the probability ratio. 
<a name="l1183"><span class="ln">1183 </span></a> 
<a name="l1184"><span class="ln">1184 </span></a>### 5.2.3 Advantage Estimation (GAE) 
<a name="l1185"><span class="ln">1185 </span></a> 
<a name="l1186"><span class="ln">1186 </span></a>Generalized Advantage Estimation (GAE) is a method to compute the advantage function $A_t$ in reinforcement learning. It uses exponentially-weighted sums of temporal difference (TD) errors $\delta_t$ to reduce variance while maintaining low bias: 
<a name="l1187"><span class="ln">1187 </span></a> 
<a name="l1188"><span class="ln">1188 </span></a>$$ 
<a name="l1189"><span class="ln">1189 </span></a>A_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l} 
<a name="l1190"><span class="ln">1190 </span></a>$$ 
<a name="l1191"><span class="ln">1191 </span></a> 
<a name="l1192"><span class="ln">1192 </span></a>where 
<a name="l1193"><span class="ln">1193 </span></a> 
<a name="l1194"><span class="ln">1194 </span></a>$$ 
<a name="l1195"><span class="ln">1195 </span></a>\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) 
<a name="l1196"><span class="ln">1196 </span></a>$$ 
<a name="l1197"><span class="ln">1197 </span></a> 
<a name="l1198"><span class="ln">1198 </span></a>GAE stabilizes policy updates in Proximal Policy Optimization (PPO) by providing smoother advantage estimates. This leads to more consistent updates and faster convergence, as GAE balances bias-variance tradeoffs in the policy gradient, helping the lander learn better control and landing strategies. 
<a name="l1199"><span class="ln">1199 </span></a> 
<a name="l1200"><span class="ln">1200 </span></a> 
<a name="l1201"><span class="ln">1201 </span></a>## 5.3 Why did I choose PPO for LunarLander? 
<a name="l1202"><span class="ln">1202 </span></a>- **Stability**: Prevents destructive policy updates (thanks Double Dueling DQN for traumatizing me) 
<a name="l1203"><span class="ln">1203 </span></a>- **Sample efficiency**: On-policy but with multiple epochs per batch 
<a name="l1204"><span class="ln">1204 </span></a>- **Continuous control**: Can handle both discrete and continuous actions 
<a name="l1205"><span class="ln">1205 </span></a>- **Robust performance**: Less sensitive to hyperparameters than other policy methods 
<a name="l1206"><span class="ln">1206 </span></a> 
<a name="l1207"><span class="ln">1207 </span></a>## 5.4 Implementation Details 
<a name="l1208"><span class="ln">1208 </span></a> 
<a name="l1209"><span class="ln">1209 </span></a>**Stable-Baselines3 Integration** 
<a name="l1210"><span class="ln">1210 </span></a> 
<a name="l1211"><span class="ln">1211 </span></a>```python 
<a name="l1212"><span class="ln">1212 </span></a>model = PPO( 
<a name="l1213"><span class="ln">1213 </span></a>    &quot;MlpPolicy&quot;, 
<a name="l1214"><span class="ln">1214 </span></a>    train_env, 
<a name="l1215"><span class="ln">1215 </span></a>    learning_rate=3e-4, 
<a name="l1216"><span class="ln">1216 </span></a>    batch_size=64, 
<a name="l1217"><span class="ln">1217 </span></a>    n_steps=2048, 
<a name="l1218"><span class="ln">1218 </span></a>    gamma=0.99, 
<a name="l1219"><span class="ln">1219 </span></a>    gae_lambda=0.95, 
<a name="l1220"><span class="ln">1220 </span></a>    clip_range=0.2, 
<a name="l1221"><span class="ln">1221 </span></a>    policy_kwargs=dict( 
<a name="l1222"><span class="ln">1222 </span></a>        net_arch=dict(pi=[256, 256], vf=[256, 256]) 
<a name="l1223"><span class="ln">1223 </span></a>    ), 
<a name="l1224"><span class="ln">1224 </span></a>    verbose=1, 
<a name="l1225"><span class="ln">1225 </span></a>    device=&quot;cpu&quot; 
<a name="l1226"><span class="ln">1226 </span></a>) 
<a name="l1227"><span class="ln">1227 </span></a>``` 
<a name="l1228"><span class="ln">1228 </span></a> 
<a name="l1229"><span class="ln">1229 </span></a>**Vectorized Environment** 
<a name="l1230"><span class="ln">1230 </span></a> 
<a name="l1231"><span class="ln">1231 </span></a>```python 
<a name="l1232"><span class="ln">1232 </span></a>train_env = make_vec_env(&quot;LunarLander-v3&quot;, n_envs=4) 
<a name="l1233"><span class="ln">1233 </span></a>``` 
<a name="l1234"><span class="ln">1234 </span></a> 
<a name="l1235"><span class="ln">1235 </span></a>- **Parallel environments**: 4 simultaneous LunarLander instances 
<a name="l1236"><span class="ln">1236 </span></a>- **Sample efficiency**: Collects 4x more experience per timestep 
<a name="l1237"><span class="ln">1237 </span></a>- **Variance reduction**: Multiple environments reduce gradient variance 
<a name="l1238"><span class="ln">1238 </span></a> 
<a name="l1239"><span class="ln">1239 </span></a>**Network Architecture** 
<a name="l1240"><span class="ln">1240 </span></a> 
<a name="l1241"><span class="ln">1241 </span></a>```python 
<a name="l1242"><span class="ln">1242 </span></a>policy_kwargs=dict( 
<a name="l1243"><span class="ln">1243 </span></a>    net_arch=dict(pi=[256, 256], vf=[256, 256]) 
<a name="l1244"><span class="ln">1244 </span></a>) 
<a name="l1245"><span class="ln">1245 </span></a>``` 
<a name="l1246"><span class="ln">1246 </span></a> 
<a name="l1247"><span class="ln">1247 </span></a>- **Actor Network (pi)**: -&gt; Policy function π(a|s) 
<a name="l1248"><span class="ln">1248 </span></a>- **Critic Network (vf)**: -&gt; Value function V(s) 
<a name="l1249"><span class="ln">1249 </span></a>- **MlpPolicy**: Stable-Baselines3's multi-layer perceptron policy 
<a name="l1250"><span class="ln">1250 </span></a>- **Separate heads**: Independent final layers for policy and value estimation 
<a name="l1251"><span class="ln">1251 </span></a> 
<a name="l1252"><span class="ln">1252 </span></a> 
<a name="l1253"><span class="ln">1253 </span></a>## 5.5 Key Hyperparameters 
<a name="l1254"><span class="ln">1254 </span></a> 
<a name="l1255"><span class="ln">1255 </span></a>**Learning Rate (3e-4)** 
<a name="l1256"><span class="ln">1256 </span></a> 
<a name="l1257"><span class="ln">1257 </span></a>- Standard PPO learning rate balancing stability and convergence speed 
<a name="l1258"><span class="ln">1258 </span></a>- Higher than DQN due to on-policy nature requiring faster adaptation 
<a name="l1259"><span class="ln">1259 </span></a> 
<a name="l1260"><span class="ln">1260 </span></a>**Batch Size (64) vs. n_steps (2048)** 
<a name="l1261"><span class="ln">1261 </span></a> 
<a name="l1262"><span class="ln">1262 </span></a>- **n_steps**: Trajectory length before policy update (2048 steps per environment) 
<a name="l1263"><span class="ln">1263 </span></a>- **batch_size**: Mini-batch size for gradient computation 
<a name="l1264"><span class="ln">1264 </span></a>- **Total Experience**: 4 envs x 2048 steps = 8192 samples per update 
<a name="l1265"><span class="ln">1265 </span></a> 
<a name="l1266"><span class="ln">1266 </span></a>**GAE Parameters** 
<a name="l1267"><span class="ln">1267 </span></a> 
<a name="l1268"><span class="ln">1268 </span></a>- **gamma (0.99)**: Discount factor for future rewards 
<a name="l1269"><span class="ln">1269 </span></a>- **gae_lambda (0.95)**: Generalized Advantage Estimation parameter 
<a name="l1270"><span class="ln">1270 </span></a>- **Balances**: Bias vs. variance in advantage estimates 
<a name="l1271"><span class="ln">1271 </span></a> 
<a name="l1272"><span class="ln">1272 </span></a>**Clipping Range (0.2)** 
<a name="l1273"><span class="ln">1273 </span></a> 
<a name="l1274"><span class="ln">1274 </span></a>- **Conservative updates**: Prevents destructive policy changes 
<a name="l1275"><span class="ln">1275 </span></a>- **Stability**: Maintains trust region around current policy 
<a name="l1276"><span class="ln">1276 </span></a>- **Typical value**: 0.2 is standard across most PPO applications 
<a name="l1277"><span class="ln">1277 </span></a> 
<a name="l1278"><span class="ln">1278 </span></a>**Training Duration** 
<a name="l1279"><span class="ln">1279 </span></a> 
<a name="l1280"><span class="ln">1280 </span></a>- **total_timesteps**: 200000 timesteps for convergence 
<a name="l1281"><span class="ln">1281 </span></a>- **CPU Training**: Uses CPU device for compatibility and stability 
<a name="l1282"><span class="ln">1282 </span></a> 
<a name="l1283"><span class="ln">1283 </span></a> 
<a name="l1284"><span class="ln">1284 </span></a>## 5.6 Advantages for LunarLander 
<a name="l1285"><span class="ln">1285 </span></a> 
<a name="l1286"><span class="ln">1286 </span></a>**Discrete Action Handling** 
<a name="l1287"><span class="ln">1287 </span></a> 
<a name="l1288"><span class="ln">1288 </span></a>- **Direct Policy Learning**: Maps 8D state space to 4-action probabilities 
<a name="l1289"><span class="ln">1289 </span></a>- **Stochastic Policy**: Natural exploration through action probability distributions 
<a name="l1290"><span class="ln">1290 </span></a>- **No Value Function Approximation Errors**: Policy-based approach avoids Q-value overestimation issues 
<a name="l1291"><span class="ln">1291 </span></a> 
<a name="l1292"><span class="ln">1292 </span></a>**Sample Efficiency** 
<a name="l1293"><span class="ln">1293 </span></a> 
<a name="l1294"><span class="ln">1294 </span></a>- **On-policy Learning**: Uses current policy data efficiently 
<a name="l1295"><span class="ln">1295 </span></a>- **Multiple Epochs**: Reuses collected data for multiple gradient steps 
<a name="l1296"><span class="ln">1296 </span></a>- **Vectorized Collection**: Parallel environments increase sample throughput 
<a name="l1297"><span class="ln">1297 </span></a> 
<a name="l1298"><span class="ln">1298 </span></a>**Stability** 
<a name="l1299"><span class="ln">1299 </span></a> 
<a name="l1300"><span class="ln">1300 </span></a>- **Clipped Updates**: Prevents catastrophic policy degradation 
<a name="l1301"><span class="ln">1301 </span></a>- **Trust Region**: Maintains reasonable policy changes 
<a name="l1302"><span class="ln">1302 </span></a>- **Robust Performance**: Less sensitive to hyperparameter choices than other policy methods 
<a name="l1303"><span class="ln">1303 </span></a> 
<a name="l1304"><span class="ln">1304 </span></a> 
<a name="l1305"><span class="ln">1305 </span></a>## 5.7 Training Process 
<a name="l1306"><span class="ln">1306 </span></a> 
<a name="l1307"><span class="ln">1307 </span></a>**Data Collection Phase** 
<a name="l1308"><span class="ln">1308 </span></a> 
<a name="l1309"><span class="ln">1309 </span></a>1. Run current policy for 2048 steps across 4 environments 
<a name="l1310"><span class="ln">1310 </span></a>2. Collect states, actions, rewards, and value estimates 
<a name="l1311"><span class="ln">1311 </span></a>3. Compute advantage estimates using GAE with λ=0.95 
<a name="l1312"><span class="ln">1312 </span></a> 
<a name="l1313"><span class="ln">1313 </span></a>**Policy Update Phase** 
<a name="l1314"><span class="ln">1314 </span></a> 
<a name="l1315"><span class="ln">1315 </span></a>1. Shuffle collected data into mini-batches of size 64 
<a name="l1316"><span class="ln">1316 </span></a>2. Compute policy and value losses with clipped objectives 
<a name="l1317"><span class="ln">1317 </span></a>3. Perform gradient updates with clipping 
<a name="l1318"><span class="ln">1318 </span></a>4. Repeat for multiple epochs on same data 
<a name="l1319"><span class="ln">1319 </span></a> 
<a name="l1320"><span class="ln">1320 </span></a>**Model Persistence and Loading** 
<a name="l1321"><span class="ln">1321 </span></a> 
<a name="l1322"><span class="ln">1322 </span></a>```python 
<a name="l1323"><span class="ln">1323 </span></a># Save final model 
<a name="l1324"><span class="ln">1324 </span></a>model.save(&quot;ppo_lunar_final&quot;) 
<a name="l1325"><span class="ln">1325 </span></a> 
<a name="l1326"><span class="ln">1326 </span></a># Loading with error handling 
<a name="l1327"><span class="ln">1327 </span></a>ppo_model = PPO.load(&quot;ppofinal.zip&quot;) 
<a name="l1328"><span class="ln">1328 </span></a>``` 
<a name="l1329"><span class="ln">1329 </span></a> 
<a name="l1330"><span class="ln">1330 </span></a>**Evaluation Integration** 
<a name="l1331"><span class="ln">1331 </span></a> 
<a name="l1332"><span class="ln">1332 </span></a>```python 
<a name="l1333"><span class="ln">1333 </span></a>eval_callback = EvalCallback( 
<a name="l1334"><span class="ln">1334 </span></a>    eval_env, 
<a name="l1335"><span class="ln">1335 </span></a>    best_model_save_path=&quot;./ppo_best/&quot;, 
<a name="l1336"><span class="ln">1336 </span></a>    log_path=&quot;./ppo_logs/&quot;, 
<a name="l1337"><span class="ln">1337 </span></a>    eval_freq=5000, 
<a name="l1338"><span class="ln">1338 </span></a>    deterministic=True, 
<a name="l1339"><span class="ln">1339 </span></a>    render=False 
<a name="l1340"><span class="ln">1340 </span></a>) 
<a name="l1341"><span class="ln">1341 </span></a>``` 
<a name="l1342"><span class="ln">1342 </span></a> 
<a name="l1343"><span class="ln">1343 </span></a>- **Regular evaluation**: Every 5000 timesteps during training 
<a name="l1344"><span class="ln">1344 </span></a>- **Deterministic testing**: Uses `deterministic=True` for consistent evaluation 
<a name="l1345"><span class="ln">1345 </span></a>- **Best model saving**: Preserves peak performance automatically 
<a name="l1346"><span class="ln">1346 </span></a>- **Logging**: Tracks performance metrics throughout training 
<a name="l1347"><span class="ln">1347 </span></a> 
<a name="l1348"><span class="ln">1348 </span></a> 
<a name="l1349"><span class="ln">1349 </span></a>## 5.8 Evaluation Protocol 
<a name="l1350"><span class="ln">1350 </span></a> 
<a name="l1351"><span class="ln">1351 </span></a>**Deterministic Policy Execution** 
<a name="l1352"><span class="ln">1352 </span></a> 
<a name="l1353"><span class="ln">1353 </span></a>``` 
<a name="l1354"><span class="ln">1354 </span></a>action, _ = model.predict(obs, deterministic=True) 
<a name="l1355"><span class="ln">1355 </span></a>``` 
<a name="l1356"><span class="ln">1356 </span></a> 
<a name="l1357"><span class="ln">1357 </span></a>- **Deterministic Actions**: Uses mean action instead of sampling for evaluation 
<a name="l1358"><span class="ln">1358 </span></a>- **Consistent Performance**: Eliminates exploration noise during testing 
<a name="l1359"><span class="ln">1359 </span></a>- **Fuel Tracking**: Monitors engine usage (main=1.0, side=0.1 units per activation) 
<a name="l1360"><span class="ln">1360 </span></a> 
<a name="l1361"><span class="ln">1361 </span></a>**Robust Model Loading** 
<a name="l1362"><span class="ln">1362 </span></a> 
<a name="l1363"><span class="ln">1363 </span></a>- **Primary Loading**: Attempts to load from &quot;ppofinal.zip&quot; 
<a name="l1364"><span class="ln">1364 </span></a>- **Fallback Options**: Multiple alternative file names for model recovery 
<a name="l1365"><span class="ln">1365 </span></a>- **Error Handling**: Creates dummy model if loading fails to prevent crashes 
<a name="l1366"><span class="ln">1366 </span></a> <hr class="ls0"><a name="l1367"><span class="ln">1367 </span></a>#%% 
<a name="l1368"><span class="ln">1368 </span></a></span><span class="s3"># This part is used to train the PPO model, not for loading</span>
<a name="l1369"><span class="ln">1369 </span></a><span class="s4">&quot;&quot;&quot;&quot; 
<a name="l1370"><span class="ln">1370 </span></a>def train_ppo_agent(): 
<a name="l1371"><span class="ln">1371 </span></a>    # Create vectorized environment for training 
<a name="l1372"><span class="ln">1372 </span></a>    train_env = make_vec_env(&quot;LunarLander-v3&quot;, n_envs=4) 
<a name="l1373"><span class="ln">1373 </span></a>    eval_env = gym.make(&quot;LunarLander-v3&quot;) 
<a name="l1374"><span class="ln">1374 </span></a> 
<a name="l1375"><span class="ln">1375 </span></a>    # Initialize PPO with optimized hyperparameters 
<a name="l1376"><span class="ln">1376 </span></a>    model = PPO( 
<a name="l1377"><span class="ln">1377 </span></a>        &quot;MlpPolicy&quot;, 
<a name="l1378"><span class="ln">1378 </span></a>        train_env, 
<a name="l1379"><span class="ln">1379 </span></a>        learning_rate=3e-4, 
<a name="l1380"><span class="ln">1380 </span></a>        batch_size=64, 
<a name="l1381"><span class="ln">1381 </span></a>        n_steps=2048, 
<a name="l1382"><span class="ln">1382 </span></a>        gamma=0.99, 
<a name="l1383"><span class="ln">1383 </span></a>        gae_lambda=0.95, 
<a name="l1384"><span class="ln">1384 </span></a>        clip_range=0.2, 
<a name="l1385"><span class="ln">1385 </span></a>        policy_kwargs=dict( 
<a name="l1386"><span class="ln">1386 </span></a>            net_arch=dict(pi=[256, 256], vf=[256, 256]) 
<a name="l1387"><span class="ln">1387 </span></a>        ), 
<a name="l1388"><span class="ln">1388 </span></a>        verbose=1, 
<a name="l1389"><span class="ln">1389 </span></a>        device=&quot;cpu&quot; 
<a name="l1390"><span class="ln">1390 </span></a>    ) 
<a name="l1391"><span class="ln">1391 </span></a> 
<a name="l1392"><span class="ln">1392 </span></a>    # Setup evaluation callback 
<a name="l1393"><span class="ln">1393 </span></a>    eval_callback = EvalCallback( 
<a name="l1394"><span class="ln">1394 </span></a>        eval_env, 
<a name="l1395"><span class="ln">1395 </span></a>        best_model_save_path=&quot;./ppo_best/&quot;, 
<a name="l1396"><span class="ln">1396 </span></a>        log_path=&quot;./ppo_logs/&quot;, 
<a name="l1397"><span class="ln">1397 </span></a>        eval_freq=5000, 
<a name="l1398"><span class="ln">1398 </span></a>        deterministic=True, 
<a name="l1399"><span class="ln">1399 </span></a>        render=False 
<a name="l1400"><span class="ln">1400 </span></a>    ) 
<a name="l1401"><span class="ln">1401 </span></a> 
<a name="l1402"><span class="ln">1402 </span></a>    # Train the model 
<a name="l1403"><span class="ln">1403 </span></a>    print(&quot;Training PPO Agent...&quot;) 
<a name="l1404"><span class="ln">1404 </span></a>    model.learn(total_timesteps=200000, callback=eval_callback) 
<a name="l1405"><span class="ln">1405 </span></a> 
<a name="l1406"><span class="ln">1406 </span></a>    # Save final model 
<a name="l1407"><span class="ln">1407 </span></a>    model.save(&quot;ppo_lunar_final&quot;) 
<a name="l1408"><span class="ln">1408 </span></a> 
<a name="l1409"><span class="ln">1409 </span></a>    # Final evaluation 
<a name="l1410"><span class="ln">1410 </span></a>    mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100) 
<a name="l1411"><span class="ln">1411 </span></a>    print(f&quot;PPO Final Evaluation: {mean_reward:.2f} ± {std_reward:.2f}&quot;) 
<a name="l1412"><span class="ln">1412 </span></a> 
<a name="l1413"><span class="ln">1413 </span></a>    train_env.close() 
<a name="l1414"><span class="ln">1414 </span></a>    eval_env.close() 
<a name="l1415"><span class="ln">1415 </span></a>    return model 
<a name="l1416"><span class="ln">1416 </span></a> 
<a name="l1417"><span class="ln">1417 </span></a> 
<a name="l1418"><span class="ln">1418 </span></a># Train PPO 
<a name="l1419"><span class="ln">1419 </span></a>ppo_model = train_ppo_agent() 
<a name="l1420"><span class="ln">1420 </span></a>&quot;&quot;&quot;</span><hr class="ls0"><a name="l1421"><span class="ln">1421 </span></a><span class="s0">#%% 
<a name="l1422"><span class="ln">1422 </span></a></span><span class="s3"># for the loading, i'll use this</span>
<a name="l1423"><span class="ln">1423 </span></a><span class="s2">from </span><span class="s0">stable_baselines3 </span><span class="s2">import </span><span class="s0">PPO</span>
<a name="l1424"><span class="ln">1424 </span></a><span class="s2">from </span><span class="s0">stable_baselines3.common.evaluation </span><span class="s2">import </span><span class="s0">evaluate_policy</span>
<a name="l1425"><span class="ln">1425 </span></a>
<a name="l1426"><span class="ln">1426 </span></a><span class="s2">try</span><span class="s0">:</span>
<a name="l1427"><span class="ln">1427 </span></a>    <span class="s0">ppo_model = PPO.load(</span><span class="s4">&quot;ppofinal.zip&quot;</span><span class="s0">)</span>
<a name="l1428"><span class="ln">1428 </span></a>    <span class="s0">print(</span><span class="s4">&quot;Successfully loaded existing PPO model from ppofinal.zip&quot;</span><span class="s0">)</span>
<a name="l1429"><span class="ln">1429 </span></a>
<a name="l1430"><span class="ln">1430 </span></a>    <span class="s3"># verify it loaded correctly</span>
<a name="l1431"><span class="ln">1431 </span></a>    <span class="s0">eval_env = gym.make(</span><span class="s4">&quot;LunarLander-v3&quot;</span><span class="s0">)</span>
<a name="l1432"><span class="ln">1432 </span></a>    <span class="s0">mean_reward</span><span class="s5">, </span><span class="s0">std_reward = evaluate_policy(ppo_model</span><span class="s5">, </span><span class="s0">eval_env</span><span class="s5">, </span><span class="s0">n_eval_episodes=</span><span class="s6">10</span><span class="s0">)</span>
<a name="l1433"><span class="ln">1433 </span></a>    <span class="s0">print(</span><span class="s4">f&quot;Loaded PPO Model Evaluation: </span><span class="s7">{</span><span class="s0">mean_reward</span><span class="s7">:</span><span class="s4">.2f</span><span class="s7">} </span><span class="s4">± </span><span class="s7">{</span><span class="s0">std_reward</span><span class="s7">:</span><span class="s4">.2f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l1434"><span class="ln">1434 </span></a>    <span class="s0">eval_env.close()</span>
<a name="l1435"><span class="ln">1435 </span></a>
<a name="l1436"><span class="ln">1436 </span></a><span class="s2">except </span><span class="s0">FileNotFoundError:</span>
<a name="l1437"><span class="ln">1437 </span></a>    <span class="s0">print(</span><span class="s4">&quot;PPO model file not found. Trying alternative names...&quot;</span><span class="s0">)</span>
<a name="l1438"><span class="ln">1438 </span></a>    <span class="s0">alternative_names = [</span><span class="s4">&quot;ppo_lunar_final.zip&quot;</span><span class="s5">, </span><span class="s4">&quot;ppo_best.zip&quot;</span><span class="s5">, </span><span class="s4">&quot;PPO_final.zip&quot;</span><span class="s0">]</span>
<a name="l1439"><span class="ln">1439 </span></a>    <span class="s0">ppo_model = </span><span class="s2">None</span>
<a name="l1440"><span class="ln">1440 </span></a>
<a name="l1441"><span class="ln">1441 </span></a>    <span class="s2">for </span><span class="s0">name </span><span class="s2">in </span><span class="s0">alternative_names:</span>
<a name="l1442"><span class="ln">1442 </span></a>        <span class="s2">try</span><span class="s0">:</span>
<a name="l1443"><span class="ln">1443 </span></a>            <span class="s0">ppo_model = PPO.load(name)</span>
<a name="l1444"><span class="ln">1444 </span></a>            <span class="s0">print(</span><span class="s4">f&quot;Successfully loaded PPO model from </span><span class="s7">{</span><span class="s0">name</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l1445"><span class="ln">1445 </span></a>            <span class="s2">break</span>
<a name="l1446"><span class="ln">1446 </span></a>        <span class="s2">except </span><span class="s0">FileNotFoundError:</span>
<a name="l1447"><span class="ln">1447 </span></a>            <span class="s2">continue</span>
<a name="l1448"><span class="ln">1448 </span></a>
<a name="l1449"><span class="ln">1449 </span></a><span class="s2">except </span><span class="s0">Exception </span><span class="s2">as </span><span class="s0">e:</span>
<a name="l1450"><span class="ln">1450 </span></a>    <span class="s0">print(</span><span class="s4">f&quot;Error loading PPO model: </span><span class="s7">{</span><span class="s0">e</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l1451"><span class="ln">1451 </span></a>    <span class="s0">print(</span><span class="s4">&quot;Creating dummy model...&quot;</span><span class="s0">)</span>
<a name="l1452"><span class="ln">1452 </span></a>    <span class="s0">temp_env = gym.make(</span><span class="s4">&quot;LunarLander-v3&quot;</span><span class="s0">)</span>
<a name="l1453"><span class="ln">1453 </span></a>    <span class="s0">ppo_model = PPO(</span><span class="s4">&quot;MlpPolicy&quot;</span><span class="s5">, </span><span class="s0">temp_env</span><span class="s5">, </span><span class="s0">verbose=</span><span class="s6">0</span><span class="s0">)</span>
<a name="l1454"><span class="ln">1454 </span></a>    <span class="s0">temp_env.close()</span>
<a name="l1455"><span class="ln">1455 </span></a>
<a name="l1456"><span class="ln">1456 </span></a><span class="s0">print(</span><span class="s4">&quot;PPO model ready for evaluation!&quot;</span><span class="s0">)</span><hr class="ls0"><a name="l1457"><span class="ln">1457 </span></a><span class="s0">#%% md 
<a name="l1458"><span class="ln">1458 </span></a># 6. Custom Reward Function Implementation 
<a name="l1459"><span class="ln">1459 </span></a> 
<a name="l1460"><span class="ln">1460 </span></a>## 6.1 Motivation for Reward Shaping 
<a name="l1461"><span class="ln">1461 </span></a> 
<a name="l1462"><span class="ln">1462 </span></a>The original LunarLander-v3 reward function, while functional, may not optimally guide learning toward specific desired behaviors. Custom reward functions allow us to: 
<a name="l1463"><span class="ln">1463 </span></a> 
<a name="l1464"><span class="ln">1464 </span></a>1. **Emphasize Fuel Efficiency**: Encourage more conservative engine usage 
<a name="l1465"><span class="ln">1465 </span></a>2. **Improve Landing Precision**: Reward more accurate landings 
<a name="l1466"><span class="ln">1466 </span></a>3. **Accelerate Learning**: Provide intermediate feedback for better behaviors 
<a name="l1467"><span class="ln">1467 </span></a> 
<a name="l1468"><span class="ln">1468 </span></a>## 6.2 Original Reward Structure Analysis 
<a name="l1469"><span class="ln">1469 </span></a> 
<a name="l1470"><span class="ln">1470 </span></a>**Standard LunarLander-v3 Rewards:** 
<a name="l1471"><span class="ln">1471 </span></a>- **Successful Landing**: +100 to +140 points 
<a name="l1472"><span class="ln">1472 </span></a>- **Crash Landing**: -100 points 
<a name="l1473"><span class="ln">1473 </span></a>- **Main Engine Usage**: -0.3 per frame 
<a name="l1474"><span class="ln">1474 </span></a>- **Side Engine Usage**: -0.03 per frame 
<a name="l1475"><span class="ln">1475 </span></a>- **Leg Ground Contact**: +10 per leg 
<a name="l1476"><span class="ln">1476 </span></a>- **Time Penalty**: Small negative reward per timestep 
<a name="l1477"><span class="ln">1477 </span></a> 
<a name="l1478"><span class="ln">1478 </span></a>## 6.3 Custom Reward Implementations 
<a name="l1479"><span class="ln">1479 </span></a> 
<a name="l1480"><span class="ln">1480 </span></a>### 6.3.1 Fuel Efficiency Wrapper 
<a name="l1481"><span class="ln">1481 </span></a>``` 
<a name="l1482"><span class="ln">1482 </span></a>class FuelEfficiencyWrapper(Wrapper): 
<a name="l1483"><span class="ln">1483 </span></a>def init(self, env, fuel_penalty_multiplier=2.0): 
<a name="l1484"><span class="ln">1484 </span></a>self.fuel_penalty_multiplier = fuel_penalty_multiplier 
<a name="l1485"><span class="ln">1485 </span></a>``` 
<a name="l1486"><span class="ln">1486 </span></a> 
<a name="l1487"><span class="ln">1487 </span></a>**Modifications from Original:** 
<a name="l1488"><span class="ln">1488 </span></a>- **Increased Main Engine Penalty**: -0.3 x (multiplier-1) additional penalty 
<a name="l1489"><span class="ln">1489 </span></a>- **Increased Side Engine Penalty**: -0.03 x (multiplier-1) additional penalty 
<a name="l1490"><span class="ln">1490 </span></a>- **Multiplier Effect**: With multiplier=3.0, penalties become 3x more severe 
<a name="l1491"><span class="ln">1491 </span></a> 
<a name="l1492"><span class="ln">1492 </span></a>**Intent and Expected Behavior:** 
<a name="l1493"><span class="ln">1493 </span></a>- **Conservative Control**: Agent learns to use engines more sparingly 
<a name="l1494"><span class="ln">1494 </span></a>- **Trajectory Optimization**: Encourages ballistic trajectories with minimal corrections 
<a name="l1495"><span class="ln">1495 </span></a>- **Fuel-Optimal Landings**: Prioritizes fuel efficiency over speed 
<a name="l1496"><span class="ln">1496 </span></a>- **Trade-off**: May result in longer episodes but more realistic fuel usage 
<a name="l1497"><span class="ln">1497 </span></a> 
<a name="l1498"><span class="ln">1498 </span></a>**Mathematical Impact:** 
<a name="l1499"><span class="ln">1499 </span></a>- Original: Main engine = -0.3 per frame 
<a name="l1500"><span class="ln">1500 </span></a>- Modified: Main engine = -0.3 - 0.3*(multiplier-1) = -0.3*multiplier 
<a name="l1501"><span class="ln">1501 </span></a>- With multiplier=3.0: Main engine = -0.9 per frame 
<a name="l1502"><span class="ln">1502 </span></a> 
<a name="l1503"><span class="ln">1503 </span></a>### 6.3.2 Landing Precision Bonus Wrapper 
<a name="l1504"><span class="ln">1504 </span></a> 
<a name="l1505"><span class="ln">1505 </span></a>``` 
<a name="l1506"><span class="ln">1506 </span></a>class LandingBonusWrapper(Wrapper): 
<a name="l1507"><span class="ln">1507 </span></a>def init(self, env, precision_bonus=50.0): 
<a name="l1508"><span class="ln">1508 </span></a>self.precision_bonus = precision_bonus 
<a name="l1509"><span class="ln">1509 </span></a>``` 
<a name="l1510"><span class="ln">1510 </span></a> 
<a name="l1511"><span class="ln">1511 </span></a>**Additional Reward Calculation:** 
<a name="l1512"><span class="ln">1512 </span></a> 
<a name="l1513"><span class="ln">1513 </span></a>``` 
<a name="l1514"><span class="ln">1514 </span></a>if leg_left and leg_right and abs(x_vel) &lt; 0.1 and abs(y_vel) &lt; 0.1: 
<a name="l1515"><span class="ln">1515 </span></a>precision_score = max(0, 1 - abs(x_pos) - abs(y_pos)) 
<a name="l1516"><span class="ln">1516 </span></a>velocity_bonus = max(0, 1 - (abs(x_vel) + abs(y_vel))) 
<a name="l1517"><span class="ln">1517 </span></a>reward += self.precision_bonus * precision_score * velocity_bonus 
<a name="l1518"><span class="ln">1518 </span></a>``` 
<a name="l1519"><span class="ln">1519 </span></a> 
<a name="l1520"><span class="ln">1520 </span></a>**Modifications from Original:** 
<a name="l1521"><span class="ln">1521 </span></a>- **Position Precision**: Bonus inversely proportional to distance from center 
<a name="l1522"><span class="ln">1522 </span></a>- **Velocity Precision**: Additional bonus for gentle landings (low velocity) 
<a name="l1523"><span class="ln">1523 </span></a>- **Combined Scoring**: Multiplicative combination rewards both position and velocity precision 
<a name="l1524"><span class="ln">1524 </span></a>- **Threshold Requirements**: Only applies to successful landings (both legs down, low velocity) 
<a name="l1525"><span class="ln">1525 </span></a> 
<a name="l1526"><span class="ln">1526 </span></a>**Intent and Expected Behavior:** 
<a name="l1527"><span class="ln">1527 </span></a>- **Accurate Landings**: Encourages landing closer to the center of the pad 
<a name="l1528"><span class="ln">1528 </span></a>- **Gentle Touchdowns**: Rewards soft landings with minimal impact velocity 
<a name="l1529"><span class="ln">1529 </span></a>- **Quality over Quantity**: Emphasizes landing precision over just successful completion 
<a name="l1530"><span class="ln">1530 </span></a>- **Skill Development**: Promotes more sophisticated control strategies 
<a name="l1531"><span class="ln">1531 </span></a> 
<a name="l1532"><span class="ln">1532 </span></a>**Reward Scaling Analysis:** 
<a name="l1533"><span class="ln">1533 </span></a>- **Maximum Bonus**: 50.0 points (perfect center landing with zero velocity) 
<a name="l1534"><span class="ln">1534 </span></a>- **Position Component**: Linear decay with distance from center 
<a name="l1535"><span class="ln">1535 </span></a>- **Velocity Component**: Linear decay with landing velocity 
<a name="l1536"><span class="ln">1536 </span></a>- **Multiplicative Effect**: Requires both good position AND velocity for full bonus 
<a name="l1537"><span class="ln">1537 </span></a> 
<a name="l1538"><span class="ln">1538 </span></a>## 6.4 Experimental Design Rationale 
<a name="l1539"><span class="ln">1539 </span></a> 
<a name="l1540"><span class="ln">1540 </span></a>**Three-way Comparison:** 
<a name="l1541"><span class="ln">1541 </span></a>1. **Original Rewards**: Baseline performance and behavior 
<a name="l1542"><span class="ln">1542 </span></a>2. **Fuel Efficiency**: Tests impact of conservative control emphasis 
<a name="l1543"><span class="ln">1543 </span></a>3. **Precision Bonus**: Tests impact of landing quality emphasis 
<a name="l1544"><span class="ln">1544 </span></a> 
<a name="l1545"><span class="ln">1545 </span></a>**Expected Outcomes:** 
<a name="l1546"><span class="ln">1546 </span></a>- **Fuel Efficiency**: Lower fuel usage, potentially longer episodes, possible reduction in success rate 
<a name="l1547"><span class="ln">1547 </span></a>- **Precision Bonus**: More accurate landings, potentially higher final rewards, improved landing quality metrics 
<a name="l1548"><span class="ln">1548 </span></a>- **Learning Speed**: Custom rewards may accelerate learning by providing more structured feedback 
<a name="l1549"><span class="ln">1549 </span></a> 
<a name="l1550"><span class="ln">1550 </span></a>**Evaluation Metrics:** 
<a name="l1551"><span class="ln">1551 </span></a>- **Success Rate**: Percentage of successful landings 
<a name="l1552"><span class="ln">1552 </span></a>- **Fuel Usage**: Average engine usage per episode 
<a name="l1553"><span class="ln">1553 </span></a>- **Landing Precision**: Distance from center and landing velocity 
<a name="l1554"><span class="ln">1554 </span></a>- **Training Efficiency**: Episodes required to reach performance thresholds 
<a name="l1555"><span class="ln">1555 </span></a> 
<a name="l1556"><span class="ln">1556 </span></a>## 6.5 Implementation Benefits 
<a name="l1557"><span class="ln">1557 </span></a> 
<a name="l1558"><span class="ln">1558 </span></a>**Modular Design**: Wrapper pattern allows easy combination and modification of reward functions 
<a name="l1559"><span class="ln">1559 </span></a> 
<a name="l1560"><span class="ln">1560 </span></a>**Backward Compatibility**: Original environment behavior preserved, only rewards modified 
<a name="l1561"><span class="ln">1561 </span></a> 
<a name="l1562"><span class="ln">1562 </span></a>**Flexible Parameters**: Multipliers and bonus values easily adjustable for fine-tuning 
<a name="l1563"><span class="ln">1563 </span></a> 
<a name="l1564"><span class="ln">1564 </span></a>**Research Value**: Enables systematic study of reward shaping effects on learning and behavior <hr class="ls0"><a name="l1565"><span class="ln">1565 </span></a>#%% 
<a name="l1566"><span class="ln">1566 </span></a></span><span class="s2">import </span><span class="s0">gymnasium </span><span class="s2">as </span><span class="s0">gym</span>
<a name="l1567"><span class="ln">1567 </span></a><span class="s2">from </span><span class="s0">gymnasium </span><span class="s2">import </span><span class="s0">Wrapper</span>
<a name="l1568"><span class="ln">1568 </span></a>
<a name="l1569"><span class="ln">1569 </span></a><span class="s2">class </span><span class="s0">FuelEfficiencyWrapper(Wrapper):</span>
<a name="l1570"><span class="ln">1570 </span></a>    <span class="s8">&quot;&quot;&quot;Custom reward wrapper that penalizes fuel usage more heavily&quot;&quot;&quot;</span>
<a name="l1571"><span class="ln">1571 </span></a>
<a name="l1572"><span class="ln">1572 </span></a>    <span class="s2">def </span><span class="s0">__init__(self</span><span class="s5">, </span><span class="s0">env</span><span class="s5">, </span><span class="s0">fuel_penalty_multiplier=</span><span class="s6">2.0</span><span class="s0">):</span>
<a name="l1573"><span class="ln">1573 </span></a>        <span class="s0">super().__init__(env)</span>
<a name="l1574"><span class="ln">1574 </span></a>        <span class="s0">self.fuel_penalty_multiplier = fuel_penalty_multiplier</span>
<a name="l1575"><span class="ln">1575 </span></a>        <span class="s0">self.original_reward = </span><span class="s6">0</span>
<a name="l1576"><span class="ln">1576 </span></a>        <span class="s0">self.custom_reward = </span><span class="s6">0</span>
<a name="l1577"><span class="ln">1577 </span></a>
<a name="l1578"><span class="ln">1578 </span></a>    <span class="s2">def </span><span class="s0">step(self</span><span class="s5">, </span><span class="s0">action):</span>
<a name="l1579"><span class="ln">1579 </span></a>        <span class="s0">obs</span><span class="s5">, </span><span class="s0">reward</span><span class="s5">, </span><span class="s0">terminated</span><span class="s5">, </span><span class="s0">truncated</span><span class="s5">, </span><span class="s0">info = self.env.step(action)</span>
<a name="l1580"><span class="ln">1580 </span></a>
<a name="l1581"><span class="ln">1581 </span></a>        <span class="s3"># Store original reward</span>
<a name="l1582"><span class="ln">1582 </span></a>        <span class="s0">self.original_reward = reward</span>
<a name="l1583"><span class="ln">1583 </span></a>
<a name="l1584"><span class="ln">1584 </span></a>        <span class="s3"># Apply custom fuel penalty</span>
<a name="l1585"><span class="ln">1585 </span></a>        <span class="s2">if </span><span class="s0">action == </span><span class="s6">2</span><span class="s0">:  </span><span class="s3"># Main engine</span>
<a name="l1586"><span class="ln">1586 </span></a>            <span class="s0">reward -= </span><span class="s6">0.3 </span><span class="s0">* (self.fuel_penalty_multiplier - </span><span class="s6">1</span><span class="s0">)</span>
<a name="l1587"><span class="ln">1587 </span></a>        <span class="s2">elif </span><span class="s0">action </span><span class="s2">in </span><span class="s0">[</span><span class="s6">1</span><span class="s5">, </span><span class="s6">3</span><span class="s0">]:  </span><span class="s3"># Side engines</span>
<a name="l1588"><span class="ln">1588 </span></a>            <span class="s0">reward -= </span><span class="s6">0.03 </span><span class="s0">* (self.fuel_penalty_multiplier - </span><span class="s6">1</span><span class="s0">)</span>
<a name="l1589"><span class="ln">1589 </span></a>
<a name="l1590"><span class="ln">1590 </span></a>        <span class="s0">self.custom_reward = reward</span>
<a name="l1591"><span class="ln">1591 </span></a>        <span class="s2">return </span><span class="s0">obs</span><span class="s5">, </span><span class="s0">reward</span><span class="s5">, </span><span class="s0">terminated</span><span class="s5">, </span><span class="s0">truncated</span><span class="s5">, </span><span class="s0">info</span>
<a name="l1592"><span class="ln">1592 </span></a>
<a name="l1593"><span class="ln">1593 </span></a>
<a name="l1594"><span class="ln">1594 </span></a><span class="s2">class </span><span class="s0">LandingBonusWrapper(Wrapper):</span>
<a name="l1595"><span class="ln">1595 </span></a>    <span class="s8">&quot;&quot;&quot;Provides additional bonus for precise landings&quot;&quot;&quot;</span>
<a name="l1596"><span class="ln">1596 </span></a>
<a name="l1597"><span class="ln">1597 </span></a>    <span class="s2">def </span><span class="s0">__init__(self</span><span class="s5">, </span><span class="s0">env</span><span class="s5">, </span><span class="s0">precision_bonus=</span><span class="s6">50.0</span><span class="s0">):</span>
<a name="l1598"><span class="ln">1598 </span></a>        <span class="s0">super().__init__(env)</span>
<a name="l1599"><span class="ln">1599 </span></a>        <span class="s0">self.precision_bonus = precision_bonus</span>
<a name="l1600"><span class="ln">1600 </span></a>
<a name="l1601"><span class="ln">1601 </span></a>    <span class="s2">def </span><span class="s0">step(self</span><span class="s5">, </span><span class="s0">action):</span>
<a name="l1602"><span class="ln">1602 </span></a>        <span class="s0">obs</span><span class="s5">, </span><span class="s0">reward</span><span class="s5">, </span><span class="s0">terminated</span><span class="s5">, </span><span class="s0">truncated</span><span class="s5">, </span><span class="s0">info = self.env.step(action)</span>
<a name="l1603"><span class="ln">1603 </span></a>
<a name="l1604"><span class="ln">1604 </span></a>        <span class="s2">if </span><span class="s0">terminated:</span>
<a name="l1605"><span class="ln">1605 </span></a>            <span class="s3"># Check if landed successfully (both legs touching, low velocity)</span>
<a name="l1606"><span class="ln">1606 </span></a>            <span class="s0">x_pos</span><span class="s5">, </span><span class="s0">y_pos</span><span class="s5">, </span><span class="s0">x_vel</span><span class="s5">, </span><span class="s0">y_vel</span><span class="s5">, </span><span class="s0">angle</span><span class="s5">, </span><span class="s0">ang_vel</span><span class="s5">, </span><span class="s0">leg_left</span><span class="s5">, </span><span class="s0">leg_right = obs</span>
<a name="l1607"><span class="ln">1607 </span></a>
<a name="l1608"><span class="ln">1608 </span></a>            <span class="s2">if </span><span class="s0">leg_left </span><span class="s2">and </span><span class="s0">leg_right </span><span class="s2">and </span><span class="s0">abs(x_vel) &lt; </span><span class="s6">0.1 </span><span class="s2">and </span><span class="s0">abs(y_vel) &lt; </span><span class="s6">0.1</span><span class="s0">:</span>
<a name="l1609"><span class="ln">1609 </span></a>                <span class="s3"># Bonus for precision (closer to center, lower velocity)</span>
<a name="l1610"><span class="ln">1610 </span></a>                <span class="s0">precision_score = max(</span><span class="s6">0</span><span class="s5">, </span><span class="s6">1 </span><span class="s0">- abs(x_pos) - abs(y_pos))</span>
<a name="l1611"><span class="ln">1611 </span></a>                <span class="s0">velocity_bonus = max(</span><span class="s6">0</span><span class="s5">, </span><span class="s6">1 </span><span class="s0">- (abs(x_vel) + abs(y_vel)))</span>
<a name="l1612"><span class="ln">1612 </span></a>                <span class="s0">reward += self.precision_bonus * precision_score * velocity_bonus</span>
<a name="l1613"><span class="ln">1613 </span></a>
<a name="l1614"><span class="ln">1614 </span></a>        <span class="s2">return </span><span class="s0">obs</span><span class="s5">, </span><span class="s0">reward</span><span class="s5">, </span><span class="s0">terminated</span><span class="s5">, </span><span class="s0">truncated</span><span class="s5">, </span><span class="s0">info</span>
<a name="l1615"><span class="ln">1615 </span></a>
<a name="l1616"><span class="ln">1616 </span></a>
<a name="l1617"><span class="ln">1617 </span></a><span class="s3"># Train agents with custom rewards</span>
<a name="l1618"><span class="ln">1618 </span></a><span class="s2">def </span><span class="s0">train_custom_reward_agents():</span>
<a name="l1619"><span class="ln">1619 </span></a>    <span class="s0">results = {}</span>
<a name="l1620"><span class="ln">1620 </span></a>
<a name="l1621"><span class="ln">1621 </span></a>    <span class="s3"># Original reward</span>
<a name="l1622"><span class="ln">1622 </span></a>    <span class="s0">env_original = gym.make(</span><span class="s4">&quot;LunarLander-v3&quot;</span><span class="s0">)</span>
<a name="l1623"><span class="ln">1623 </span></a>    <span class="s0">agent_original = DoubleDQNAgent(env_original)</span>
<a name="l1624"><span class="ln">1624 </span></a>    <span class="s0">print(</span><span class="s4">&quot;Training with original rewards...&quot;</span><span class="s0">)</span>
<a name="l1625"><span class="ln">1625 </span></a>    <span class="s0">rewards_original = train_agent_custom(agent_original</span><span class="s5">, </span><span class="s0">env_original</span><span class="s5">, </span><span class="s0">episodes=</span><span class="s6">1000</span><span class="s0">)</span>
<a name="l1626"><span class="ln">1626 </span></a>    <span class="s0">results[</span><span class="s4">'Original'</span><span class="s0">] = rewards_original</span>
<a name="l1627"><span class="ln">1627 </span></a>
<a name="l1628"><span class="ln">1628 </span></a>    <span class="s3"># Fuel efficiency penalty</span>
<a name="l1629"><span class="ln">1629 </span></a>    <span class="s0">env_fuel = FuelEfficiencyWrapper(gym.make(</span><span class="s4">&quot;LunarLander-v3&quot;</span><span class="s0">)</span><span class="s5">, </span><span class="s0">fuel_penalty_multiplier=</span><span class="s6">3.0</span><span class="s0">)</span>
<a name="l1630"><span class="ln">1630 </span></a>    <span class="s0">agent_fuel = DoubleDQNAgent(env_fuel)</span>
<a name="l1631"><span class="ln">1631 </span></a>    <span class="s0">print(</span><span class="s4">&quot;Training with fuel efficiency penalty...&quot;</span><span class="s0">)</span>
<a name="l1632"><span class="ln">1632 </span></a>    <span class="s0">rewards_fuel = train_agent_custom(agent_fuel</span><span class="s5">, </span><span class="s0">env_fuel</span><span class="s5">, </span><span class="s0">episodes=</span><span class="s6">1000</span><span class="s0">)</span>
<a name="l1633"><span class="ln">1633 </span></a>    <span class="s0">results[</span><span class="s4">'Fuel Efficient'</span><span class="s0">] = rewards_fuel</span>
<a name="l1634"><span class="ln">1634 </span></a>
<a name="l1635"><span class="ln">1635 </span></a>    <span class="s3"># Landing precision bonus</span>
<a name="l1636"><span class="ln">1636 </span></a>    <span class="s0">env_precision = LandingBonusWrapper(gym.make(</span><span class="s4">&quot;LunarLander-v3&quot;</span><span class="s0">)</span><span class="s5">, </span><span class="s0">precision_bonus=</span><span class="s6">100.0</span><span class="s0">)</span>
<a name="l1637"><span class="ln">1637 </span></a>    <span class="s0">agent_precision = DoubleDQNAgent(env_precision)</span>
<a name="l1638"><span class="ln">1638 </span></a>    <span class="s0">print(</span><span class="s4">&quot;Training with precision bonus...&quot;</span><span class="s0">)</span>
<a name="l1639"><span class="ln">1639 </span></a>    <span class="s0">rewards_precision = train_agent_custom(agent_precision</span><span class="s5">, </span><span class="s0">env_precision</span><span class="s5">, </span><span class="s0">episodes=</span><span class="s6">1000</span><span class="s0">)</span>
<a name="l1640"><span class="ln">1640 </span></a>    <span class="s0">results[</span><span class="s4">'Precision Bonus'</span><span class="s0">] = rewards_precision</span>
<a name="l1641"><span class="ln">1641 </span></a>
<a name="l1642"><span class="ln">1642 </span></a>    <span class="s2">return </span><span class="s0">results</span>
<a name="l1643"><span class="ln">1643 </span></a>
<a name="l1644"><span class="ln">1644 </span></a>
<a name="l1645"><span class="ln">1645 </span></a><span class="s2">def </span><span class="s0">train_agent_custom(agent</span><span class="s5">, </span><span class="s0">env</span><span class="s5">, </span><span class="s0">episodes=</span><span class="s6">1000</span><span class="s0">):</span>
<a name="l1646"><span class="ln">1646 </span></a>    <span class="s0">rewards = []</span>
<a name="l1647"><span class="ln">1647 </span></a>    <span class="s2">for </span><span class="s0">ep </span><span class="s2">in </span><span class="s0">trange(episodes</span><span class="s5">, </span><span class="s0">desc=</span><span class="s4">&quot;Training&quot;</span><span class="s0">):</span>
<a name="l1648"><span class="ln">1648 </span></a>        <span class="s0">state</span><span class="s5">, </span><span class="s0">_ = env.reset()</span>
<a name="l1649"><span class="ln">1649 </span></a>        <span class="s0">total_reward = </span><span class="s6">0</span>
<a name="l1650"><span class="ln">1650 </span></a>        <span class="s0">done = </span><span class="s2">False</span>
<a name="l1651"><span class="ln">1651 </span></a>
<a name="l1652"><span class="ln">1652 </span></a>        <span class="s2">while not </span><span class="s0">done:</span>
<a name="l1653"><span class="ln">1653 </span></a>            <span class="s0">action = agent.act(state)</span>
<a name="l1654"><span class="ln">1654 </span></a>            <span class="s0">next_state</span><span class="s5">, </span><span class="s0">reward</span><span class="s5">, </span><span class="s0">terminated</span><span class="s5">, </span><span class="s0">truncated</span><span class="s5">, </span><span class="s0">_ = env.step(action)</span>
<a name="l1655"><span class="ln">1655 </span></a>            <span class="s0">done = terminated </span><span class="s2">or </span><span class="s0">truncated</span>
<a name="l1656"><span class="ln">1656 </span></a>
<a name="l1657"><span class="ln">1657 </span></a>            <span class="s0">agent.remember(state</span><span class="s5">, </span><span class="s0">action</span><span class="s5">, </span><span class="s0">reward</span><span class="s5">, </span><span class="s0">next_state</span><span class="s5">, </span><span class="s0">done)</span>
<a name="l1658"><span class="ln">1658 </span></a>            <span class="s0">agent.replay()</span>
<a name="l1659"><span class="ln">1659 </span></a>
<a name="l1660"><span class="ln">1660 </span></a>            <span class="s0">state = next_state</span>
<a name="l1661"><span class="ln">1661 </span></a>            <span class="s0">total_reward += reward</span>
<a name="l1662"><span class="ln">1662 </span></a>
<a name="l1663"><span class="ln">1663 </span></a>        <span class="s0">rewards.append(total_reward)</span>
<a name="l1664"><span class="ln">1664 </span></a>
<a name="l1665"><span class="ln">1665 </span></a>        <span class="s2">if </span><span class="s0">(ep + </span><span class="s6">1</span><span class="s0">) % </span><span class="s6">100 </span><span class="s0">== </span><span class="s6">0</span><span class="s0">:</span>
<a name="l1666"><span class="ln">1666 </span></a>            <span class="s0">avg_reward = np.mean(rewards[-</span><span class="s6">100</span><span class="s0">:])</span>
<a name="l1667"><span class="ln">1667 </span></a>            <span class="s0">print(</span><span class="s4">f&quot;Episode </span><span class="s7">{</span><span class="s0">ep + </span><span class="s6">1</span><span class="s7">}</span><span class="s4">, Avg Reward: </span><span class="s7">{</span><span class="s0">avg_reward</span><span class="s7">:</span><span class="s4">.2f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l1668"><span class="ln">1668 </span></a>
<a name="l1669"><span class="ln">1669 </span></a>    <span class="s0">env.close()</span>
<a name="l1670"><span class="ln">1670 </span></a>    <span class="s2">return </span><span class="s0">rewards</span>
<a name="l1671"><span class="ln">1671 </span></a>
<a name="l1672"><span class="ln">1672 </span></a>
<a name="l1673"><span class="ln">1673 </span></a><span class="s3"># Run custom reward training</span>
<a name="l1674"><span class="ln">1674 </span></a><span class="s0">custom_results = train_custom_reward_agents()</span>
<a name="l1675"><span class="ln">1675 </span></a><hr class="ls0"><a name="l1676"><span class="ln">1676 </span></a><span class="s0">#%% md 
<a name="l1677"><span class="ln">1677 </span></a># 7. Hyperparameter Analysis 
<a name="l1678"><span class="ln">1678 </span></a>## 7.1 Learning Rate Comparison 
<a name="l1679"><span class="ln">1679 </span></a> 
<a name="l1680"><span class="ln">1680 </span></a>Learning rate controls the magnitude of parameter updates during training. Three values were tested: 0.0001, 0.0005, and 0.001. 
<a name="l1681"><span class="ln">1681 </span></a> 
<a name="l1682"><span class="ln">1682 </span></a>**Expected Impact:** 
<a name="l1683"><span class="ln">1683 </span></a> 
<a name="l1684"><span class="ln">1684 </span></a>- **Lower rates (0.0001)**: Stable but slow convergence 
<a name="l1685"><span class="ln">1685 </span></a>- **Higher rates (0.001)**: Faster initial learning but potential instability 
<a name="l1686"><span class="ln">1686 </span></a>- **Medium rates (0.0005)**: Balance between speed and stability 
<a name="l1687"><span class="ln">1687 </span></a> 
<a name="l1688"><span class="ln">1688 </span></a>**Experimental Results:** 
<a name="l1689"><span class="ln">1689 </span></a>Based on the 550-episode training experiments, the learning rate comparison revealed significant performance differences: 
<a name="l1690"><span class="ln">1690 </span></a> 
<a name="l1691"><span class="ln">1691 </span></a>- **lr = 0.0001**: Demonstrated the most stable convergence with consistent improvement over episodes. This conservative learning rate prevented gradient explosion and maintained steady policy refinement. 
<a name="l1692"><span class="ln">1692 </span></a>- **lr = 0.0005**: Showed faster initial learning but exhibited more volatility in performance, with occasional dips in the rolling average reward. 
<a name="l1693"><span class="ln">1693 </span></a>- **lr = 0.001**: Displayed rapid early improvements but suffered from training instability, with frequent oscillations and potential overshooting of optimal policy parameters. 
<a name="l1694"><span class="ln">1694 </span></a> 
<a name="l1695"><span class="ln">1695 </span></a>**Key Findings:** 
<a name="l1696"><span class="ln">1696 </span></a>The conservative learning rate of 0.0001 proved most effective for the LunarLander environment, aligning with research showing that excessive learning rates can cause networks to overshoot minima and prevent convergence. 
<a name="l1697"><span class="ln">1697 </span></a> 
<a name="l1698"><span class="ln">1698 </span></a>## 7.2 Discount Factor (γ) Comparison 
<a name="l1699"><span class="ln">1699 </span></a> 
<a name="l1700"><span class="ln">1700 </span></a>Gamma determines how much future rewards are valued relative to immediate rewards. Values tested: 0.95, 0.99, 0.995. 
<a name="l1701"><span class="ln">1701 </span></a> 
<a name="l1702"><span class="ln">1702 </span></a>**Theoretical Impact:** 
<a name="l1703"><span class="ln">1703 </span></a> 
<a name="l1704"><span class="ln">1704 </span></a>- **γ = 0.95**: Emphasizes immediate rewards, shorter planning horizon 
<a name="l1705"><span class="ln">1705 </span></a>- **γ = 0.99**: Standard value balancing immediate and future rewards 
<a name="l1706"><span class="ln">1706 </span></a>- **γ = 0.995**: Strong emphasis on long-term planning, critical for landing tasks 
<a name="l1707"><span class="ln">1707 </span></a> 
<a name="l1708"><span class="ln">1708 </span></a>**Experimental Results:** 
<a name="l1709"><span class="ln">1709 </span></a>The discount factor experiments over 550 episodes revealed critical insights for the LunarLander task: 
<a name="l1710"><span class="ln">1710 </span></a> 
<a name="l1711"><span class="ln">1711 </span></a>- **γ = 0.95**: Failed to achieve consistent landing success. The short planning horizon prevented the agent from properly crediting the eventual landing reward to the trajectory planning actions taken earlier in the episode. The agent learned basic flight control but struggled with landing coordination. 
<a name="l1712"><span class="ln">1712 </span></a>- **γ = 0.99**: Achieved optimal performance with the best balance between immediate fuel efficiency and long-term landing success. This value enabled proper temporal credit assignment while maintaining urgency for timely landings. 
<a name="l1713"><span class="ln">1713 </span></a>- **γ = 0.995**: While enabling long-term planning, this high discount factor led to inefficient behaviors where the agent could maintain flight indefinitely without pressure to land quickly, resulting in longer episodes and suboptimal fuel usage. 
<a name="l1714"><span class="ln">1714 </span></a> 
<a name="l1715"><span class="ln">1715 </span></a>**Temporal Credit Assignment Analysis:** 
<a name="l1716"><span class="ln">1716 </span></a>The LunarLander environment requires coordinating actions across variable episode lengths (typically 200-400 steps). A discount factor of 0.99 provides the optimal temporal horizon, allowing rewards from successful landings to propagate back approximately 100 steps while maintaining sufficient urgency for efficient landing strategies. 
<a name="l1717"><span class="ln">1717 </span></a> 
<a name="l1718"><span class="ln">1718 </span></a>## 7.3 Experimental Design 
<a name="l1719"><span class="ln">1719 </span></a> 
<a name="l1720"><span class="ln">1720 </span></a>Each hyperparameter configuration was trained for 550 episodes to observe convergence patterns. Rolling averages (50-episode window) smoothed out noise for clearer comparison of learning trajectories. The experiments used identical network architectures, replay buffer settings, and exploration strategies to isolate hyperparameter effects. 
<a name="l1721"><span class="ln">1721 </span></a> 
<a name="l1722"><span class="ln">1722 </span></a>## 7.4 Final Implementation Choices 
<a name="l1723"><span class="ln">1723 </span></a> 
<a name="l1724"><span class="ln">1724 </span></a>Based on the experimental results, the final DDQN v4 implementation adopted the following &quot;battle-tested proven hyperparameters&quot;: 
<a name="l1725"><span class="ln">1725 </span></a> 
<a name="l1726"><span class="ln">1726 </span></a>**Selected Learning Rate: 0.0001** 
<a name="l1727"><span class="ln">1727 </span></a> 
<a name="l1728"><span class="ln">1728 </span></a>- Chosen for its superior stability and consistent convergence 
<a name="l1729"><span class="ln">1729 </span></a>- Prevents gradient explosion issues that plagued higher learning rates 
<a name="l1730"><span class="ln">1730 </span></a>- Enables fine-grained policy refinement in the continuous state space 
<a name="l1731"><span class="ln">1731 </span></a>- Aligns with successful DQN implementations in similar environments 
<a name="l1732"><span class="ln">1732 </span></a> 
<a name="l1733"><span class="ln">1733 </span></a>**Selected Discount Factor: 0.99** 
<a name="l1734"><span class="ln">1734 </span></a> 
<a name="l1735"><span class="ln">1735 </span></a>- Optimal balance between immediate and future reward consideration 
<a name="l1736"><span class="ln">1736 </span></a>- Enables proper temporal credit assignment for landing sequences 
<a name="l1737"><span class="ln">1737 </span></a>- Maintains sufficient urgency for fuel-efficient trajectories 
<a name="l1738"><span class="ln">1738 </span></a>- Prevents both myopic behavior (γ=0.95) and excessive patience (γ=0.995) 
<a name="l1739"><span class="ln">1739 </span></a> 
<a name="l1740"><span class="ln">1740 </span></a> 
<a name="l1741"><span class="ln">1741 </span></a>## 7.5 Hyperparameter Interdependencies 
<a name="l1742"><span class="ln">1742 </span></a> 
<a name="l1743"><span class="ln">1743 </span></a>The experiments revealed important interactions between learning rate and discount factor: 
<a name="l1744"><span class="ln">1744 </span></a> 
<a name="l1745"><span class="ln">1745 </span></a>**Conservative Learning + High Gamma (0.0001, 0.995)**: Stable but inefficient learning with prolonged episodes 
<a name="l1746"><span class="ln">1746 </span></a> 
<a name="l1747"><span class="ln">1747 </span></a>**Aggressive Learning + Low Gamma (0.001, 0.95)**: Fast initial progress but poor final performance due to instability and short-sighted planning 
<a name="l1748"><span class="ln">1748 </span></a> 
<a name="l1749"><span class="ln">1749 </span></a>**Optimal Combination (0.0001, 0.99)**: Achieved the best performance by combining stable parameter updates with appropriate temporal planning horizons 
<a name="l1750"><span class="ln">1750 </span></a> 
<a name="l1751"><span class="ln">1751 </span></a>## 7.6 Convergence Analysis 
<a name="l1752"><span class="ln">1752 </span></a> 
<a name="l1753"><span class="ln">1753 </span></a>The winning hyperparameter combination (lr=0.0001, γ=0.99) demonstrated: 
<a name="l1754"><span class="ln">1754 </span></a> 
<a name="l1755"><span class="ln">1755 </span></a>- **Convergence Speed**: Reached 200+ average reward around episode 450-500 
<a name="l1756"><span class="ln">1756 </span></a>- **Stability**: Minimal variance in rolling averages after convergence 
<a name="l1757"><span class="ln">1757 </span></a>- **Final Performance**: Sustained performance above the 200-point &quot;solved&quot; threshold 
<a name="l1758"><span class="ln">1758 </span></a>- **Fuel Efficiency**: Maintained conservative engine usage patterns throughout training 
<a name="l1759"><span class="ln">1759 </span></a> 
<a name="l1760"><span class="ln">1760 </span></a>This analysis confirms that hyperparameter selection is crucial for reinforcement learning success, with the optimal values being environment-specific and requiring empirical validation through systematic experimentation. <hr class="ls0"><a name="l1761"><span class="ln">1761 </span></a>#%% 
<a name="l1762"><span class="ln">1762 </span></a></span><span class="s2">def </span><span class="s0">compare_hyperparameters():</span>
<a name="l1763"><span class="ln">1763 </span></a>    <span class="s3"># Learning rate comparison</span>
<a name="l1764"><span class="ln">1764 </span></a>    <span class="s0">learning_rates = [</span><span class="s6">0.0001</span><span class="s5">, </span><span class="s6">0.0005</span><span class="s5">, </span><span class="s6">0.001</span><span class="s0">]</span>
<a name="l1765"><span class="ln">1765 </span></a>    <span class="s0">lr_results = {}</span>
<a name="l1766"><span class="ln">1766 </span></a>
<a name="l1767"><span class="ln">1767 </span></a>    <span class="s2">for </span><span class="s0">lr </span><span class="s2">in </span><span class="s0">learning_rates:</span>
<a name="l1768"><span class="ln">1768 </span></a>        <span class="s0">print(</span><span class="s4">f&quot;Testing learning rate: </span><span class="s7">{</span><span class="s0">lr</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l1769"><span class="ln">1769 </span></a>        <span class="s0">env = gym.make(</span><span class="s4">&quot;LunarLander-v3&quot;</span><span class="s0">)</span>
<a name="l1770"><span class="ln">1770 </span></a>        <span class="s0">agent = DoubleDQNAgent(env)</span>
<a name="l1771"><span class="ln">1771 </span></a>        <span class="s0">agent.optimizer = optim.Adam(agent.q_net.parameters()</span><span class="s5">, </span><span class="s0">lr=lr</span><span class="s5">, </span><span class="s0">weight_decay=</span><span class="s6">1e-5</span><span class="s0">)</span>
<a name="l1772"><span class="ln">1772 </span></a>
<a name="l1773"><span class="ln">1773 </span></a>        <span class="s0">rewards = train_agent_custom(agent</span><span class="s5">, </span><span class="s0">env</span><span class="s5">, </span><span class="s0">episodes=</span><span class="s6">550</span><span class="s0">)</span>
<a name="l1774"><span class="ln">1774 </span></a>        <span class="s0">lr_results[lr] = rewards</span>
<a name="l1775"><span class="ln">1775 </span></a>
<a name="l1776"><span class="ln">1776 </span></a>    <span class="s3"># Discount factor comparison</span>
<a name="l1777"><span class="ln">1777 </span></a>    <span class="s0">gammas = [</span><span class="s6">0.95</span><span class="s5">, </span><span class="s6">0.99</span><span class="s5">, </span><span class="s6">0.995</span><span class="s0">]</span>
<a name="l1778"><span class="ln">1778 </span></a>    <span class="s0">gamma_results = {}</span>
<a name="l1779"><span class="ln">1779 </span></a>
<a name="l1780"><span class="ln">1780 </span></a>    <span class="s2">for </span><span class="s0">gamma </span><span class="s2">in </span><span class="s0">gammas:</span>
<a name="l1781"><span class="ln">1781 </span></a>        <span class="s0">print(</span><span class="s4">f&quot;Testing gamma: </span><span class="s7">{</span><span class="s0">gamma</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l1782"><span class="ln">1782 </span></a>        <span class="s0">env = gym.make(</span><span class="s4">&quot;LunarLander-v3&quot;</span><span class="s0">)</span>
<a name="l1783"><span class="ln">1783 </span></a>        <span class="s0">agent = DoubleDQNAgent(env)</span>
<a name="l1784"><span class="ln">1784 </span></a>        <span class="s0">agent.gamma = gamma</span>
<a name="l1785"><span class="ln">1785 </span></a>
<a name="l1786"><span class="ln">1786 </span></a>        <span class="s0">rewards = train_agent_custom(agent</span><span class="s5">, </span><span class="s0">env</span><span class="s5">, </span><span class="s0">episodes=</span><span class="s6">550</span><span class="s0">)</span>
<a name="l1787"><span class="ln">1787 </span></a>        <span class="s0">gamma_results[gamma] = rewards</span>
<a name="l1788"><span class="ln">1788 </span></a>
<a name="l1789"><span class="ln">1789 </span></a>    <span class="s2">return </span><span class="s0">lr_results</span><span class="s5">, </span><span class="s0">gamma_results</span>
<a name="l1790"><span class="ln">1790 </span></a>
<a name="l1791"><span class="ln">1791 </span></a><span class="s3"># Run hyperparameter comparison</span>
<a name="l1792"><span class="ln">1792 </span></a><span class="s0">lr_results</span><span class="s5">, </span><span class="s0">gamma_results = compare_hyperparameters()</span>
<a name="l1793"><span class="ln">1793 </span></a>
<a name="l1794"><span class="ln">1794 </span></a><span class="s3"># Plot comparisons</span>
<a name="l1795"><span class="ln">1795 </span></a><span class="s0">fig</span><span class="s5">, </span><span class="s0">(ax1</span><span class="s5">, </span><span class="s0">ax2) = plt.subplots(</span><span class="s6">1</span><span class="s5">, </span><span class="s6">2</span><span class="s5">, </span><span class="s0">figsize=(</span><span class="s6">15</span><span class="s5">, </span><span class="s6">6</span><span class="s0">))</span>
<a name="l1796"><span class="ln">1796 </span></a>
<a name="l1797"><span class="ln">1797 </span></a><span class="s3"># Learning rate comparison</span>
<a name="l1798"><span class="ln">1798 </span></a><span class="s2">for </span><span class="s0">lr</span><span class="s5">, </span><span class="s0">rewards </span><span class="s2">in </span><span class="s0">lr_results.items():</span>
<a name="l1799"><span class="ln">1799 </span></a>    <span class="s0">rolling_avg = pd.Series(rewards).rolling(</span><span class="s6">50</span><span class="s0">).mean()</span>
<a name="l1800"><span class="ln">1800 </span></a>    <span class="s0">ax1.plot(rolling_avg</span><span class="s5">, </span><span class="s0">label=</span><span class="s4">f'LR=</span><span class="s7">{</span><span class="s0">lr</span><span class="s7">}</span><span class="s4">'</span><span class="s0">)</span>
<a name="l1801"><span class="ln">1801 </span></a><span class="s0">ax1.set_title(</span><span class="s4">'Learning Rate Comparison'</span><span class="s0">)</span>
<a name="l1802"><span class="ln">1802 </span></a><span class="s0">ax1.set_xlabel(</span><span class="s4">'Episode'</span><span class="s0">)</span>
<a name="l1803"><span class="ln">1803 </span></a><span class="s0">ax1.set_ylabel(</span><span class="s4">'Average Reward (50-episode rolling)'</span><span class="s0">)</span>
<a name="l1804"><span class="ln">1804 </span></a><span class="s0">ax1.legend()</span>
<a name="l1805"><span class="ln">1805 </span></a><span class="s0">ax1.grid(</span><span class="s2">True</span><span class="s0">)</span>
<a name="l1806"><span class="ln">1806 </span></a>
<a name="l1807"><span class="ln">1807 </span></a><span class="s3"># Gamma comparison</span>
<a name="l1808"><span class="ln">1808 </span></a><span class="s2">for </span><span class="s0">gamma</span><span class="s5">, </span><span class="s0">rewards </span><span class="s2">in </span><span class="s0">gamma_results.items():</span>
<a name="l1809"><span class="ln">1809 </span></a>    <span class="s0">rolling_avg = pd.Series(rewards).rolling(</span><span class="s6">50</span><span class="s0">).mean()</span>
<a name="l1810"><span class="ln">1810 </span></a>    <span class="s0">ax2.plot(rolling_avg</span><span class="s5">, </span><span class="s0">label=</span><span class="s4">f'γ=</span><span class="s7">{</span><span class="s0">gamma</span><span class="s7">}</span><span class="s4">'</span><span class="s0">)</span>
<a name="l1811"><span class="ln">1811 </span></a><span class="s0">ax2.set_title(</span><span class="s4">'Discount Factor Comparison'</span><span class="s0">)</span>
<a name="l1812"><span class="ln">1812 </span></a><span class="s0">ax2.set_xlabel(</span><span class="s4">'Episode'</span><span class="s0">)</span>
<a name="l1813"><span class="ln">1813 </span></a><span class="s0">ax2.set_ylabel(</span><span class="s4">'Average Reward (50-episode rolling)'</span><span class="s0">)</span>
<a name="l1814"><span class="ln">1814 </span></a><span class="s0">ax2.legend()</span>
<a name="l1815"><span class="ln">1815 </span></a><span class="s0">ax2.grid(</span><span class="s2">True</span><span class="s0">)</span>
<a name="l1816"><span class="ln">1816 </span></a>
<a name="l1817"><span class="ln">1817 </span></a><span class="s0">plt.tight_layout()</span>
<a name="l1818"><span class="ln">1818 </span></a><span class="s0">plt.show()</span><hr class="ls0"><a name="l1819"><span class="ln">1819 </span></a><span class="s0">#%% md 
<a name="l1820"><span class="ln">1820 </span></a># 8. Performance Evaluation 
<a name="l1821"><span class="ln">1821 </span></a> 
<a name="l1822"><span class="ln">1822 </span></a>## 8.1 Evaluation Metrics 
<a name="l1823"><span class="ln">1823 </span></a> 
<a name="l1824"><span class="ln">1824 </span></a>**Success rate**: Percentage of episodes where both legs contact ground with velocity &lt; 0.5 m/s. Measures primary objective achievement. 
<a name="l1825"><span class="ln">1825 </span></a> 
<a name="l1826"><span class="ln">1826 </span></a>**Fuel efficiency**: Engine usage quantified as main engine (+1.0) and side engines (+0.1) per activation. Lower values indicate more efficient control strategies. 
<a name="l1827"><span class="ln">1827 </span></a> 
<a name="l1828"><span class="ln">1828 </span></a>**Landing precision**: For successful landings, calculated as `1 / (1 + |x_pos| + |y_pos|)`. Higher values indicate closer proximity to landing pad center. 
<a name="l1829"><span class="ln">1829 </span></a> 
<a name="l1830"><span class="ln">1830 </span></a>**Crash detection**: Episodes with total reward &lt; -50, indicating collision or boundary violations. 
<a name="l1831"><span class="ln">1831 </span></a> 
<a name="l1832"><span class="ln">1832 </span></a>## 8.2 Evaluation Protocol 
<a name="l1833"><span class="ln">1833 </span></a> 
<a name="l1834"><span class="ln">1834 </span></a>**Deterministic testing**: Epsilon set to 0.0 for DQN, deterministic=True for PPO to eliminate exploration noise and assess learned policy performance. 
<a name="l1835"><span class="ln">1835 </span></a> 
<a name="l1836"><span class="ln">1836 </span></a>**Sample size**: 100 episodes per agent ensures statistical significance and reliable performance estimates. 
<a name="l1837"><span class="ln">1837 </span></a> 
<a name="l1838"><span class="ln">1838 </span></a>**State analysis**: Final state vector examination determines landing quality through position, velocity, and leg contact data. 
<a name="l1839"><span class="ln">1839 </span></a> 
<a name="l1840"><span class="ln">1840 </span></a>**Multi-session validation**: 10 independent evaluation sessions with 100 episodes each (1,000 total episodes) to assess performance consistency and statistical robustness. 
<a name="l1841"><span class="ln">1841 </span></a> 
<a name="l1842"><span class="ln">1842 </span></a>## 8.3 Experimental Results 
<a name="l1843"><span class="ln">1843 </span></a> 
<a name="l1844"><span class="ln">1844 </span></a>### Single Session Performance 
<a name="l1845"><span class="ln">1845 </span></a> 
<a name="l1846"><span class="ln">1846 </span></a>**DDQN Performance Evaluation Results:** 
<a name="l1847"><span class="ln">1847 </span></a> 
<a name="l1848"><span class="ln">1848 </span></a>- Success Rate: 77.0% 
<a name="l1849"><span class="ln">1849 </span></a>- Average Reward: 246.38 
<a name="l1850"><span class="ln">1850 </span></a>- Average Fuel Usage: 104.21 units 
<a name="l1851"><span class="ln">1851 </span></a>- Average Landing Precision: 0.901 
<a name="l1852"><span class="ln">1852 </span></a>- Crashes: 0 
<a name="l1853"><span class="ln">1853 </span></a> 
<a name="l1854"><span class="ln">1854 </span></a>**PPO Performance Evaluation Results:** 
<a name="l1855"><span class="ln">1855 </span></a> 
<a name="l1856"><span class="ln">1856 </span></a>- Success Rate: 94.0% 
<a name="l1857"><span class="ln">1857 </span></a>- Average Reward: 235.94 
<a name="l1858"><span class="ln">1858 </span></a>- Average Fuel Usage: 212.04 units 
<a name="l1859"><span class="ln">1859 </span></a>- Average Landing Precision: 0.938 
<a name="l1860"><span class="ln">1860 </span></a>- Crashes: 0 
<a name="l1861"><span class="ln">1861 </span></a> 
<a name="l1862"><span class="ln">1862 </span></a> 
<a name="l1863"><span class="ln">1863 </span></a>### Multi-Session Performance Statistics (10 Sessions x 100 Episodes) 
<a name="l1864"><span class="ln">1864 </span></a> 
<a name="l1865"><span class="ln">1865 </span></a>**DDQN Aggregate Results:** 
<a name="l1866"><span class="ln">1866 </span></a> 
<a name="l1867"><span class="ln">1867 </span></a>- **Success Rate**: 79.9% ± 4.6% (range: 74.0%-89.0%) 
<a name="l1868"><span class="ln">1868 </span></a>- **Average Reward**: 245.54 ± 6.12 (range: 237.16-258.88) 
<a name="l1869"><span class="ln">1869 </span></a>- **Average Fuel Usage**: 104.47 ± 3.12 units (range: 100.79-109.93) 
<a name="l1870"><span class="ln">1870 </span></a>- **Average Landing Precision**: 0.901 ± 0.005 (range: 0.895-0.908) 
<a name="l1871"><span class="ln">1871 </span></a>- **Total Crashes**: 3 across 1,000 episodes (0.3% crash rate) 
<a name="l1872"><span class="ln">1872 </span></a> 
<a name="l1873"><span class="ln">1873 </span></a>**PPO Aggregate Results:** 
<a name="l1874"><span class="ln">1874 </span></a> 
<a name="l1875"><span class="ln">1875 </span></a>- **Success Rate**: 92.0% ± 2.4% (range: 88.0%-97.0%) 
<a name="l1876"><span class="ln">1876 </span></a>- **Average Reward**: 242.33 ± 1.88 (range: 238.75-244.99) 
<a name="l1877"><span class="ln">1877 </span></a>- **Average Fuel Usage**: 210.23 ± 1.02 units (range: 209.21-211.70) 
<a name="l1878"><span class="ln">1878 </span></a>- **Average Landing Precision**: 0.947 ± 0.004 (range: 0.939-0.953) 
<a name="l1879"><span class="ln">1879 </span></a>- **Total Crashes**: 1 across 1,000 episodes (0.1% crash rate) 
<a name="l1880"><span class="ln">1880 </span></a> 
<a name="l1881"><span class="ln">1881 </span></a> 
<a name="l1882"><span class="ln">1882 </span></a>## 8.4 Comparative Analysis 
<a name="l1883"><span class="ln">1883 </span></a> 
<a name="l1884"><span class="ln">1884 </span></a>### Performance Trade-offs 
<a name="l1885"><span class="ln">1885 </span></a> 
<a name="l1886"><span class="ln">1886 </span></a>**PPO's Superior Success Rate**: PPO achieves a 12.1 percentage point advantage in success rate (92.0% vs 79.9%), demonstrating significantly more reliable landing performance. This 15.2% relative improvement represents PPO's superior policy stability and exploration-exploitation balance. 
<a name="l1887"><span class="ln">1887 </span></a> 
<a name="l1888"><span class="ln">1888 </span></a>**DDQN's Exceptional Fuel Efficiency**: DDQN demonstrates remarkable fuel efficiency, consuming only 104.47 units compared to PPO's 210.23 units—a 50.3% reduction in fuel usage. This efficiency advantage stems from DDQN's value-based learning approach that better optimizes the fuel-reward trade-off. 
<a name="l1889"><span class="ln">1889 </span></a> 
<a name="l1890"><span class="ln">1890 </span></a>**Reward Performance Paradox**: Despite lower success rates, DDQN achieves marginally higher average rewards (245.54 vs 242.33), indicating that successful DDQN landings generate higher individual episode rewards, likely due to fuel efficiency bonuses. 
<a name="l1891"><span class="ln">1891 </span></a> 
<a name="l1892"><span class="ln">1892 </span></a>### Statistical Robustness and Consistency 
<a name="l1893"><span class="ln">1893 </span></a> 
<a name="l1894"><span class="ln">1894 </span></a>**PPO's Superior Consistency**: PPO shows lower performance variance across sessions with standard deviations of 2.4% (success rate) and 1.88 (reward) compared to DDQN's 4.6% and 6.12 respectively. This 48% lower variability indicates more predictable deployment performance. 
<a name="l1895"><span class="ln">1895 </span></a> 
<a name="l1896"><span class="ln">1896 </span></a>**Landing Precision Analysis**: PPO achieves superior landing precision (0.947 vs 0.901), representing a 5.1% improvement in spatial accuracy. This suggests PPO's policy-based approach enables more refined control near the landing target. 
<a name="l1897"><span class="ln">1897 </span></a> 
<a name="l1898"><span class="ln">1898 </span></a>**Crash Rate Comparison**: Both algorithms demonstrate excellent safety with minimal crashes (PPO: 0.1%, DDQN: 0.3%), confirming robust learning has occurred. 
<a name="l1899"><span class="ln">1899 </span></a> 
<a name="l1900"><span class="ln">1900 </span></a>### Algorithmic Behavioral Differences 
<a name="l1901"><span class="ln">1901 </span></a> 
<a name="l1902"><span class="ln">1902 </span></a>**DDQN's Conservative Strategy**: The 50% fuel efficiency advantage suggests DDQN learned a more conservative, ballistic-trajectory approach that minimizes engine usage while accepting lower success rates. This behavior aligns with value-based learning optimizing long-term cumulative rewards. 
<a name="l1903"><span class="ln">1903 </span></a> 
<a name="l1904"><span class="ln">1904 </span></a>**PPO's Aggressive Reliability**: PPO's higher fuel consumption paired with 92% success rate indicates a more aggressive control strategy that prioritizes landing success over fuel conservation. This reflects policy-based learning's direct optimization of action selection for task completion. 
<a name="l1905"><span class="ln">1905 </span></a> 
<a name="l1906"><span class="ln">1906 </span></a>**Deployment Implications**: DDQN suits scenarios prioritizing fuel efficiency and operational cost minimization, while PPO excels in applications requiring maximum landing success rates despite higher resource consumption. 
<a name="l1907"><span class="ln">1907 </span></a> 
<a name="l1908"><span class="ln">1908 </span></a>### Performance Validation 
<a name="l1909"><span class="ln">1909 </span></a> 
<a name="l1910"><span class="ln">1910 </span></a>Both algorithms exceed the 200-point &quot;solved&quot; threshold consistently, confirming successful reinforcement learning convergence. The multi-session evaluation with 1,000 episodes per algorithm provides statistically significant performance characterization, validating the robustness of both approaches for the LunarLander control task. <hr class="ls0"><a name="l1911"><span class="ln">1911 </span></a>#%% 
<a name="l1912"><span class="ln">1912 </span></a></span><span class="s3"># Load the trained DQN agent from saved weights</span>
<a name="l1913"><span class="ln">1913 </span></a><span class="s0">env = gym.make(</span><span class="s4">&quot;LunarLander-v3&quot;</span><span class="s0">)</span>
<a name="l1914"><span class="ln">1914 </span></a><span class="s0">agent = DoubleDQNAgent(env)</span>
<a name="l1915"><span class="ln">1915 </span></a>
<a name="l1916"><span class="ln">1916 </span></a><span class="s3"># Load the best trained weights</span>
<a name="l1917"><span class="ln">1917 </span></a><span class="s0">agent.q_net.load_state_dict(torch.load(</span><span class="s4">&quot;dqn_success.pth&quot;</span><span class="s5">, </span><span class="s0">map_location=device))</span>
<a name="l1918"><span class="ln">1918 </span></a><span class="s0">agent.q_net.eval()  </span><span class="s3"># Set to evaluation mode</span>
<a name="l1919"><span class="ln">1919 </span></a>
<a name="l1920"><span class="ln">1920 </span></a><span class="s3"># For evaluation, set epsilon to 0 - no random exploration</span>
<a name="l1921"><span class="ln">1921 </span></a><span class="s0">agent.epsilon = </span><span class="s6">0.0</span>
<a name="l1922"><span class="ln">1922 </span></a>
<a name="l1923"><span class="ln">1923 </span></a><span class="s0">print(</span><span class="s4">&quot;Trained DQN agent loaded successfully!&quot;</span><span class="s0">)</span>
<a name="l1924"><span class="ln">1924 </span></a><span class="s0">print(</span><span class="s4">f&quot;Agent epsilon: </span><span class="s7">{</span><span class="s0">agent.epsilon</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l1925"><span class="ln">1925 </span></a><span class="s0">print(</span><span class="s4">f&quot;Model on GPU: </span><span class="s7">{</span><span class="s0">next(agent.q_net.parameters()).is_cuda</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l1926"><span class="ln">1926 </span></a>
<a name="l1927"><span class="ln">1927 </span></a><span class="s2">def </span><span class="s0">evaluate_agent_performance(agent</span><span class="s5">, </span><span class="s0">env</span><span class="s5">, </span><span class="s0">n_episodes=</span><span class="s6">100</span><span class="s0">):</span>
<a name="l1928"><span class="ln">1928 </span></a>
<a name="l1929"><span class="ln">1929 </span></a>    <span class="s3"># Temporarily disable exploration</span>
<a name="l1930"><span class="ln">1930 </span></a>    <span class="s0">original_epsilon = agent.epsilon</span>
<a name="l1931"><span class="ln">1931 </span></a>    <span class="s0">agent.epsilon = </span><span class="s6">0.0</span>
<a name="l1932"><span class="ln">1932 </span></a>
<a name="l1933"><span class="ln">1933 </span></a>    <span class="s0">results = {</span>
<a name="l1934"><span class="ln">1934 </span></a>        <span class="s4">'rewards'</span><span class="s0">: []</span><span class="s5">,</span>
<a name="l1935"><span class="ln">1935 </span></a>        <span class="s4">'episode_lengths'</span><span class="s0">: []</span><span class="s5">,</span>
<a name="l1936"><span class="ln">1936 </span></a>        <span class="s4">'successful_landings'</span><span class="s0">: </span><span class="s6">0</span><span class="s5">,</span>
<a name="l1937"><span class="ln">1937 </span></a>        <span class="s4">'crashes'</span><span class="s0">: </span><span class="s6">0</span><span class="s5">,</span>
<a name="l1938"><span class="ln">1938 </span></a>        <span class="s4">'fuel_usage'</span><span class="s0">: []</span><span class="s5">,</span>
<a name="l1939"><span class="ln">1939 </span></a>        <span class="s4">'landing_precision'</span><span class="s0">: []</span>
<a name="l1940"><span class="ln">1940 </span></a>    <span class="s0">}</span>
<a name="l1941"><span class="ln">1941 </span></a>
<a name="l1942"><span class="ln">1942 </span></a>    <span class="s2">for </span><span class="s0">episode </span><span class="s2">in </span><span class="s0">range(n_episodes):</span>
<a name="l1943"><span class="ln">1943 </span></a>        <span class="s0">state</span><span class="s5">, </span><span class="s0">_ = env.reset()</span>
<a name="l1944"><span class="ln">1944 </span></a>        <span class="s0">total_reward = </span><span class="s6">0</span>
<a name="l1945"><span class="ln">1945 </span></a>        <span class="s0">episode_length = </span><span class="s6">0</span>
<a name="l1946"><span class="ln">1946 </span></a>        <span class="s0">fuel_used = </span><span class="s6">0</span>
<a name="l1947"><span class="ln">1947 </span></a>        <span class="s0">done = </span><span class="s2">False</span>
<a name="l1948"><span class="ln">1948 </span></a>
<a name="l1949"><span class="ln">1949 </span></a>        <span class="s2">while not </span><span class="s0">done:</span>
<a name="l1950"><span class="ln">1950 </span></a>            <span class="s0">action = agent.act(state)</span>
<a name="l1951"><span class="ln">1951 </span></a>            <span class="s0">next_state</span><span class="s5">, </span><span class="s0">reward</span><span class="s5">, </span><span class="s0">terminated</span><span class="s5">, </span><span class="s0">truncated</span><span class="s5">, </span><span class="s0">_ = env.step(action)</span>
<a name="l1952"><span class="ln">1952 </span></a>            <span class="s0">done = terminated </span><span class="s2">or </span><span class="s0">truncated</span>
<a name="l1953"><span class="ln">1953 </span></a>
<a name="l1954"><span class="ln">1954 </span></a>            <span class="s3"># Track fuel usage</span>
<a name="l1955"><span class="ln">1955 </span></a>            <span class="s2">if </span><span class="s0">action == </span><span class="s6">2</span><span class="s0">:  </span><span class="s3"># Main engine</span>
<a name="l1956"><span class="ln">1956 </span></a>                <span class="s0">fuel_used += </span><span class="s6">1</span>
<a name="l1957"><span class="ln">1957 </span></a>            <span class="s2">elif </span><span class="s0">action </span><span class="s2">in </span><span class="s0">[</span><span class="s6">1</span><span class="s5">, </span><span class="s6">3</span><span class="s0">]:  </span><span class="s3"># Side engines</span>
<a name="l1958"><span class="ln">1958 </span></a>                <span class="s0">fuel_used += </span><span class="s6">0.1</span>
<a name="l1959"><span class="ln">1959 </span></a>
<a name="l1960"><span class="ln">1960 </span></a>            <span class="s0">state = next_state</span>
<a name="l1961"><span class="ln">1961 </span></a>            <span class="s0">total_reward += reward</span>
<a name="l1962"><span class="ln">1962 </span></a>            <span class="s0">episode_length += </span><span class="s6">1</span>
<a name="l1963"><span class="ln">1963 </span></a>
<a name="l1964"><span class="ln">1964 </span></a>        <span class="s3"># Analyze final state</span>
<a name="l1965"><span class="ln">1965 </span></a>        <span class="s0">x_pos</span><span class="s5">, </span><span class="s0">y_pos</span><span class="s5">, </span><span class="s0">x_vel</span><span class="s5">, </span><span class="s0">y_vel</span><span class="s5">, </span><span class="s0">angle</span><span class="s5">, </span><span class="s0">ang_vel</span><span class="s5">, </span><span class="s0">leg_left</span><span class="s5">, </span><span class="s0">leg_right = state</span>
<a name="l1966"><span class="ln">1966 </span></a>
<a name="l1967"><span class="ln">1967 </span></a>        <span class="s3"># Check landing success</span>
<a name="l1968"><span class="ln">1968 </span></a>        <span class="s2">if </span><span class="s0">leg_left </span><span class="s2">and </span><span class="s0">leg_right </span><span class="s2">and </span><span class="s0">abs(x_vel) &lt; </span><span class="s6">0.5 </span><span class="s2">and </span><span class="s0">abs(y_vel) &lt; </span><span class="s6">0.5</span><span class="s0">:</span>
<a name="l1969"><span class="ln">1969 </span></a>            <span class="s0">results[</span><span class="s4">'successful_landings'</span><span class="s0">] += </span><span class="s6">1</span>
<a name="l1970"><span class="ln">1970 </span></a>            <span class="s3"># Calculate landing precision</span>
<a name="l1971"><span class="ln">1971 </span></a>            <span class="s0">precision = </span><span class="s6">1 </span><span class="s0">/ (</span><span class="s6">1 </span><span class="s0">+ abs(x_pos) + abs(y_pos))</span>
<a name="l1972"><span class="ln">1972 </span></a>            <span class="s0">results[</span><span class="s4">'landing_precision'</span><span class="s0">].append(precision)</span>
<a name="l1973"><span class="ln">1973 </span></a>        <span class="s2">elif </span><span class="s0">total_reward &lt; -</span><span class="s6">50</span><span class="s0">:  </span><span class="s3"># Likely crashed</span>
<a name="l1974"><span class="ln">1974 </span></a>            <span class="s0">results[</span><span class="s4">'crashes'</span><span class="s0">] += </span><span class="s6">1</span>
<a name="l1975"><span class="ln">1975 </span></a>
<a name="l1976"><span class="ln">1976 </span></a>        <span class="s0">results[</span><span class="s4">'rewards'</span><span class="s0">].append(total_reward)</span>
<a name="l1977"><span class="ln">1977 </span></a>        <span class="s0">results[</span><span class="s4">'episode_lengths'</span><span class="s0">].append(episode_length)</span>
<a name="l1978"><span class="ln">1978 </span></a>        <span class="s0">results[</span><span class="s4">'fuel_usage'</span><span class="s0">].append(fuel_used)</span>
<a name="l1979"><span class="ln">1979 </span></a>
<a name="l1980"><span class="ln">1980 </span></a>    <span class="s3"># Restore original epsilon</span>
<a name="l1981"><span class="ln">1981 </span></a>    <span class="s0">agent.epsilon = original_epsilon</span>
<a name="l1982"><span class="ln">1982 </span></a>
<a name="l1983"><span class="ln">1983 </span></a>    <span class="s3"># Calculate metrics</span>
<a name="l1984"><span class="ln">1984 </span></a>    <span class="s0">success_rate = results[</span><span class="s4">'successful_landings'</span><span class="s0">] / n_episodes * </span><span class="s6">100</span>
<a name="l1985"><span class="ln">1985 </span></a>    <span class="s0">avg_reward = np.mean(results[</span><span class="s4">'rewards'</span><span class="s0">])</span>
<a name="l1986"><span class="ln">1986 </span></a>    <span class="s0">avg_fuel = np.mean(results[</span><span class="s4">'fuel_usage'</span><span class="s0">])</span>
<a name="l1987"><span class="ln">1987 </span></a>    <span class="s0">avg_precision = np.mean(results[</span><span class="s4">'landing_precision'</span><span class="s0">]) </span><span class="s2">if </span><span class="s0">results[</span><span class="s4">'landing_precision'</span><span class="s0">] </span><span class="s2">else </span><span class="s6">0</span>
<a name="l1988"><span class="ln">1988 </span></a>
<a name="l1989"><span class="ln">1989 </span></a>    <span class="s0">print(</span><span class="s4">f&quot;</span><span class="s7">\n</span><span class="s4">DDQN Performance Evaluation Results:&quot;</span><span class="s0">)</span>
<a name="l1990"><span class="ln">1990 </span></a>    <span class="s0">print(</span><span class="s4">f&quot;Success Rate: </span><span class="s7">{</span><span class="s0">success_rate</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">%&quot;</span><span class="s0">)</span>
<a name="l1991"><span class="ln">1991 </span></a>    <span class="s0">print(</span><span class="s4">f&quot;Average Reward: </span><span class="s7">{</span><span class="s0">avg_reward</span><span class="s7">:</span><span class="s4">.2f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l1992"><span class="ln">1992 </span></a>    <span class="s0">print(</span><span class="s4">f&quot;Average Fuel Usage: </span><span class="s7">{</span><span class="s0">avg_fuel</span><span class="s7">:</span><span class="s4">.2f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l1993"><span class="ln">1993 </span></a>    <span class="s0">print(</span><span class="s4">f&quot;Average Landing Precision: </span><span class="s7">{</span><span class="s0">avg_precision</span><span class="s7">:</span><span class="s4">.3f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l1994"><span class="ln">1994 </span></a>    <span class="s0">print(</span><span class="s4">f&quot;Crashes: </span><span class="s7">{</span><span class="s0">results[</span><span class="s4">'crashes'</span><span class="s0">]</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l1995"><span class="ln">1995 </span></a>
<a name="l1996"><span class="ln">1996 </span></a>    <span class="s2">return </span><span class="s0">{</span>
<a name="l1997"><span class="ln">1997 </span></a>        <span class="s4">'success_rate'</span><span class="s0">: success_rate</span><span class="s5">,</span>
<a name="l1998"><span class="ln">1998 </span></a>        <span class="s4">'avg_reward'</span><span class="s0">: avg_reward</span><span class="s5">,</span>
<a name="l1999"><span class="ln">1999 </span></a>        <span class="s4">'avg_fuel'</span><span class="s0">: avg_fuel</span><span class="s5">,</span>
<a name="l2000"><span class="ln">2000 </span></a>        <span class="s4">'avg_precision'</span><span class="s0">: avg_precision</span><span class="s5">,</span>
<a name="l2001"><span class="ln">2001 </span></a>        <span class="s4">'crashes'</span><span class="s0">: results[</span><span class="s4">'crashes'</span><span class="s0">]</span>
<a name="l2002"><span class="ln">2002 </span></a>    <span class="s0">}</span>
<a name="l2003"><span class="ln">2003 </span></a>
<a name="l2004"><span class="ln">2004 </span></a><span class="s2">def </span><span class="s0">evaluate_ppo_performance(model</span><span class="s5">, </span><span class="s0">env</span><span class="s5">, </span><span class="s0">n_episodes=</span><span class="s6">100</span><span class="s0">):</span>
<a name="l2005"><span class="ln">2005 </span></a>    <span class="s0">results = {</span>
<a name="l2006"><span class="ln">2006 </span></a>        <span class="s4">'rewards'</span><span class="s0">: []</span><span class="s5">,</span>
<a name="l2007"><span class="ln">2007 </span></a>        <span class="s4">'episode_lengths'</span><span class="s0">: []</span><span class="s5">,</span>
<a name="l2008"><span class="ln">2008 </span></a>        <span class="s4">'successful_landings'</span><span class="s0">: </span><span class="s6">0</span><span class="s5">,</span>
<a name="l2009"><span class="ln">2009 </span></a>        <span class="s4">'crashes'</span><span class="s0">: </span><span class="s6">0</span><span class="s5">,</span>
<a name="l2010"><span class="ln">2010 </span></a>        <span class="s4">'fuel_usage'</span><span class="s0">: []</span><span class="s5">,</span>
<a name="l2011"><span class="ln">2011 </span></a>        <span class="s4">'landing_precision'</span><span class="s0">: []</span>
<a name="l2012"><span class="ln">2012 </span></a>    <span class="s0">}</span>
<a name="l2013"><span class="ln">2013 </span></a>
<a name="l2014"><span class="ln">2014 </span></a>    <span class="s2">for </span><span class="s0">episode </span><span class="s2">in </span><span class="s0">range(n_episodes):</span>
<a name="l2015"><span class="ln">2015 </span></a>        <span class="s0">obs</span><span class="s5">, </span><span class="s0">_ = env.reset()</span>
<a name="l2016"><span class="ln">2016 </span></a>        <span class="s0">total_reward = </span><span class="s6">0</span>
<a name="l2017"><span class="ln">2017 </span></a>        <span class="s0">episode_length = </span><span class="s6">0</span>
<a name="l2018"><span class="ln">2018 </span></a>        <span class="s0">fuel_used = </span><span class="s6">0</span>
<a name="l2019"><span class="ln">2019 </span></a>        <span class="s0">done = </span><span class="s2">False</span>
<a name="l2020"><span class="ln">2020 </span></a>
<a name="l2021"><span class="ln">2021 </span></a>        <span class="s2">while not </span><span class="s0">done:</span>
<a name="l2022"><span class="ln">2022 </span></a>            <span class="s0">action</span><span class="s5">, </span><span class="s0">_ = model.predict(obs</span><span class="s5">, </span><span class="s0">deterministic=</span><span class="s2">True</span><span class="s0">)</span>
<a name="l2023"><span class="ln">2023 </span></a>            <span class="s0">obs</span><span class="s5">, </span><span class="s0">reward</span><span class="s5">, </span><span class="s0">terminated</span><span class="s5">, </span><span class="s0">truncated</span><span class="s5">, </span><span class="s0">_ = env.step(action)</span>
<a name="l2024"><span class="ln">2024 </span></a>            <span class="s0">done = terminated </span><span class="s2">or </span><span class="s0">truncated</span>
<a name="l2025"><span class="ln">2025 </span></a>
<a name="l2026"><span class="ln">2026 </span></a>            <span class="s3"># Track fuel usage</span>
<a name="l2027"><span class="ln">2027 </span></a>            <span class="s2">if </span><span class="s0">action == </span><span class="s6">2</span><span class="s0">:  </span><span class="s3"># Main engine</span>
<a name="l2028"><span class="ln">2028 </span></a>                <span class="s0">fuel_used += </span><span class="s6">1</span>
<a name="l2029"><span class="ln">2029 </span></a>            <span class="s2">elif </span><span class="s0">action </span><span class="s2">in </span><span class="s0">[</span><span class="s6">1</span><span class="s5">, </span><span class="s6">3</span><span class="s0">]:  </span><span class="s3"># Side engines</span>
<a name="l2030"><span class="ln">2030 </span></a>                <span class="s0">fuel_used += </span><span class="s6">0.1</span>
<a name="l2031"><span class="ln">2031 </span></a>
<a name="l2032"><span class="ln">2032 </span></a>            <span class="s0">total_reward += reward</span>
<a name="l2033"><span class="ln">2033 </span></a>            <span class="s0">episode_length += </span><span class="s6">1</span>
<a name="l2034"><span class="ln">2034 </span></a>
<a name="l2035"><span class="ln">2035 </span></a>        <span class="s3"># Analyze final state</span>
<a name="l2036"><span class="ln">2036 </span></a>        <span class="s0">x_pos</span><span class="s5">, </span><span class="s0">y_pos</span><span class="s5">, </span><span class="s0">x_vel</span><span class="s5">, </span><span class="s0">y_vel</span><span class="s5">, </span><span class="s0">angle</span><span class="s5">, </span><span class="s0">ang_vel</span><span class="s5">, </span><span class="s0">leg_left</span><span class="s5">, </span><span class="s0">leg_right = obs</span>
<a name="l2037"><span class="ln">2037 </span></a>
<a name="l2038"><span class="ln">2038 </span></a>        <span class="s3"># Check landing success</span>
<a name="l2039"><span class="ln">2039 </span></a>        <span class="s2">if </span><span class="s0">leg_left </span><span class="s2">and </span><span class="s0">leg_right </span><span class="s2">and </span><span class="s0">abs(x_vel) &lt; </span><span class="s6">0.5 </span><span class="s2">and </span><span class="s0">abs(y_vel) &lt; </span><span class="s6">0.5</span><span class="s0">:</span>
<a name="l2040"><span class="ln">2040 </span></a>            <span class="s0">results[</span><span class="s4">'successful_landings'</span><span class="s0">] += </span><span class="s6">1</span>
<a name="l2041"><span class="ln">2041 </span></a>            <span class="s3"># Calculate landing precision</span>
<a name="l2042"><span class="ln">2042 </span></a>            <span class="s0">precision = </span><span class="s6">1 </span><span class="s0">/ (</span><span class="s6">1 </span><span class="s0">+ abs(x_pos) + abs(y_pos))</span>
<a name="l2043"><span class="ln">2043 </span></a>            <span class="s0">results[</span><span class="s4">'landing_precision'</span><span class="s0">].append(precision)</span>
<a name="l2044"><span class="ln">2044 </span></a>        <span class="s2">elif </span><span class="s0">total_reward &lt; -</span><span class="s6">50</span><span class="s0">:  </span><span class="s3"># Likely crashed</span>
<a name="l2045"><span class="ln">2045 </span></a>            <span class="s0">results[</span><span class="s4">'crashes'</span><span class="s0">] += </span><span class="s6">1</span>
<a name="l2046"><span class="ln">2046 </span></a>
<a name="l2047"><span class="ln">2047 </span></a>        <span class="s0">results[</span><span class="s4">'rewards'</span><span class="s0">].append(total_reward)</span>
<a name="l2048"><span class="ln">2048 </span></a>        <span class="s0">results[</span><span class="s4">'episode_lengths'</span><span class="s0">].append(episode_length)</span>
<a name="l2049"><span class="ln">2049 </span></a>        <span class="s0">results[</span><span class="s4">'fuel_usage'</span><span class="s0">].append(fuel_used)</span>
<a name="l2050"><span class="ln">2050 </span></a>
<a name="l2051"><span class="ln">2051 </span></a>    <span class="s3"># Calculate metrics</span>
<a name="l2052"><span class="ln">2052 </span></a>    <span class="s0">success_rate = results[</span><span class="s4">'successful_landings'</span><span class="s0">] / n_episodes * </span><span class="s6">100</span>
<a name="l2053"><span class="ln">2053 </span></a>    <span class="s0">avg_reward = np.mean(results[</span><span class="s4">'rewards'</span><span class="s0">])</span>
<a name="l2054"><span class="ln">2054 </span></a>    <span class="s0">avg_fuel = np.mean(results[</span><span class="s4">'fuel_usage'</span><span class="s0">])</span>
<a name="l2055"><span class="ln">2055 </span></a>    <span class="s0">avg_precision = np.mean(results[</span><span class="s4">'landing_precision'</span><span class="s0">]) </span><span class="s2">if </span><span class="s0">results[</span><span class="s4">'landing_precision'</span><span class="s0">] </span><span class="s2">else </span><span class="s6">0</span>
<a name="l2056"><span class="ln">2056 </span></a>
<a name="l2057"><span class="ln">2057 </span></a>    <span class="s0">print(</span><span class="s4">f&quot;</span><span class="s7">\n</span><span class="s4">PPO Performance Evaluation Results:&quot;</span><span class="s0">)</span>
<a name="l2058"><span class="ln">2058 </span></a>    <span class="s0">print(</span><span class="s4">f&quot;Success Rate: </span><span class="s7">{</span><span class="s0">success_rate</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">%&quot;</span><span class="s0">)</span>
<a name="l2059"><span class="ln">2059 </span></a>    <span class="s0">print(</span><span class="s4">f&quot;Average Reward: </span><span class="s7">{</span><span class="s0">avg_reward</span><span class="s7">:</span><span class="s4">.2f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l2060"><span class="ln">2060 </span></a>    <span class="s0">print(</span><span class="s4">f&quot;Average Fuel Usage: </span><span class="s7">{</span><span class="s0">avg_fuel</span><span class="s7">:</span><span class="s4">.2f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l2061"><span class="ln">2061 </span></a>    <span class="s0">print(</span><span class="s4">f&quot;Average Landing Precision: </span><span class="s7">{</span><span class="s0">avg_precision</span><span class="s7">:</span><span class="s4">.3f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l2062"><span class="ln">2062 </span></a>    <span class="s0">print(</span><span class="s4">f&quot;Crashes: </span><span class="s7">{</span><span class="s0">results[</span><span class="s4">'crashes'</span><span class="s0">]</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l2063"><span class="ln">2063 </span></a>
<a name="l2064"><span class="ln">2064 </span></a>    <span class="s2">return </span><span class="s0">{</span>
<a name="l2065"><span class="ln">2065 </span></a>        <span class="s4">'success_rate'</span><span class="s0">: success_rate</span><span class="s5">,</span>
<a name="l2066"><span class="ln">2066 </span></a>        <span class="s4">'avg_reward'</span><span class="s0">: avg_reward</span><span class="s5">,</span>
<a name="l2067"><span class="ln">2067 </span></a>        <span class="s4">'avg_fuel'</span><span class="s0">: avg_fuel</span><span class="s5">,</span>
<a name="l2068"><span class="ln">2068 </span></a>        <span class="s4">'avg_precision'</span><span class="s0">: avg_precision</span><span class="s5">,</span>
<a name="l2069"><span class="ln">2069 </span></a>        <span class="s4">'crashes'</span><span class="s0">: results[</span><span class="s4">'crashes'</span><span class="s0">]</span>
<a name="l2070"><span class="ln">2070 </span></a>    <span class="s0">}</span>
<a name="l2071"><span class="ln">2071 </span></a>
<a name="l2072"><span class="ln">2072 </span></a><span class="s3"># Multiple evaluation sessions functions</span>
<a name="l2073"><span class="ln">2073 </span></a><span class="s2">def </span><span class="s0">run_multiple_evaluations(agent</span><span class="s5">, </span><span class="s0">model</span><span class="s5">, </span><span class="s0">n_sessions=</span><span class="s6">5</span><span class="s5">, </span><span class="s0">n_episodes=</span><span class="s6">100</span><span class="s0">):</span>
<a name="l2074"><span class="ln">2074 </span></a>    <span class="s0">dqn_session_results = []</span>
<a name="l2075"><span class="ln">2075 </span></a>    <span class="s0">ppo_session_results = []</span>
<a name="l2076"><span class="ln">2076 </span></a>
<a name="l2077"><span class="ln">2077 </span></a>    <span class="s2">for </span><span class="s0">i </span><span class="s2">in </span><span class="s0">range(n_sessions):</span>
<a name="l2078"><span class="ln">2078 </span></a>        <span class="s0">print(</span><span class="s4">f&quot;</span><span class="s7">\n</span><span class="s4">Running evaluation session </span><span class="s7">{</span><span class="s0">i+</span><span class="s6">1</span><span class="s7">}</span><span class="s4">/</span><span class="s7">{</span><span class="s0">n_sessions</span><span class="s7">}</span><span class="s4">...&quot;</span><span class="s0">)</span>
<a name="l2079"><span class="ln">2079 </span></a>
<a name="l2080"><span class="ln">2080 </span></a>        <span class="s3"># DQN evaluation</span>
<a name="l2081"><span class="ln">2081 </span></a>        <span class="s0">dqn_env = gym.make(</span><span class="s4">&quot;LunarLander-v3&quot;</span><span class="s0">)</span>
<a name="l2082"><span class="ln">2082 </span></a>        <span class="s0">dqn_results = evaluate_agent_performance(agent</span><span class="s5">, </span><span class="s0">dqn_env</span><span class="s5">, </span><span class="s0">n_episodes)</span>
<a name="l2083"><span class="ln">2083 </span></a>        <span class="s0">dqn_session_results.append(dqn_results)</span>
<a name="l2084"><span class="ln">2084 </span></a>        <span class="s0">dqn_env.close()</span>
<a name="l2085"><span class="ln">2085 </span></a>
<a name="l2086"><span class="ln">2086 </span></a>        <span class="s3"># PPO evaluation</span>
<a name="l2087"><span class="ln">2087 </span></a>        <span class="s0">ppo_env = gym.make(</span><span class="s4">&quot;LunarLander-v3&quot;</span><span class="s0">)</span>
<a name="l2088"><span class="ln">2088 </span></a>        <span class="s0">ppo_results = evaluate_ppo_performance(model</span><span class="s5">, </span><span class="s0">ppo_env</span><span class="s5">, </span><span class="s0">n_episodes)</span>
<a name="l2089"><span class="ln">2089 </span></a>        <span class="s0">ppo_session_results.append(ppo_results)</span>
<a name="l2090"><span class="ln">2090 </span></a>        <span class="s0">ppo_env.close()</span>
<a name="l2091"><span class="ln">2091 </span></a>
<a name="l2092"><span class="ln">2092 </span></a>    <span class="s2">return </span><span class="s0">dqn_session_results</span><span class="s5">, </span><span class="s0">ppo_session_results</span>
<a name="l2093"><span class="ln">2093 </span></a>
<a name="l2094"><span class="ln">2094 </span></a><span class="s2">def </span><span class="s0">plot_metric_variability(dqn_results</span><span class="s5">, </span><span class="s0">ppo_results</span><span class="s5">, </span><span class="s0">metric_name</span><span class="s5">, </span><span class="s0">ylabel):</span>
<a name="l2095"><span class="ln">2095 </span></a>    <span class="s0">dqn_metrics = [res[metric_name] </span><span class="s2">for </span><span class="s0">res </span><span class="s2">in </span><span class="s0">dqn_results]</span>
<a name="l2096"><span class="ln">2096 </span></a>    <span class="s0">ppo_metrics = [res[metric_name] </span><span class="s2">for </span><span class="s0">res </span><span class="s2">in </span><span class="s0">ppo_results]</span>
<a name="l2097"><span class="ln">2097 </span></a>
<a name="l2098"><span class="ln">2098 </span></a>    <span class="s0">sessions = list(range(</span><span class="s6">1</span><span class="s5">, </span><span class="s0">len(dqn_metrics)+</span><span class="s6">1</span><span class="s0">))</span>
<a name="l2099"><span class="ln">2099 </span></a>
<a name="l2100"><span class="ln">2100 </span></a>    <span class="s0">plt.figure(figsize=(</span><span class="s6">12</span><span class="s5">, </span><span class="s6">6</span><span class="s0">))</span>
<a name="l2101"><span class="ln">2101 </span></a>    <span class="s0">plt.plot(sessions</span><span class="s5">, </span><span class="s0">dqn_metrics</span><span class="s5">, </span><span class="s0">marker=</span><span class="s4">'o'</span><span class="s5">, </span><span class="s0">linewidth=</span><span class="s6">2</span><span class="s5">, </span><span class="s0">markersize=</span><span class="s6">8</span><span class="s5">, </span><span class="s0">label=</span><span class="s4">'DQN'</span><span class="s5">, </span><span class="s0">color=</span><span class="s4">'blue'</span><span class="s0">)</span>
<a name="l2102"><span class="ln">2102 </span></a>    <span class="s0">plt.plot(sessions</span><span class="s5">, </span><span class="s0">ppo_metrics</span><span class="s5">, </span><span class="s0">marker=</span><span class="s4">'s'</span><span class="s5">, </span><span class="s0">linewidth=</span><span class="s6">2</span><span class="s5">, </span><span class="s0">markersize=</span><span class="s6">8</span><span class="s5">, </span><span class="s0">label=</span><span class="s4">'PPO'</span><span class="s5">, </span><span class="s0">color=</span><span class="s4">'orange'</span><span class="s0">)</span>
<a name="l2103"><span class="ln">2103 </span></a>
<a name="l2104"><span class="ln">2104 </span></a>    <span class="s3"># Add mean lines</span>
<a name="l2105"><span class="ln">2105 </span></a>    <span class="s0">dqn_mean = np.mean(dqn_metrics)</span>
<a name="l2106"><span class="ln">2106 </span></a>    <span class="s0">ppo_mean = np.mean(ppo_metrics)</span>
<a name="l2107"><span class="ln">2107 </span></a>    <span class="s0">plt.axhline(y=dqn_mean</span><span class="s5">, </span><span class="s0">color=</span><span class="s4">'blue'</span><span class="s5">, </span><span class="s0">linestyle=</span><span class="s4">'--'</span><span class="s5">, </span><span class="s0">alpha=</span><span class="s6">0.7</span><span class="s5">, </span><span class="s0">label=</span><span class="s4">f'DQN Mean: </span><span class="s7">{</span><span class="s0">dqn_mean</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">'</span><span class="s0">)</span>
<a name="l2108"><span class="ln">2108 </span></a>    <span class="s0">plt.axhline(y=ppo_mean</span><span class="s5">, </span><span class="s0">color=</span><span class="s4">'orange'</span><span class="s5">, </span><span class="s0">linestyle=</span><span class="s4">'--'</span><span class="s5">, </span><span class="s0">alpha=</span><span class="s6">0.7</span><span class="s5">, </span><span class="s0">label=</span><span class="s4">f'PPO Mean: </span><span class="s7">{</span><span class="s0">ppo_mean</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">'</span><span class="s0">)</span>
<a name="l2109"><span class="ln">2109 </span></a>
<a name="l2110"><span class="ln">2110 </span></a>    <span class="s0">plt.title(</span><span class="s4">f'</span><span class="s7">{</span><span class="s0">ylabel</span><span class="s7">} </span><span class="s4">Variability Across Evaluation Sessions'</span><span class="s0">)</span>
<a name="l2111"><span class="ln">2111 </span></a>    <span class="s0">plt.xlabel(</span><span class="s4">'Evaluation Session'</span><span class="s0">)</span>
<a name="l2112"><span class="ln">2112 </span></a>    <span class="s0">plt.ylabel(ylabel)</span>
<a name="l2113"><span class="ln">2113 </span></a>    <span class="s0">plt.xticks(sessions)</span>
<a name="l2114"><span class="ln">2114 </span></a>    <span class="s0">plt.grid(</span><span class="s2">True</span><span class="s5">, </span><span class="s0">alpha=</span><span class="s6">0.3</span><span class="s0">)</span>
<a name="l2115"><span class="ln">2115 </span></a>    <span class="s0">plt.legend()</span>
<a name="l2116"><span class="ln">2116 </span></a>    <span class="s0">plt.tight_layout()</span>
<a name="l2117"><span class="ln">2117 </span></a>    <span class="s0">plt.show()</span>
<a name="l2118"><span class="ln">2118 </span></a>
<a name="l2119"><span class="ln">2119 </span></a>    <span class="s3"># Print statistics</span>
<a name="l2120"><span class="ln">2120 </span></a>    <span class="s0">print(</span><span class="s4">f&quot;</span><span class="s7">\n{</span><span class="s0">ylabel</span><span class="s7">} </span><span class="s4">Statistics:&quot;</span><span class="s0">)</span>
<a name="l2121"><span class="ln">2121 </span></a>    <span class="s0">print(</span><span class="s4">f&quot;DQN: </span><span class="s7">{</span><span class="s0">dqn_mean</span><span class="s7">:</span><span class="s4">.2f</span><span class="s7">} </span><span class="s4">± </span><span class="s7">{</span><span class="s0">np.std(dqn_metrics)</span><span class="s7">:</span><span class="s4">.2f</span><span class="s7">} </span><span class="s4">(range: </span><span class="s7">{</span><span class="s0">min(dqn_metrics)</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">-</span><span class="s7">{</span><span class="s0">max(dqn_metrics)</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">)&quot;</span><span class="s0">)</span>
<a name="l2122"><span class="ln">2122 </span></a>    <span class="s0">print(</span><span class="s4">f&quot;PPO: </span><span class="s7">{</span><span class="s0">ppo_mean</span><span class="s7">:</span><span class="s4">.2f</span><span class="s7">} </span><span class="s4">± </span><span class="s7">{</span><span class="s0">np.std(ppo_metrics)</span><span class="s7">:</span><span class="s4">.2f</span><span class="s7">} </span><span class="s4">(range: </span><span class="s7">{</span><span class="s0">min(ppo_metrics)</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">-</span><span class="s7">{</span><span class="s0">max(ppo_metrics)</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">)&quot;</span><span class="s0">)</span>
<a name="l2123"><span class="ln">2123 </span></a>
<a name="l2124"><span class="ln">2124 </span></a><span class="s3"># Run single evaluation first</span>
<a name="l2125"><span class="ln">2125 </span></a><span class="s0">print(</span><span class="s4">&quot;=== Single Evaluation Session ===&quot;</span><span class="s0">)</span>
<a name="l2126"><span class="ln">2126 </span></a><span class="s0">dqn_performance = evaluate_agent_performance(agent</span><span class="s5">, </span><span class="s0">gym.make(</span><span class="s4">&quot;LunarLander-v3&quot;</span><span class="s0">))</span>
<a name="l2127"><span class="ln">2127 </span></a><span class="s0">ppo_performance = evaluate_ppo_performance(ppo_model</span><span class="s5">, </span><span class="s0">gym.make(</span><span class="s4">&quot;LunarLander-v3&quot;</span><span class="s0">))</span>
<a name="l2128"><span class="ln">2128 </span></a>
<a name="l2129"><span class="ln">2129 </span></a><span class="s3"># Run multiple evaluation sessions</span>
<a name="l2130"><span class="ln">2130 </span></a><span class="s0">print(</span><span class="s4">&quot;</span><span class="s7">\n</span><span class="s4">=== Multiple Evaluation Sessions ===&quot;</span><span class="s0">)</span>
<a name="l2131"><span class="ln">2131 </span></a><span class="s0">n_sessions = </span><span class="s6">10</span>
<a name="l2132"><span class="ln">2132 </span></a><span class="s0">n_episodes = </span><span class="s6">100</span>
<a name="l2133"><span class="ln">2133 </span></a>
<a name="l2134"><span class="ln">2134 </span></a><span class="s0">dqn_session_results</span><span class="s5">, </span><span class="s0">ppo_session_results = run_multiple_evaluations(agent</span><span class="s5">, </span><span class="s0">ppo_model</span><span class="s5">, </span><span class="s0">n_sessions</span><span class="s5">, </span><span class="s0">n_episodes)</span>
<a name="l2135"><span class="ln">2135 </span></a>
<a name="l2136"><span class="ln">2136 </span></a><span class="s3"># Plot variability graphs</span>
<a name="l2137"><span class="ln">2137 </span></a><span class="s0">plot_metric_variability(dqn_session_results</span><span class="s5">, </span><span class="s0">ppo_session_results</span><span class="s5">, </span><span class="s4">'success_rate'</span><span class="s5">, </span><span class="s4">'Success Rate (%)'</span><span class="s0">)</span>
<a name="l2138"><span class="ln">2138 </span></a><span class="s0">plot_metric_variability(dqn_session_results</span><span class="s5">, </span><span class="s0">ppo_session_results</span><span class="s5">, </span><span class="s4">'avg_reward'</span><span class="s5">, </span><span class="s4">'Average Reward'</span><span class="s0">)</span>
<a name="l2139"><span class="ln">2139 </span></a><span class="s0">plot_metric_variability(dqn_session_results</span><span class="s5">, </span><span class="s0">ppo_session_results</span><span class="s5">, </span><span class="s4">'avg_fuel'</span><span class="s5">, </span><span class="s4">'Average Fuel Usage'</span><span class="s0">)</span>
<a name="l2140"><span class="ln">2140 </span></a><span class="s0">plot_metric_variability(dqn_session_results</span><span class="s5">, </span><span class="s0">ppo_session_results</span><span class="s5">, </span><span class="s4">'avg_precision'</span><span class="s5">, </span><span class="s4">'Average Landing Precision'</span><span class="s0">)</span>
<a name="l2141"><span class="ln">2141 </span></a>
<a name="l2142"><span class="ln">2142 </span></a><span class="s0">print(</span><span class="s4">&quot;</span><span class="s7">\n</span><span class="s4">=== Variability Analysis Complete ===&quot;</span><span class="s0">)</span>
<a name="l2143"><span class="ln">2143 </span></a><hr class="ls0"><a name="l2144"><span class="ln">2144 </span></a><span class="s0">#%% md 
<a name="l2145"><span class="ln">2145 </span></a># 9. Video Generation 
<a name="l2146"><span class="ln">2146 </span></a> 
<a name="l2147"><span class="ln">2147 </span></a>## 9.1 Rendering Pipeline 
<a name="l2148"><span class="ln">2148 </span></a> 
<a name="l2149"><span class="ln">2149 </span></a>**Frame capture**: Environment rendered in &quot;rgb_array&quot; mode to capture pixel data at each timestep during agent execution. The rendering pipeline uses `gym.make(&quot;LunarLander-v3&quot;, render_mode=&quot;rgb_array&quot;)` to enable frame-by-frame capture. 
<a name="l2150"><span class="ln">2150 </span></a> 
<a name="l2151"><span class="ln">2151 </span></a>**Episode recording**: Complete episodes recorded from reset to termination, capturing the full landing sequence with step-by-step visual progression. Each frame is captured via `env.render()` and stored in a frames list for subsequent animation creation. 
<a name="l2152"><span class="ln">2152 </span></a> 
<a name="l2153"><span class="ln">2153 </span></a>**Animation creation**: Matplotlib's `FuncAnimation` converts frame sequence into playable video with step counter and reward display overlays, using 100ms intervals between frames for smooth playback. 
<a name="l2154"><span class="ln">2154 </span></a> 
<a name="l2155"><span class="ln">2155 </span></a>## 9.2 Implementation Details 
<a name="l2156"><span class="ln">2156 </span></a> 
<a name="l2157"><span class="ln">2157 </span></a>**Deterministic playback**: 
<a name="l2158"><span class="ln">2158 </span></a> 
<a name="l2159"><span class="ln">2159 </span></a>- **DQN**: Exploration disabled by temporarily setting `agent.epsilon = 0.0` with proper restoration afterward 
<a name="l2160"><span class="ln">2160 </span></a>- **PPO**: Deterministic mode enabled via `model.predict(obs, deterministic=True)` to eliminate stochastic policy sampling 
<a name="l2161"><span class="ln">2161 </span></a>- **Purpose**: Showcases learned policy without random actions for consistent evaluation 
<a name="l2162"><span class="ln">2162 </span></a> 
<a name="l2163"><span class="ln">2163 </span></a>**Frame management**: 
<a name="l2164"><span class="ln">2164 </span></a> 
<a name="l2165"><span class="ln">2165 </span></a>- **Robust None handling**: Each `env.render()` call checked for None return values to prevent corruption 
<a name="l2166"><span class="ln">2166 </span></a>- **Final frame capture**: Additional frame captured after episode termination to ensure complete landing sequence 
<a name="l2167"><span class="ln">2167 </span></a>- **Memory efficiency**: Frames stored in list structure for sequential animation processing 
<a name="l2168"><span class="ln">2168 </span></a> 
<a name="l2169"><span class="ln">2169 </span></a>**Performance monitoring**: 
<a name="l2170"><span class="ln">2170 </span></a> 
<a name="l2171"><span class="ln">2171 </span></a>- **Step counter**: Real-time display of current step and total episode length 
<a name="l2172"><span class="ln">2172 </span></a>- **Cumulative reward**: Live reward tracking with final episode total 
<a name="l2173"><span class="ln">2173 </span></a>- **Episode completion reporting**: Console output showing step count and final reward for analysis 
<a name="l2174"><span class="ln">2174 </span></a> 
<a name="l2175"><span class="ln">2175 </span></a>**Animation Configuration**: 
<a name="l2176"><span class="ln">2176 </span></a> 
<a name="l2177"><span class="ln">2177 </span></a>```python 
<a name="l2178"><span class="ln">2178 </span></a>anim = animation.FuncAnimation( 
<a name="l2179"><span class="ln">2179 </span></a>    fig, animate, frames=len(frames), 
<a name="l2180"><span class="ln">2180 </span></a>    interval=100, repeat=True, blit=False 
<a name="l2181"><span class="ln">2181 </span></a>) 
<a name="l2182"><span class="ln">2182 </span></a>``` 
<a name="l2183"><span class="ln">2183 </span></a> 
<a name="l2184"><span class="ln">2184 </span></a>- **Interval**: 100ms between frames for natural playback speed 
<a name="l2185"><span class="ln">2185 </span></a>- **Repeat**: Continuous looping for extended observation 
<a name="l2186"><span class="ln">2186 </span></a>- **Blit**: Disabled for compatibility with dynamic title updates 
<a name="l2187"><span class="ln">2187 </span></a> 
<a name="l2188"><span class="ln">2188 </span></a> 
<a name="l2189"><span class="ln">2189 </span></a>## 9.3 Jupyter Integration 
<a name="l2190"><span class="ln">2190 </span></a> 
<a name="l2191"><span class="ln">2191 </span></a>**Web-compatible display**: Videos rendered using `IPython.display.HTML(anim.to_jshtml())` for seamless notebook integration without external dependencies. 
<a name="l2192"><span class="ln">2192 </span></a> 
<a name="l2193"><span class="ln">2193 </span></a>**Interactive playback**: Generated videos include standard web video controls (play/pause, seeking) for detailed policy analysis. 
<a name="l2194"><span class="ln">2194 </span></a> 
<a name="l2195"><span class="ln">2195 </span></a>**Real-time generation**: Videos created on-demand during evaluation sessions, enabling immediate visual feedback on training progress. 
<a name="l2196"><span class="ln">2196 </span></a> 
<a name="l2197"><span class="ln">2197 </span></a>## 9.4 Visualization Benefits 
<a name="l2198"><span class="ln">2198 </span></a> 
<a name="l2199"><span class="ln">2199 </span></a>**Policy inspection**: Visual verification of learned behaviors and landing strategies, enabling qualitative assessment of agent performance through observation of trajectory patterns, engine usage, and landing approach angles. 
<a name="l2200"><span class="ln">2200 </span></a> 
<a name="l2201"><span class="ln">2201 </span></a>**Debugging**: Identification of failure modes and suboptimal control patterns through visual observation of agent actions, including premature engine cutoffs, oscillatory behavior, or inefficient trajectories. 
<a name="l2202"><span class="ln">2202 </span></a> 
<a name="l2203"><span class="ln">2203 </span></a>**Algorithm comparison**: Side-by-side analysis of different algorithms' approaches to the landing task, highlighting behavioral differences between DQN's conservative fuel-efficient strategies and PPO's more aggressive success-oriented control patterns. 
<a name="l2204"><span class="ln">2204 </span></a> 
<a name="l2205"><span class="ln">2205 </span></a>**Documentation**: Visual evidence of training success for reporting and presentation purposes, providing compelling demonstrations of learned policies in action. 
<a name="l2206"><span class="ln">2206 </span></a> 
<a name="l2207"><span class="ln">2207 </span></a>## 9.5 Technical Implementation 
<a name="l2208"><span class="ln">2208 </span></a> 
<a name="l2209"><span class="ln">2209 </span></a>**Dual video generation**: Separate functions (`create_agent_video` and `create_ppo_video`) handle algorithm-specific requirements while maintaining consistent output format. 
<a name="l2210"><span class="ln">2210 </span></a> 
<a name="l2211"><span class="ln">2211 </span></a>**Error recovery**: Comprehensive error handling prevents crashes from empty frame lists or rendering failures, with graceful fallbacks and informative error messages. 
<a name="l2212"><span class="ln">2212 </span></a> 
<a name="l2213"><span class="ln">2213 </span></a>**Environment cleanup**: Proper environment closure after video generation prevents memory leaks and resource conflicts in extended evaluation sessions. 
<a name="l2214"><span class="ln">2214 </span></a> 
<a name="l2215"><span class="ln">2215 </span></a>**Maximum episode protection**: 1000-step limit prevents infinite episodes from corrupting video generation process while allowing complete landing sequences to be captured. <hr class="ls0"><a name="l2216"><span class="ln">2216 </span></a>#%% 
<a name="l2217"><span class="ln">2217 </span></a></span><span class="s2">import </span><span class="s0">matplotlib.pyplot </span><span class="s2">as </span><span class="s0">plt</span>
<a name="l2218"><span class="ln">2218 </span></a><span class="s2">import </span><span class="s0">matplotlib.animation </span><span class="s2">as </span><span class="s0">animation</span>
<a name="l2219"><span class="ln">2219 </span></a><span class="s2">from </span><span class="s0">IPython.display </span><span class="s2">import </span><span class="s0">HTML</span>
<a name="l2220"><span class="ln">2220 </span></a>
<a name="l2221"><span class="ln">2221 </span></a>
<a name="l2222"><span class="ln">2222 </span></a><span class="s2">def </span><span class="s0">create_agent_video(agent</span><span class="s5">, </span><span class="s0">env</span><span class="s5">, </span><span class="s0">max_steps=</span><span class="s6">1000</span><span class="s0">):</span>
<a name="l2223"><span class="ln">2223 </span></a>
<a name="l2224"><span class="ln">2224 </span></a>    <span class="s3"># Disable exploration for demonstration</span>
<a name="l2225"><span class="ln">2225 </span></a>    <span class="s0">original_epsilon = agent.epsilon</span>
<a name="l2226"><span class="ln">2226 </span></a>    <span class="s0">agent.epsilon = </span><span class="s6">0.0</span>
<a name="l2227"><span class="ln">2227 </span></a>
<a name="l2228"><span class="ln">2228 </span></a>    <span class="s0">frames = []</span>
<a name="l2229"><span class="ln">2229 </span></a>    <span class="s0">obs</span><span class="s5">, </span><span class="s0">_ = env.reset()</span>
<a name="l2230"><span class="ln">2230 </span></a>    <span class="s0">total_reward = </span><span class="s6">0</span>
<a name="l2231"><span class="ln">2231 </span></a>    <span class="s0">step_count = </span><span class="s6">0</span>
<a name="l2232"><span class="ln">2232 </span></a>
<a name="l2233"><span class="ln">2233 </span></a>    <span class="s2">while </span><span class="s0">step_count &lt; max_steps:</span>
<a name="l2234"><span class="ln">2234 </span></a>        <span class="s3"># Render frame</span>
<a name="l2235"><span class="ln">2235 </span></a>        <span class="s0">frame = env.render()</span>
<a name="l2236"><span class="ln">2236 </span></a>        <span class="s2">if </span><span class="s0">frame </span><span class="s2">is not None</span><span class="s0">:</span>
<a name="l2237"><span class="ln">2237 </span></a>            <span class="s0">frames.append(frame)</span>
<a name="l2238"><span class="ln">2238 </span></a>
<a name="l2239"><span class="ln">2239 </span></a>        <span class="s3"># Get action and step</span>
<a name="l2240"><span class="ln">2240 </span></a>        <span class="s0">action = agent.act(obs)</span>
<a name="l2241"><span class="ln">2241 </span></a>        <span class="s0">obs</span><span class="s5">, </span><span class="s0">reward</span><span class="s5">, </span><span class="s0">terminated</span><span class="s5">, </span><span class="s0">truncated</span><span class="s5">, </span><span class="s0">_ = env.step(action)</span>
<a name="l2242"><span class="ln">2242 </span></a>        <span class="s0">total_reward += reward</span>
<a name="l2243"><span class="ln">2243 </span></a>        <span class="s0">step_count += </span><span class="s6">1</span>
<a name="l2244"><span class="ln">2244 </span></a>
<a name="l2245"><span class="ln">2245 </span></a>        <span class="s2">if </span><span class="s0">terminated </span><span class="s2">or </span><span class="s0">truncated:</span>
<a name="l2246"><span class="ln">2246 </span></a>            <span class="s3"># Capture final frame</span>
<a name="l2247"><span class="ln">2247 </span></a>            <span class="s0">final_frame = env.render()</span>
<a name="l2248"><span class="ln">2248 </span></a>            <span class="s2">if </span><span class="s0">final_frame </span><span class="s2">is not None</span><span class="s0">:</span>
<a name="l2249"><span class="ln">2249 </span></a>                <span class="s0">frames.append(final_frame)</span>
<a name="l2250"><span class="ln">2250 </span></a>            <span class="s2">break</span>
<a name="l2251"><span class="ln">2251 </span></a>
<a name="l2252"><span class="ln">2252 </span></a>    <span class="s3"># Restore epsilon</span>
<a name="l2253"><span class="ln">2253 </span></a>    <span class="s0">agent.epsilon = original_epsilon</span>
<a name="l2254"><span class="ln">2254 </span></a>
<a name="l2255"><span class="ln">2255 </span></a>    <span class="s0">print(</span><span class="s4">f&quot;DQN Episode completed in </span><span class="s7">{</span><span class="s0">step_count</span><span class="s7">} </span><span class="s4">steps with reward: </span><span class="s7">{</span><span class="s0">total_reward</span><span class="s7">:</span><span class="s4">.2f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l2256"><span class="ln">2256 </span></a>
<a name="l2257"><span class="ln">2257 </span></a>    <span class="s2">if not </span><span class="s0">frames:</span>
<a name="l2258"><span class="ln">2258 </span></a>        <span class="s0">print(</span><span class="s4">&quot;No frames captured!&quot;</span><span class="s0">)</span>
<a name="l2259"><span class="ln">2259 </span></a>        <span class="s2">return None</span><span class="s5">, </span><span class="s0">total_reward</span>
<a name="l2260"><span class="ln">2260 </span></a>
<a name="l2261"><span class="ln">2261 </span></a>    <span class="s3"># Create animation</span>
<a name="l2262"><span class="ln">2262 </span></a>    <span class="s0">fig</span><span class="s5">, </span><span class="s0">ax = plt.subplots(figsize=(</span><span class="s6">10</span><span class="s5">, </span><span class="s6">8</span><span class="s0">))</span>
<a name="l2263"><span class="ln">2263 </span></a>    <span class="s0">ax.axis(</span><span class="s4">'off'</span><span class="s0">)</span>
<a name="l2264"><span class="ln">2264 </span></a>
<a name="l2265"><span class="ln">2265 </span></a>    <span class="s2">def </span><span class="s0">animate(frame_idx):</span>
<a name="l2266"><span class="ln">2266 </span></a>        <span class="s0">ax.clear()</span>
<a name="l2267"><span class="ln">2267 </span></a>        <span class="s0">ax.imshow(frames[frame_idx])</span>
<a name="l2268"><span class="ln">2268 </span></a>        <span class="s0">ax.set_title(</span><span class="s4">f&quot;DQN Agent - Step </span><span class="s7">{</span><span class="s0">frame_idx + </span><span class="s6">1</span><span class="s7">}</span><span class="s4">/</span><span class="s7">{</span><span class="s0">len(frames)</span><span class="s7">} </span><span class="s4">- Total Reward: </span><span class="s7">{</span><span class="s0">total_reward</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l2269"><span class="ln">2269 </span></a>        <span class="s0">ax.axis(</span><span class="s4">'off'</span><span class="s0">)</span>
<a name="l2270"><span class="ln">2270 </span></a>
<a name="l2271"><span class="ln">2271 </span></a>    <span class="s0">anim = animation.FuncAnimation(</span>
<a name="l2272"><span class="ln">2272 </span></a>        <span class="s0">fig</span><span class="s5">, </span><span class="s0">animate</span><span class="s5">, </span><span class="s0">frames=len(frames)</span><span class="s5">,</span>
<a name="l2273"><span class="ln">2273 </span></a>        <span class="s0">interval=</span><span class="s6">100</span><span class="s5">, </span><span class="s0">repeat=</span><span class="s2">True</span><span class="s5">, </span><span class="s0">blit=</span><span class="s2">False</span>
<a name="l2274"><span class="ln">2274 </span></a>    <span class="s0">)</span>
<a name="l2275"><span class="ln">2275 </span></a>
<a name="l2276"><span class="ln">2276 </span></a>    <span class="s2">return </span><span class="s0">anim</span><span class="s5">, </span><span class="s0">total_reward</span>
<a name="l2277"><span class="ln">2277 </span></a>
<a name="l2278"><span class="ln">2278 </span></a>
<a name="l2279"><span class="ln">2279 </span></a><span class="s2">def </span><span class="s0">create_ppo_video(model</span><span class="s5">, </span><span class="s0">env</span><span class="s5">, </span><span class="s0">max_steps=</span><span class="s6">1000</span><span class="s0">):</span>
<a name="l2280"><span class="ln">2280 </span></a>
<a name="l2281"><span class="ln">2281 </span></a>    <span class="s0">frames = []</span>
<a name="l2282"><span class="ln">2282 </span></a>    <span class="s0">obs</span><span class="s5">, </span><span class="s0">_ = env.reset()</span>
<a name="l2283"><span class="ln">2283 </span></a>    <span class="s0">total_reward = </span><span class="s6">0</span>
<a name="l2284"><span class="ln">2284 </span></a>    <span class="s0">step_count = </span><span class="s6">0</span>
<a name="l2285"><span class="ln">2285 </span></a>
<a name="l2286"><span class="ln">2286 </span></a>    <span class="s2">while </span><span class="s0">step_count &lt; max_steps:</span>
<a name="l2287"><span class="ln">2287 </span></a>        <span class="s0">frame = env.render()</span>
<a name="l2288"><span class="ln">2288 </span></a>        <span class="s2">if </span><span class="s0">frame </span><span class="s2">is not None</span><span class="s0">:</span>
<a name="l2289"><span class="ln">2289 </span></a>            <span class="s0">frames.append(frame)</span>
<a name="l2290"><span class="ln">2290 </span></a>
<a name="l2291"><span class="ln">2291 </span></a>        <span class="s0">action</span><span class="s5">, </span><span class="s0">_ = model.predict(obs</span><span class="s5">, </span><span class="s0">deterministic=</span><span class="s2">True</span><span class="s0">)</span>
<a name="l2292"><span class="ln">2292 </span></a>        <span class="s0">obs</span><span class="s5">, </span><span class="s0">reward</span><span class="s5">, </span><span class="s0">terminated</span><span class="s5">, </span><span class="s0">truncated</span><span class="s5">, </span><span class="s0">_ = env.step(action)</span>
<a name="l2293"><span class="ln">2293 </span></a>        <span class="s0">total_reward += reward</span>
<a name="l2294"><span class="ln">2294 </span></a>        <span class="s0">step_count += </span><span class="s6">1</span>
<a name="l2295"><span class="ln">2295 </span></a>
<a name="l2296"><span class="ln">2296 </span></a>        <span class="s2">if </span><span class="s0">terminated </span><span class="s2">or </span><span class="s0">truncated:</span>
<a name="l2297"><span class="ln">2297 </span></a>            <span class="s0">final_frame = env.render()</span>
<a name="l2298"><span class="ln">2298 </span></a>            <span class="s2">if </span><span class="s0">final_frame </span><span class="s2">is not None</span><span class="s0">:</span>
<a name="l2299"><span class="ln">2299 </span></a>                <span class="s0">frames.append(final_frame)</span>
<a name="l2300"><span class="ln">2300 </span></a>            <span class="s2">break</span>
<a name="l2301"><span class="ln">2301 </span></a>
<a name="l2302"><span class="ln">2302 </span></a>    <span class="s0">print(</span><span class="s4">f&quot;PPO Episode completed in </span><span class="s7">{</span><span class="s0">step_count</span><span class="s7">} </span><span class="s4">steps with reward: </span><span class="s7">{</span><span class="s0">total_reward</span><span class="s7">:</span><span class="s4">.2f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l2303"><span class="ln">2303 </span></a>
<a name="l2304"><span class="ln">2304 </span></a>    <span class="s2">if not </span><span class="s0">frames:</span>
<a name="l2305"><span class="ln">2305 </span></a>        <span class="s0">print(</span><span class="s4">&quot;No frames captured!&quot;</span><span class="s0">)</span>
<a name="l2306"><span class="ln">2306 </span></a>        <span class="s2">return None</span><span class="s5">, </span><span class="s0">total_reward</span>
<a name="l2307"><span class="ln">2307 </span></a>
<a name="l2308"><span class="ln">2308 </span></a>    <span class="s3"># Create animation</span>
<a name="l2309"><span class="ln">2309 </span></a>    <span class="s0">fig</span><span class="s5">, </span><span class="s0">ax = plt.subplots(figsize=(</span><span class="s6">10</span><span class="s5">, </span><span class="s6">8</span><span class="s0">))</span>
<a name="l2310"><span class="ln">2310 </span></a>    <span class="s0">ax.axis(</span><span class="s4">'off'</span><span class="s0">)</span>
<a name="l2311"><span class="ln">2311 </span></a>
<a name="l2312"><span class="ln">2312 </span></a>    <span class="s2">def </span><span class="s0">animate(frame_idx):</span>
<a name="l2313"><span class="ln">2313 </span></a>        <span class="s0">ax.clear()</span>
<a name="l2314"><span class="ln">2314 </span></a>        <span class="s0">ax.imshow(frames[frame_idx])</span>
<a name="l2315"><span class="ln">2315 </span></a>        <span class="s0">ax.set_title(</span><span class="s4">f&quot;PPO Agent - Step </span><span class="s7">{</span><span class="s0">frame_idx + </span><span class="s6">1</span><span class="s7">}</span><span class="s4">/</span><span class="s7">{</span><span class="s0">len(frames)</span><span class="s7">} </span><span class="s4">- Total Reward: </span><span class="s7">{</span><span class="s0">total_reward</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s0">)</span>
<a name="l2316"><span class="ln">2316 </span></a>        <span class="s0">ax.axis(</span><span class="s4">'off'</span><span class="s0">)</span>
<a name="l2317"><span class="ln">2317 </span></a>
<a name="l2318"><span class="ln">2318 </span></a>    <span class="s0">anim = animation.FuncAnimation(</span>
<a name="l2319"><span class="ln">2319 </span></a>        <span class="s0">fig</span><span class="s5">, </span><span class="s0">animate</span><span class="s5">, </span><span class="s0">frames=len(frames)</span><span class="s5">,</span>
<a name="l2320"><span class="ln">2320 </span></a>        <span class="s0">interval=</span><span class="s6">100</span><span class="s5">, </span><span class="s0">repeat=</span><span class="s2">True</span><span class="s5">, </span><span class="s0">blit=</span><span class="s2">False</span>
<a name="l2321"><span class="ln">2321 </span></a>    <span class="s0">)</span>
<a name="l2322"><span class="ln">2322 </span></a>
<a name="l2323"><span class="ln">2323 </span></a>    <span class="s2">return </span><span class="s0">anim</span><span class="s5">, </span><span class="s0">total_reward</span>
<a name="l2324"><span class="ln">2324 </span></a>
<a name="l2325"><span class="ln">2325 </span></a>
<a name="l2326"><span class="ln">2326 </span></a><span class="s3"># Generate videos</span>
<a name="l2327"><span class="ln">2327 </span></a><span class="s0">print(</span><span class="s4">&quot;Creating DQN video...&quot;</span><span class="s0">)</span>
<a name="l2328"><span class="ln">2328 </span></a><span class="s0">env_render = gym.make(</span><span class="s4">&quot;LunarLander-v3&quot;</span><span class="s5">, </span><span class="s0">render_mode=</span><span class="s4">&quot;rgb_array&quot;</span><span class="s0">)</span>
<a name="l2329"><span class="ln">2329 </span></a><span class="s0">dqn_video</span><span class="s5">, </span><span class="s0">dqn_reward = create_agent_video(agent</span><span class="s5">, </span><span class="s0">env_render)</span>
<a name="l2330"><span class="ln">2330 </span></a>
<a name="l2331"><span class="ln">2331 </span></a><span class="s0">print(</span><span class="s4">&quot;Creating PPO video...&quot;</span><span class="s0">)</span>
<a name="l2332"><span class="ln">2332 </span></a><span class="s0">ppo_video</span><span class="s5">, </span><span class="s0">ppo_reward = create_ppo_video(ppo_model</span><span class="s5">, </span><span class="s0">env_render)</span>
<a name="l2333"><span class="ln">2333 </span></a>
<a name="l2334"><span class="ln">2334 </span></a><span class="s0">env_render.close()</span>
<a name="l2335"><span class="ln">2335 </span></a>
<a name="l2336"><span class="ln">2336 </span></a><span class="s3"># Display videos</span>
<a name="l2337"><span class="ln">2337 </span></a><span class="s2">if </span><span class="s0">dqn_video:</span>
<a name="l2338"><span class="ln">2338 </span></a>    <span class="s0">print(</span><span class="s4">&quot;DQN Agent Performance:&quot;</span><span class="s0">)</span>
<a name="l2339"><span class="ln">2339 </span></a>    <span class="s0">display(HTML(dqn_video.to_jshtml()))</span>
<a name="l2340"><span class="ln">2340 </span></a>
<a name="l2341"><span class="ln">2341 </span></a><span class="s2">if </span><span class="s0">ppo_video:</span>
<a name="l2342"><span class="ln">2342 </span></a>    <span class="s0">print(</span><span class="s4">&quot;PPO Agent Performance:&quot;</span><span class="s0">)</span>
<a name="l2343"><span class="ln">2343 </span></a>    <span class="s0">display(HTML(ppo_video.to_jshtml()))</span>
<a name="l2344"><span class="ln">2344 </span></a><hr class="ls0"><a name="l2345"><span class="ln">2345 </span></a><span class="s0">#%% md 
<a name="l2346"><span class="ln">2346 </span></a># 10. Final Results and Comparison Documentation 
<a name="l2347"><span class="ln">2347 </span></a> 
<a name="l2348"><span class="ln">2348 </span></a>## 10.1 Overview 
<a name="l2349"><span class="ln">2349 </span></a> 
<a name="l2350"><span class="ln">2350 </span></a>The Final Results and Comparison section implements a comprehensive visualization framework that consolidates all experimental results into a unified 3x3 dashboard. This section serves as the culminating analysis tool that integrates training performance, hyperparameter studies, custom reward experiments, and final agent evaluations into a single comparative display. 
<a name="l2351"><span class="ln">2351 </span></a> 
<a name="l2352"><span class="ln">2352 </span></a>## 10.2 Visualization Architecture 
<a name="l2353"><span class="ln">2353 </span></a> 
<a name="l2354"><span class="ln">2354 </span></a>### Grid Layout Design 
<a name="l2355"><span class="ln">2355 </span></a> 
<a name="l2356"><span class="ln">2356 </span></a>**3x3 Subplot Grid Structure** 
<a name="l2357"><span class="ln">2357 </span></a> 
<a name="l2358"><span class="ln">2358 </span></a>```python 
<a name="l2359"><span class="ln">2359 </span></a>fig = plt.figure(figsize=(20, 15)) 
<a name="l2360"><span class="ln">2360 </span></a>gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3) 
<a name="l2361"><span class="ln">2361 </span></a>``` 
<a name="l2362"><span class="ln">2362 </span></a> 
<a name="l2363"><span class="ln">2363 </span></a>- **Canvas Size**: 20x15 inches for high-resolution display and detailed analysis 
<a name="l2364"><span class="ln">2364 </span></a>- **Grid Spacing**: 0.3 horizontal and vertical spacing for clear separation between plots 
<a name="l2365"><span class="ln">2365 </span></a>- **Systematic Organization**: Each position strategically assigned to specific analysis components 
<a name="l2366"><span class="ln">2366 </span></a> 
<a name="l2367"><span class="ln">2367 </span></a> 
<a name="l2368"><span class="ln">2368 </span></a>### Individual Plot Components 
<a name="l2369"><span class="ln">2369 </span></a> 
<a name="l2370"><span class="ln">2370 </span></a>**DDQN Training Progress** 
<a name="l2371"><span class="ln">2371 </span></a> 
<a name="l2372"><span class="ln">2372 </span></a>- Displays 100-episode rolling average of training rewards 
<a name="l2373"><span class="ln">2373 </span></a>- Loads training data from memory or pickle file with graceful fallback 
<a name="l2374"><span class="ln">2374 </span></a>- Visualizes convergence patterns and learning stability over episodes 
<a name="l2375"><span class="ln">2375 </span></a> 
<a name="l2376"><span class="ln">2376 </span></a>**Custom Reward Function Comparison** 
<a name="l2377"><span class="ln">2377 </span></a> 
<a name="l2378"><span class="ln">2378 </span></a>- Compares original, fuel-efficient, and precision bonus reward implementations 
<a name="l2379"><span class="ln">2379 </span></a>- Uses 50-episode rolling averages for smoother trend visualization 
<a name="l2380"><span class="ln">2380 </span></a>- Demonstrates impact of reward shaping on learning trajectories 
<a name="l2381"><span class="ln">2381 </span></a> 
<a name="l2382"><span class="ln">2382 </span></a>**Learning Rate Hyperparameter Analysis** 
<a name="l2383"><span class="ln">2383 </span></a> 
<a name="l2384"><span class="ln">2384 </span></a>- Visualizes performance across learning rates (0.0001, 0.0005, 0.001) 
<a name="l2385"><span class="ln">2385 </span></a>- Shows convergence speed and stability trade-offs 
<a name="l2386"><span class="ln">2386 </span></a>- Enables identification of optimal learning rate for the environment 
<a name="l2387"><span class="ln">2387 </span></a> 
<a name="l2388"><span class="ln">2388 </span></a>**Discount Factor (Gamma) Analysis** 
<a name="l2389"><span class="ln">2389 </span></a> 
<a name="l2390"><span class="ln">2390 </span></a>- Compares gamma values (0.95, 0.99, 0.995) impact on learning 
<a name="l2391"><span class="ln">2391 </span></a>- Illustrates temporal credit assignment effects 
<a name="l2392"><span class="ln">2392 </span></a>- Demonstrates planning horizon implications for landing tasks 
<a name="l2393"><span class="ln">2393 </span></a> 
<a name="l2394"><span class="ln">2394 </span></a>**Algorithm Success Rate Comparison** 
<a name="l2395"><span class="ln">2395 </span></a> 
<a name="l2396"><span class="ln">2396 </span></a>- Bar chart comparing DDQN vs PPO success rates 
<a name="l2397"><span class="ln">2397 </span></a>- Includes percentage labels on bars for precise comparison 
<a name="l2398"><span class="ln">2398 </span></a>- Highlights primary performance metric for landing reliability 
<a name="l2399"><span class="ln">2399 </span></a> 
<a name="l2400"><span class="ln">2400 </span></a>**Fuel Efficiency Analysis** 
<a name="l2401"><span class="ln">2401 </span></a> 
<a name="l2402"><span class="ln">2402 </span></a>- Bar chart comparing average fuel consumption between algorithms 
<a name="l2403"><span class="ln">2403 </span></a>- Quantifies resource usage trade-offs between approaches 
<a name="l2404"><span class="ln">2404 </span></a>- Critical metric for practical spacecraft applications 
<a name="l2405"><span class="ln">2405 </span></a> 
<a name="l2406"><span class="ln">2406 </span></a>**Reward Distribution Analysis** 
<a name="l2407"><span class="ln">2407 </span></a> 
<a name="l2408"><span class="ln">2408 </span></a>- Histogram overlay showing reward distribution variability 
<a name="l2409"><span class="ln">2409 </span></a>- Uses multi-session evaluation data for statistical robustness 
<a name="l2410"><span class="ln">2410 </span></a>- Reveals performance consistency and variance patterns 
<a name="l2411"><span class="ln">2411 </span></a> 
<a name="l2412"><span class="ln">2412 </span></a>**Episode Length Comparison** 
<a name="l2413"><span class="ln">2413 </span></a> 
<a name="l2414"><span class="ln">2414 </span></a>- Bar chart showing average steps per episode 
<a name="l2415"><span class="ln">2415 </span></a>- Indicates efficiency of task completion 
<a name="l2416"><span class="ln">2416 </span></a>- Approximated values used when detailed data unavailable 
<a name="l2417"><span class="ln">2417 </span></a> 
<a name="l2418"><span class="ln">2418 </span></a>**Performance Summary Panel** 
<a name="l2419"><span class="ln">2419 </span></a> 
<a name="l2420"><span class="ln">2420 </span></a>- Text-based comprehensive results summary 
<a name="l2421"><span class="ln">2421 </span></a>- Includes success rates, rewards, and fuel usage for both algorithms 
<a name="l2422"><span class="ln">2422 </span></a>- Provides training episode counts and final convergence metrics 
<a name="l2423"><span class="ln">2423 </span></a> 
<a name="l2424"><span class="ln">2424 </span></a> 
<a name="l2425"><span class="ln">2425 </span></a>## 10.3 Data Handling and Robustness 
<a name="l2426"><span class="ln">2426 </span></a> 
<a name="l2427"><span class="ln">2427 </span></a>### Graceful Degradation System 
<a name="l2428"><span class="ln">2428 </span></a> 
<a name="l2429"><span class="ln">2429 </span></a>**Missing Data Handling** 
<a name="l2430"><span class="ln">2430 </span></a> 
<a name="l2431"><span class="ln">2431 </span></a>``` 
<a name="l2432"><span class="ln">2432 </span></a>if 'rewards' in locals() or 'rewards' in globals(): 
<a name="l2433"><span class="ln">2433 </span></a>    # Plot actual data 
<a name="l2434"><span class="ln">2434 </span></a>else: 
<a name="l2435"><span class="ln">2435 </span></a>    # Display placeholder with informative message 
<a name="l2436"><span class="ln">2436 </span></a>``` 
<a name="l2437"><span class="ln">2437 </span></a> 
<a name="l2438"><span class="ln">2438 </span></a>- **Conditional Plotting**: Checks for data availability before visualization 
<a name="l2439"><span class="ln">2439 </span></a>- **Informative Placeholders**: Displays descriptive text when data unavailable 
<a name="l2440"><span class="ln">2440 </span></a>- **Error Prevention**: Prevents crashes from missing experimental results 
<a name="l2441"><span class="ln">2441 </span></a> 
<a name="l2442"><span class="ln">2442 </span></a>**Dynamic Content Loading** 
<a name="l2443"><span class="ln">2443 </span></a> 
<a name="l2444"><span class="ln">2444 </span></a>- **Training Data**: Attempts to load from multiple sources (memory, pickle files) 
<a name="l2445"><span class="ln">2445 </span></a>- **Evaluation Results**: Uses performance metrics if available 
<a name="l2446"><span class="ln">2446 </span></a>- **Experimental Results**: Incorporates hyperparameter and custom reward data when present 
<a name="l2447"><span class="ln">2447 </span></a> 
<a name="l2448"><span class="ln">2448 </span></a>## 10.4 Summary Table Generation 
<a name="l2449"><span class="ln">2449 </span></a> 
<a name="l2450"><span class="ln">2450 </span></a>### Comparative Performance Matrix 
<a name="l2451"><span class="ln">2451 </span></a> 
<a name="l2452"><span class="ln">2452 </span></a>**Structured Data Presentation** 
<a name="l2453"><span class="ln">2453 </span></a> 
<a name="l2454"><span class="ln">2454 </span></a>```python 
<a name="l2455"><span class="ln">2455 </span></a>summary_data = { 
<a name="l2456"><span class="ln">2456 </span></a>    'Metric': ['Algorithm Type', 'Success Rate (%)', 'Avg Reward', 'Avg Fuel Usage'], 
<a name="l2457"><span class="ln">2457 </span></a>    'DQN': ['Value-based (Off-policy)', f&quot;{dqn_performance['success_rate']:.1f}&quot;, ...], 
<a name="l2458"><span class="ln">2458 </span></a>    'PPO': ['Policy-based (On-policy)', f&quot;{ppo_performance['success_rate']:.1f}&quot;, ...] 
<a name="l2459"><span class="ln">2459 </span></a>} 
<a name="l2460"><span class="ln">2460 </span></a>``` 
<a name="l2461"><span class="ln">2461 </span></a> 
<a name="l2462"><span class="ln">2462 </span></a>**Key Features** 
<a name="l2463"><span class="ln">2463 </span></a> 
<a name="l2464"><span class="ln">2464 </span></a>- **Algorithm Classification**: Distinguishes value-based vs policy-based approaches 
<a name="l2465"><span class="ln">2465 </span></a>- **Performance Metrics**: Success rate, average reward, and fuel efficiency 
<a name="l2466"><span class="ln">2466 </span></a>- **Formatted Output**: Consistent decimal precision for professional presentation 
<a name="l2467"><span class="ln">2467 </span></a>- **Pandas Integration**: Uses DataFrame for clean tabular display 
<a name="l2468"><span class="ln">2468 </span></a> <hr class="ls0"><a name="l2469"><span class="ln">2469 </span></a>#%% 
<a name="l2470"><span class="ln">2470 </span></a></span><span class="s3"># Create comparison with all results</span>
<a name="l2471"><span class="ln">2471 </span></a><span class="s0">fig = plt.figure(figsize=(</span><span class="s6">20</span><span class="s5">, </span><span class="s6">15</span><span class="s0">))</span>
<a name="l2472"><span class="ln">2472 </span></a>
<a name="l2473"><span class="ln">2473 </span></a><span class="s3"># Create 3x3 grid for better organization</span>
<a name="l2474"><span class="ln">2474 </span></a><span class="s0">gs = fig.add_gridspec(</span><span class="s6">3</span><span class="s5">, </span><span class="s6">3</span><span class="s5">, </span><span class="s0">hspace=</span><span class="s6">0.3</span><span class="s5">, </span><span class="s0">wspace=</span><span class="s6">0.3</span><span class="s0">)</span>
<a name="l2475"><span class="ln">2475 </span></a>
<a name="l2476"><span class="ln">2476 </span></a><span class="s3"># 1. Training curves comparison (top left)</span>
<a name="l2477"><span class="ln">2477 </span></a><span class="s0">ax1 = fig.add_subplot(gs[</span><span class="s6">0</span><span class="s5">, </span><span class="s6">0</span><span class="s0">])</span>
<a name="l2478"><span class="ln">2478 </span></a><span class="s2">if </span><span class="s4">'rewards' </span><span class="s2">in </span><span class="s0">locals() </span><span class="s2">or </span><span class="s4">'rewards' </span><span class="s2">in </span><span class="s0">globals():</span>
<a name="l2479"><span class="ln">2479 </span></a>    <span class="s0">ax1.plot(pd.Series(rewards).rolling(</span><span class="s6">100</span><span class="s0">).mean()</span><span class="s5">, </span><span class="s0">label=</span><span class="s4">'DQN'</span><span class="s5">, </span><span class="s0">linewidth=</span><span class="s6">2</span><span class="s5">, </span><span class="s0">color=</span><span class="s4">'blue'</span><span class="s0">)</span>
<a name="l2480"><span class="ln">2480 </span></a>    <span class="s0">ax1.set_title(</span><span class="s4">'DDQN Training Progress'</span><span class="s0">)</span>
<a name="l2481"><span class="ln">2481 </span></a>    <span class="s0">ax1.set_xlabel(</span><span class="s4">'Episode'</span><span class="s0">)</span>
<a name="l2482"><span class="ln">2482 </span></a>    <span class="s0">ax1.set_ylabel(</span><span class="s4">'Average Reward (100-episode rolling)'</span><span class="s0">)</span>
<a name="l2483"><span class="ln">2483 </span></a>    <span class="s0">ax1.legend()</span>
<a name="l2484"><span class="ln">2484 </span></a><span class="s2">else</span><span class="s0">:</span>
<a name="l2485"><span class="ln">2485 </span></a>    <span class="s3"># Create dummy data or load from file if available</span>
<a name="l2486"><span class="ln">2486 </span></a>    <span class="s2">try</span><span class="s0">:</span>
<a name="l2487"><span class="ln">2487 </span></a>        <span class="s3"># Try to load training data if saved</span>
<a name="l2488"><span class="ln">2488 </span></a>        <span class="s2">import </span><span class="s0">pickle</span>
<a name="l2489"><span class="ln">2489 </span></a>
<a name="l2490"><span class="ln">2490 </span></a>        <span class="s2">with </span><span class="s0">open(</span><span class="s4">'training_rewards.pkl'</span><span class="s5">, </span><span class="s4">'rb'</span><span class="s0">) </span><span class="s2">as </span><span class="s0">f:</span>
<a name="l2491"><span class="ln">2491 </span></a>            <span class="s0">rewards = pickle.load(f)</span>
<a name="l2492"><span class="ln">2492 </span></a>        <span class="s0">ax1.plot(pd.Series(rewards).rolling(</span><span class="s6">100</span><span class="s0">).mean()</span><span class="s5">, </span><span class="s0">label=</span><span class="s4">'DQN'</span><span class="s5">, </span><span class="s0">linewidth=</span><span class="s6">2</span><span class="s5">, </span><span class="s0">color=</span><span class="s4">'blue'</span><span class="s0">)</span>
<a name="l2493"><span class="ln">2493 </span></a>        <span class="s0">ax1.set_title(</span><span class="s4">'DDQN Training Progress (Loaded)'</span><span class="s0">)</span>
<a name="l2494"><span class="ln">2494 </span></a>    <span class="s2">except</span><span class="s0">:</span>
<a name="l2495"><span class="ln">2495 </span></a>        <span class="s3"># If no saved data, show placeholder</span>
<a name="l2496"><span class="ln">2496 </span></a>        <span class="s0">ax1.text(</span><span class="s6">0.5</span><span class="s5">, </span><span class="s6">0.5</span><span class="s5">, </span><span class="s4">'DQN Training Data</span><span class="s7">\n</span><span class="s4">Not Available'</span><span class="s5">,</span>
<a name="l2497"><span class="ln">2497 </span></a>                 <span class="s0">ha=</span><span class="s4">'center'</span><span class="s5">, </span><span class="s0">va=</span><span class="s4">'center'</span><span class="s5">, </span><span class="s0">transform=ax1.transAxes</span><span class="s5">,</span>
<a name="l2498"><span class="ln">2498 </span></a>                 <span class="s0">fontsize=</span><span class="s6">12</span><span class="s5">, </span><span class="s0">style=</span><span class="s4">'italic'</span><span class="s0">)</span>
<a name="l2499"><span class="ln">2499 </span></a>        <span class="s0">ax1.set_title(</span><span class="s4">'DQN Training Progress'</span><span class="s0">)</span>
<a name="l2500"><span class="ln">2500 </span></a>    <span class="s0">ax1.set_xlabel(</span><span class="s4">'Episode'</span><span class="s0">)</span>
<a name="l2501"><span class="ln">2501 </span></a>    <span class="s0">ax1.set_ylabel(</span><span class="s4">'Average Reward'</span><span class="s0">)</span>
<a name="l2502"><span class="ln">2502 </span></a><span class="s0">ax1.grid(</span><span class="s2">True</span><span class="s0">)</span>
<a name="l2503"><span class="ln">2503 </span></a>
<a name="l2504"><span class="ln">2504 </span></a><span class="s3"># 2. Custom reward comparison (top middle)</span>
<a name="l2505"><span class="ln">2505 </span></a><span class="s0">ax2 = fig.add_subplot(gs[</span><span class="s6">0</span><span class="s5">, </span><span class="s6">1</span><span class="s0">])</span>
<a name="l2506"><span class="ln">2506 </span></a><span class="s2">if </span><span class="s4">'custom_results' </span><span class="s2">in </span><span class="s0">locals():</span>
<a name="l2507"><span class="ln">2507 </span></a>    <span class="s2">for </span><span class="s0">reward_type</span><span class="s5">, </span><span class="s0">reward_data </span><span class="s2">in </span><span class="s0">custom_results.items():</span>
<a name="l2508"><span class="ln">2508 </span></a>        <span class="s0">rolling_avg = pd.Series(reward_data).rolling(</span><span class="s6">50</span><span class="s0">).mean()</span>
<a name="l2509"><span class="ln">2509 </span></a>        <span class="s0">ax2.plot(rolling_avg</span><span class="s5">, </span><span class="s0">label=reward_type</span><span class="s5">, </span><span class="s0">linewidth=</span><span class="s6">2</span><span class="s0">)</span>
<a name="l2510"><span class="ln">2510 </span></a>    <span class="s0">ax2.legend()</span>
<a name="l2511"><span class="ln">2511 </span></a><span class="s2">else</span><span class="s0">:</span>
<a name="l2512"><span class="ln">2512 </span></a>    <span class="s0">ax2.text(</span><span class="s6">0.5</span><span class="s5">, </span><span class="s6">0.5</span><span class="s5">, </span><span class="s4">'Custom Reward</span><span class="s7">\n</span><span class="s4">Experiments</span><span class="s7">\n</span><span class="s4">Completed'</span><span class="s5">,</span>
<a name="l2513"><span class="ln">2513 </span></a>             <span class="s0">ha=</span><span class="s4">'center'</span><span class="s5">, </span><span class="s0">va=</span><span class="s4">'center'</span><span class="s5">, </span><span class="s0">transform=ax2.transAxes</span><span class="s5">,</span>
<a name="l2514"><span class="ln">2514 </span></a>             <span class="s0">fontsize=</span><span class="s6">12</span><span class="s5">, </span><span class="s0">style=</span><span class="s4">'italic'</span><span class="s0">)</span>
<a name="l2515"><span class="ln">2515 </span></a><span class="s0">ax2.set_title(</span><span class="s4">'Custom Reward Functions'</span><span class="s0">)</span>
<a name="l2516"><span class="ln">2516 </span></a><span class="s0">ax2.set_xlabel(</span><span class="s4">'Episode'</span><span class="s0">)</span>
<a name="l2517"><span class="ln">2517 </span></a><span class="s0">ax2.set_ylabel(</span><span class="s4">'Average Reward'</span><span class="s0">)</span>
<a name="l2518"><span class="ln">2518 </span></a><span class="s0">ax2.grid(</span><span class="s2">True</span><span class="s0">)</span>
<a name="l2519"><span class="ln">2519 </span></a>
<a name="l2520"><span class="ln">2520 </span></a><span class="s3"># 3. Hyperparameter comparison - Learning Rate (top right)</span>
<a name="l2521"><span class="ln">2521 </span></a><span class="s0">ax3 = fig.add_subplot(gs[</span><span class="s6">0</span><span class="s5">, </span><span class="s6">2</span><span class="s0">])</span>
<a name="l2522"><span class="ln">2522 </span></a><span class="s2">if </span><span class="s4">'lr_results' </span><span class="s2">in </span><span class="s0">locals():</span>
<a name="l2523"><span class="ln">2523 </span></a>    <span class="s2">for </span><span class="s0">lr</span><span class="s5">, </span><span class="s0">rewards_lr </span><span class="s2">in </span><span class="s0">lr_results.items():</span>
<a name="l2524"><span class="ln">2524 </span></a>        <span class="s0">rolling_avg = pd.Series(rewards_lr).rolling(</span><span class="s6">50</span><span class="s0">).mean()</span>
<a name="l2525"><span class="ln">2525 </span></a>        <span class="s0">ax3.plot(rolling_avg</span><span class="s5">, </span><span class="s0">label=</span><span class="s4">f'LR=</span><span class="s7">{</span><span class="s0">lr</span><span class="s7">}</span><span class="s4">'</span><span class="s5">, </span><span class="s0">linewidth=</span><span class="s6">2</span><span class="s0">)</span>
<a name="l2526"><span class="ln">2526 </span></a>    <span class="s0">ax3.legend()</span>
<a name="l2527"><span class="ln">2527 </span></a><span class="s2">else</span><span class="s0">:</span>
<a name="l2528"><span class="ln">2528 </span></a>    <span class="s0">ax3.text(</span><span class="s6">0.5</span><span class="s5">, </span><span class="s6">0.5</span><span class="s5">, </span><span class="s4">'Hyperparameter</span><span class="s7">\n</span><span class="s4">Comparison</span><span class="s7">\n</span><span class="s4">Not Completed'</span><span class="s5">,</span>
<a name="l2529"><span class="ln">2529 </span></a>             <span class="s0">ha=</span><span class="s4">'center'</span><span class="s5">, </span><span class="s0">va=</span><span class="s4">'center'</span><span class="s5">, </span><span class="s0">transform=ax3.transAxes</span><span class="s5">,</span>
<a name="l2530"><span class="ln">2530 </span></a>             <span class="s0">fontsize=</span><span class="s6">12</span><span class="s5">, </span><span class="s0">style=</span><span class="s4">'italic'</span><span class="s0">)</span>
<a name="l2531"><span class="ln">2531 </span></a><span class="s0">ax3.set_title(</span><span class="s4">'Learning Rate Comparison'</span><span class="s0">)</span>
<a name="l2532"><span class="ln">2532 </span></a><span class="s0">ax3.set_xlabel(</span><span class="s4">'Episode'</span><span class="s0">)</span>
<a name="l2533"><span class="ln">2533 </span></a><span class="s0">ax3.set_ylabel(</span><span class="s4">'Average Reward'</span><span class="s0">)</span>
<a name="l2534"><span class="ln">2534 </span></a><span class="s0">ax3.grid(</span><span class="s2">True</span><span class="s0">)</span>
<a name="l2535"><span class="ln">2535 </span></a>
<a name="l2536"><span class="ln">2536 </span></a><span class="s3"># 4. Hyperparameter comparison - Gamma (middle left)</span>
<a name="l2537"><span class="ln">2537 </span></a><span class="s0">ax4 = fig.add_subplot(gs[</span><span class="s6">1</span><span class="s5">, </span><span class="s6">0</span><span class="s0">])</span>
<a name="l2538"><span class="ln">2538 </span></a><span class="s2">if </span><span class="s4">'gamma_results' </span><span class="s2">in </span><span class="s0">locals():</span>
<a name="l2539"><span class="ln">2539 </span></a>    <span class="s2">for </span><span class="s0">gamma</span><span class="s5">, </span><span class="s0">rewards_gamma </span><span class="s2">in </span><span class="s0">gamma_results.items():</span>
<a name="l2540"><span class="ln">2540 </span></a>        <span class="s0">rolling_avg = pd.Series(rewards_gamma).rolling(</span><span class="s6">50</span><span class="s0">).mean()</span>
<a name="l2541"><span class="ln">2541 </span></a>        <span class="s0">ax4.plot(rolling_avg</span><span class="s5">, </span><span class="s0">label=</span><span class="s4">f'γ=</span><span class="s7">{</span><span class="s0">gamma</span><span class="s7">}</span><span class="s4">'</span><span class="s5">, </span><span class="s0">linewidth=</span><span class="s6">2</span><span class="s0">)</span>
<a name="l2542"><span class="ln">2542 </span></a>    <span class="s0">ax4.legend()</span>
<a name="l2543"><span class="ln">2543 </span></a><span class="s2">else</span><span class="s0">:</span>
<a name="l2544"><span class="ln">2544 </span></a>    <span class="s0">ax4.text(</span><span class="s6">0.5</span><span class="s5">, </span><span class="s6">0.5</span><span class="s5">, </span><span class="s4">'Gamma Analysis</span><span class="s7">\n</span><span class="s4">Not Completed'</span><span class="s5">,</span>
<a name="l2545"><span class="ln">2545 </span></a>             <span class="s0">ha=</span><span class="s4">'center'</span><span class="s5">, </span><span class="s0">va=</span><span class="s4">'center'</span><span class="s5">, </span><span class="s0">transform=ax4.transAxes</span><span class="s5">,</span>
<a name="l2546"><span class="ln">2546 </span></a>             <span class="s0">fontsize=</span><span class="s6">12</span><span class="s5">, </span><span class="s0">style=</span><span class="s4">'italic'</span><span class="s0">)</span>
<a name="l2547"><span class="ln">2547 </span></a><span class="s0">ax4.set_title(</span><span class="s4">'Discount Factor Comparison'</span><span class="s0">)</span>
<a name="l2548"><span class="ln">2548 </span></a><span class="s0">ax4.set_xlabel(</span><span class="s4">'Episode'</span><span class="s0">)</span>
<a name="l2549"><span class="ln">2549 </span></a><span class="s0">ax4.set_ylabel(</span><span class="s4">'Average Reward'</span><span class="s0">)</span>
<a name="l2550"><span class="ln">2550 </span></a><span class="s0">ax4.grid(</span><span class="s2">True</span><span class="s0">)</span>
<a name="l2551"><span class="ln">2551 </span></a>
<a name="l2552"><span class="ln">2552 </span></a><span class="s3"># 5. Success rate comparison (middle middle)</span>
<a name="l2553"><span class="ln">2553 </span></a><span class="s0">ax5 = fig.add_subplot(gs[</span><span class="s6">1</span><span class="s5">, </span><span class="s6">1</span><span class="s0">])</span>
<a name="l2554"><span class="ln">2554 </span></a><span class="s2">if </span><span class="s4">'dqn_performance' </span><span class="s2">in </span><span class="s0">locals() </span><span class="s2">and </span><span class="s4">'ppo_performance' </span><span class="s2">in </span><span class="s0">locals():</span>
<a name="l2555"><span class="ln">2555 </span></a>    <span class="s0">algorithms = [</span><span class="s4">'DQN'</span><span class="s5">, </span><span class="s4">'PPO'</span><span class="s0">]</span>
<a name="l2556"><span class="ln">2556 </span></a>    <span class="s0">success_rates = [</span>
<a name="l2557"><span class="ln">2557 </span></a>        <span class="s0">dqn_performance[</span><span class="s4">'success_rate'</span><span class="s0">]</span><span class="s5">,</span>
<a name="l2558"><span class="ln">2558 </span></a>        <span class="s0">ppo_performance[</span><span class="s4">'success_rate'</span><span class="s0">]</span>
<a name="l2559"><span class="ln">2559 </span></a>    <span class="s0">]</span>
<a name="l2560"><span class="ln">2560 </span></a>    <span class="s0">bars = ax5.bar(algorithms</span><span class="s5">, </span><span class="s0">success_rates</span><span class="s5">, </span><span class="s0">color=[</span><span class="s4">'blue'</span><span class="s5">, </span><span class="s4">'orange'</span><span class="s0">])</span>
<a name="l2561"><span class="ln">2561 </span></a>    <span class="s0">ax5.set_title(</span><span class="s4">'Success Rate Comparison'</span><span class="s0">)</span>
<a name="l2562"><span class="ln">2562 </span></a>    <span class="s0">ax5.set_ylabel(</span><span class="s4">'Success Rate (%)'</span><span class="s0">)</span>
<a name="l2563"><span class="ln">2563 </span></a>    <span class="s3"># Add value labels on bars</span>
<a name="l2564"><span class="ln">2564 </span></a>    <span class="s2">for </span><span class="s0">bar</span><span class="s5">, </span><span class="s0">rate </span><span class="s2">in </span><span class="s0">zip(bars</span><span class="s5">, </span><span class="s0">success_rates):</span>
<a name="l2565"><span class="ln">2565 </span></a>        <span class="s0">ax5.text(bar.get_x() + bar.get_width() / </span><span class="s6">2</span><span class="s5">, </span><span class="s0">bar.get_height() + </span><span class="s6">1</span><span class="s5">,</span>
<a name="l2566"><span class="ln">2566 </span></a>                 <span class="s4">f'</span><span class="s7">{</span><span class="s0">rate</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">%'</span><span class="s5">, </span><span class="s0">ha=</span><span class="s4">'center'</span><span class="s5">, </span><span class="s0">va=</span><span class="s4">'bottom'</span><span class="s0">)</span>
<a name="l2567"><span class="ln">2567 </span></a><span class="s2">else</span><span class="s0">:</span>
<a name="l2568"><span class="ln">2568 </span></a>    <span class="s0">ax5.text(</span><span class="s6">0.5</span><span class="s5">, </span><span class="s6">0.5</span><span class="s5">, </span><span class="s4">'Performance</span><span class="s7">\n</span><span class="s4">Evaluation</span><span class="s7">\n</span><span class="s4">Required'</span><span class="s5">,</span>
<a name="l2569"><span class="ln">2569 </span></a>             <span class="s0">ha=</span><span class="s4">'center'</span><span class="s5">, </span><span class="s0">va=</span><span class="s4">'center'</span><span class="s5">, </span><span class="s0">transform=ax5.transAxes</span><span class="s5">,</span>
<a name="l2570"><span class="ln">2570 </span></a>             <span class="s0">fontsize=</span><span class="s6">12</span><span class="s5">, </span><span class="s0">style=</span><span class="s4">'italic'</span><span class="s0">)</span>
<a name="l2571"><span class="ln">2571 </span></a>    <span class="s0">ax5.set_title(</span><span class="s4">'Success Rate Comparison'</span><span class="s0">)</span>
<a name="l2572"><span class="ln">2572 </span></a>
<a name="l2573"><span class="ln">2573 </span></a><span class="s3"># 6. Fuel efficiency comparison (middle right)</span>
<a name="l2574"><span class="ln">2574 </span></a><span class="s0">ax6 = fig.add_subplot(gs[</span><span class="s6">1</span><span class="s5">, </span><span class="s6">2</span><span class="s0">])</span>
<a name="l2575"><span class="ln">2575 </span></a><span class="s2">if </span><span class="s4">'dqn_performance' </span><span class="s2">in </span><span class="s0">locals() </span><span class="s2">and </span><span class="s4">'ppo_performance' </span><span class="s2">in </span><span class="s0">locals():</span>
<a name="l2576"><span class="ln">2576 </span></a>    <span class="s0">fuel_usage = [</span>
<a name="l2577"><span class="ln">2577 </span></a>        <span class="s0">dqn_performance[</span><span class="s4">'avg_fuel'</span><span class="s0">]</span><span class="s5">,</span>
<a name="l2578"><span class="ln">2578 </span></a>        <span class="s0">ppo_performance[</span><span class="s4">'avg_fuel'</span><span class="s0">]</span>
<a name="l2579"><span class="ln">2579 </span></a>    <span class="s0">]</span>
<a name="l2580"><span class="ln">2580 </span></a>    <span class="s0">bars = ax6.bar(algorithms</span><span class="s5">, </span><span class="s0">fuel_usage</span><span class="s5">, </span><span class="s0">color=[</span><span class="s4">'blue'</span><span class="s5">, </span><span class="s4">'orange'</span><span class="s0">])</span>
<a name="l2581"><span class="ln">2581 </span></a>    <span class="s0">ax6.set_title(</span><span class="s4">'Average Fuel Usage'</span><span class="s0">)</span>
<a name="l2582"><span class="ln">2582 </span></a>    <span class="s0">ax6.set_ylabel(</span><span class="s4">'Fuel Units'</span><span class="s0">)</span>
<a name="l2583"><span class="ln">2583 </span></a>    <span class="s3"># Add value labels on bars</span>
<a name="l2584"><span class="ln">2584 </span></a>    <span class="s2">for </span><span class="s0">bar</span><span class="s5">, </span><span class="s0">fuel </span><span class="s2">in </span><span class="s0">zip(bars</span><span class="s5">, </span><span class="s0">fuel_usage):</span>
<a name="l2585"><span class="ln">2585 </span></a>        <span class="s0">ax6.text(bar.get_x() + bar.get_width() / </span><span class="s6">2</span><span class="s5">, </span><span class="s0">bar.get_height() + </span><span class="s6">0.5</span><span class="s5">,</span>
<a name="l2586"><span class="ln">2586 </span></a>                 <span class="s4">f'</span><span class="s7">{</span><span class="s0">fuel</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">'</span><span class="s5">, </span><span class="s0">ha=</span><span class="s4">'center'</span><span class="s5">, </span><span class="s0">va=</span><span class="s4">'bottom'</span><span class="s0">)</span>
<a name="l2587"><span class="ln">2587 </span></a><span class="s2">else</span><span class="s0">:</span>
<a name="l2588"><span class="ln">2588 </span></a>    <span class="s0">ax6.text(</span><span class="s6">0.5</span><span class="s5">, </span><span class="s6">0.5</span><span class="s5">, </span><span class="s4">'Fuel Efficiency</span><span class="s7">\n</span><span class="s4">Data</span><span class="s7">\n</span><span class="s4">Required'</span><span class="s5">,</span>
<a name="l2589"><span class="ln">2589 </span></a>             <span class="s0">ha=</span><span class="s4">'center'</span><span class="s5">, </span><span class="s0">va=</span><span class="s4">'center'</span><span class="s5">, </span><span class="s0">transform=ax6.transAxes</span><span class="s5">,</span>
<a name="l2590"><span class="ln">2590 </span></a>             <span class="s0">fontsize=</span><span class="s6">12</span><span class="s5">, </span><span class="s0">style=</span><span class="s4">'italic'</span><span class="s0">)</span>
<a name="l2591"><span class="ln">2591 </span></a>    <span class="s0">ax6.set_title(</span><span class="s4">'Average Fuel Usage'</span><span class="s0">)</span>
<a name="l2592"><span class="ln">2592 </span></a>
<a name="l2593"><span class="ln">2593 </span></a><span class="s3"># 7. Reward distribution comparison (bottom left)</span>
<a name="l2594"><span class="ln">2594 </span></a><span class="s0">ax7 = fig.add_subplot(gs[</span><span class="s6">2</span><span class="s5">, </span><span class="s6">0</span><span class="s0">])</span>
<a name="l2595"><span class="ln">2595 </span></a><span class="s2">if </span><span class="s4">'dqn_session_results' </span><span class="s2">in </span><span class="s0">locals() </span><span class="s2">and </span><span class="s4">'ppo_session_results' </span><span class="s2">in </span><span class="s0">locals():</span>
<a name="l2596"><span class="ln">2596 </span></a>    <span class="s0">dqn_rewards = [res[</span><span class="s4">'avg_reward'</span><span class="s0">] </span><span class="s2">for </span><span class="s0">res </span><span class="s2">in </span><span class="s0">dqn_session_results]</span>
<a name="l2597"><span class="ln">2597 </span></a>    <span class="s0">ppo_rewards = [res[</span><span class="s4">'avg_reward'</span><span class="s0">] </span><span class="s2">for </span><span class="s0">res </span><span class="s2">in </span><span class="s0">ppo_session_results]</span>
<a name="l2598"><span class="ln">2598 </span></a>    <span class="s0">ax7.hist(dqn_rewards</span><span class="s5">, </span><span class="s0">alpha=</span><span class="s6">0.7</span><span class="s5">, </span><span class="s0">label=</span><span class="s4">'DQN'</span><span class="s5">, </span><span class="s0">bins=</span><span class="s6">10</span><span class="s5">, </span><span class="s0">color=</span><span class="s4">'blue'</span><span class="s0">)</span>
<a name="l2599"><span class="ln">2599 </span></a>    <span class="s0">ax7.hist(ppo_rewards</span><span class="s5">, </span><span class="s0">alpha=</span><span class="s6">0.7</span><span class="s5">, </span><span class="s0">label=</span><span class="s4">'PPO'</span><span class="s5">, </span><span class="s0">bins=</span><span class="s6">10</span><span class="s5">, </span><span class="s0">color=</span><span class="s4">'orange'</span><span class="s0">)</span>
<a name="l2600"><span class="ln">2600 </span></a>    <span class="s0">ax7.set_title(</span><span class="s4">'Reward Distribution'</span><span class="s0">)</span>
<a name="l2601"><span class="ln">2601 </span></a>    <span class="s0">ax7.set_xlabel(</span><span class="s4">'Average Reward'</span><span class="s0">)</span>
<a name="l2602"><span class="ln">2602 </span></a>    <span class="s0">ax7.set_ylabel(</span><span class="s4">'Frequency'</span><span class="s0">)</span>
<a name="l2603"><span class="ln">2603 </span></a>    <span class="s0">ax7.legend()</span>
<a name="l2604"><span class="ln">2604 </span></a><span class="s2">else</span><span class="s0">:</span>
<a name="l2605"><span class="ln">2605 </span></a>    <span class="s0">ax7.text(</span><span class="s6">0.5</span><span class="s5">, </span><span class="s6">0.5</span><span class="s5">, </span><span class="s4">'Reward</span><span class="s7">\n</span><span class="s4">Distribution</span><span class="s7">\n</span><span class="s4">Data Required'</span><span class="s5">,</span>
<a name="l2606"><span class="ln">2606 </span></a>             <span class="s0">ha=</span><span class="s4">'center'</span><span class="s5">, </span><span class="s0">va=</span><span class="s4">'center'</span><span class="s5">, </span><span class="s0">transform=ax7.transAxes</span><span class="s5">,</span>
<a name="l2607"><span class="ln">2607 </span></a>             <span class="s0">fontsize=</span><span class="s6">12</span><span class="s5">, </span><span class="s0">style=</span><span class="s4">'italic'</span><span class="s0">)</span>
<a name="l2608"><span class="ln">2608 </span></a>    <span class="s0">ax7.set_title(</span><span class="s4">'Reward Distribution'</span><span class="s0">)</span>
<a name="l2609"><span class="ln">2609 </span></a>
<a name="l2610"><span class="ln">2610 </span></a><span class="s3"># 8. Episode length comparison (bottom middle)</span>
<a name="l2611"><span class="ln">2611 </span></a><span class="s0">ax8 = fig.add_subplot(gs[</span><span class="s6">2</span><span class="s5">, </span><span class="s6">1</span><span class="s0">])</span>
<a name="l2612"><span class="ln">2612 </span></a><span class="s2">if </span><span class="s4">'dqn_performance' </span><span class="s2">in </span><span class="s0">locals() </span><span class="s2">and </span><span class="s4">'ppo_performance' </span><span class="s2">in </span><span class="s0">locals():</span>
<a name="l2613"><span class="ln">2613 </span></a>    <span class="s0">episode_lengths = [</span>
<a name="l2614"><span class="ln">2614 </span></a>        <span class="s6">200</span><span class="s5">,  </span><span class="s3"># Approximate for DQN</span>
<a name="l2615"><span class="ln">2615 </span></a>        <span class="s6">200  </span><span class="s3"># Approximate for PPO</span>
<a name="l2616"><span class="ln">2616 </span></a>    <span class="s0">]</span>
<a name="l2617"><span class="ln">2617 </span></a>    <span class="s0">bars = ax8.bar(algorithms</span><span class="s5">, </span><span class="s0">episode_lengths</span><span class="s5">, </span><span class="s0">color=[</span><span class="s4">'blue'</span><span class="s5">, </span><span class="s4">'orange'</span><span class="s0">])</span>
<a name="l2618"><span class="ln">2618 </span></a>    <span class="s0">ax8.set_title(</span><span class="s4">'Average Episode Length'</span><span class="s0">)</span>
<a name="l2619"><span class="ln">2619 </span></a>    <span class="s0">ax8.set_ylabel(</span><span class="s4">'Steps per Episode'</span><span class="s0">)</span>
<a name="l2620"><span class="ln">2620 </span></a>    <span class="s3"># Add value labels</span>
<a name="l2621"><span class="ln">2621 </span></a>    <span class="s2">for </span><span class="s0">bar</span><span class="s5">, </span><span class="s0">length </span><span class="s2">in </span><span class="s0">zip(bars</span><span class="s5">, </span><span class="s0">episode_lengths):</span>
<a name="l2622"><span class="ln">2622 </span></a>        <span class="s0">ax8.text(bar.get_x() + bar.get_width() / </span><span class="s6">2</span><span class="s5">, </span><span class="s0">bar.get_height() + </span><span class="s6">5</span><span class="s5">,</span>
<a name="l2623"><span class="ln">2623 </span></a>                 <span class="s4">f'</span><span class="s7">{</span><span class="s0">length</span><span class="s7">:</span><span class="s4">.0f</span><span class="s7">}</span><span class="s4">'</span><span class="s5">, </span><span class="s0">ha=</span><span class="s4">'center'</span><span class="s5">, </span><span class="s0">va=</span><span class="s4">'bottom'</span><span class="s0">)</span>
<a name="l2624"><span class="ln">2624 </span></a><span class="s2">else</span><span class="s0">:</span>
<a name="l2625"><span class="ln">2625 </span></a>    <span class="s0">ax8.text(</span><span class="s6">0.5</span><span class="s5">, </span><span class="s6">0.5</span><span class="s5">, </span><span class="s4">'Episode Length</span><span class="s7">\n</span><span class="s4">Data Required'</span><span class="s5">,</span>
<a name="l2626"><span class="ln">2626 </span></a>             <span class="s0">ha=</span><span class="s4">'center'</span><span class="s5">, </span><span class="s0">va=</span><span class="s4">'center'</span><span class="s5">, </span><span class="s0">transform=ax8.transAxes</span><span class="s5">,</span>
<a name="l2627"><span class="ln">2627 </span></a>             <span class="s0">fontsize=</span><span class="s6">12</span><span class="s5">, </span><span class="s0">style=</span><span class="s4">'italic'</span><span class="s0">)</span>
<a name="l2628"><span class="ln">2628 </span></a>    <span class="s0">ax8.set_title(</span><span class="s4">'Average Episode Length'</span><span class="s0">)</span>
<a name="l2629"><span class="ln">2629 </span></a>
<a name="l2630"><span class="ln">2630 </span></a><span class="s3"># 9. Final performance summary (bottom right)</span>
<a name="l2631"><span class="ln">2631 </span></a><span class="s0">ax9 = fig.add_subplot(gs[</span><span class="s6">2</span><span class="s5">, </span><span class="s6">2</span><span class="s0">])</span>
<a name="l2632"><span class="ln">2632 </span></a><span class="s0">ax9.axis(</span><span class="s4">'off'</span><span class="s0">)</span>
<a name="l2633"><span class="ln">2633 </span></a>
<a name="l2634"><span class="ln">2634 </span></a><span class="s3"># Create summary text</span>
<a name="l2635"><span class="ln">2635 </span></a><span class="s0">summary_text = </span><span class="s4">&quot;PERFORMANCE SUMMARY</span><span class="s7">\n\n</span><span class="s4">&quot;</span>
<a name="l2636"><span class="ln">2636 </span></a><span class="s2">if </span><span class="s4">'dqn_performance' </span><span class="s2">in </span><span class="s0">locals():</span>
<a name="l2637"><span class="ln">2637 </span></a>    <span class="s0">summary_text += </span><span class="s4">f&quot;DQN Results:</span><span class="s7">\n</span><span class="s4">&quot;</span>
<a name="l2638"><span class="ln">2638 </span></a>    <span class="s0">summary_text += </span><span class="s4">f&quot;• Success Rate: </span><span class="s7">{</span><span class="s0">dqn_performance[</span><span class="s4">'success_rate'</span><span class="s0">]</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">%</span><span class="s7">\n</span><span class="s4">&quot;</span>
<a name="l2639"><span class="ln">2639 </span></a>    <span class="s0">summary_text += </span><span class="s4">f&quot;• Avg Reward: </span><span class="s7">{</span><span class="s0">dqn_performance[</span><span class="s4">'avg_reward'</span><span class="s0">]</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}\n</span><span class="s4">&quot;</span>
<a name="l2640"><span class="ln">2640 </span></a>    <span class="s0">summary_text += </span><span class="s4">f&quot;• Avg Fuel: </span><span class="s7">{</span><span class="s0">dqn_performance[</span><span class="s4">'avg_fuel'</span><span class="s0">]</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}\n\n</span><span class="s4">&quot;</span>
<a name="l2641"><span class="ln">2641 </span></a>
<a name="l2642"><span class="ln">2642 </span></a><span class="s2">if </span><span class="s4">'ppo_performance' </span><span class="s2">in </span><span class="s0">locals():</span>
<a name="l2643"><span class="ln">2643 </span></a>    <span class="s0">summary_text += </span><span class="s4">f&quot;PPO Results:</span><span class="s7">\n</span><span class="s4">&quot;</span>
<a name="l2644"><span class="ln">2644 </span></a>    <span class="s0">summary_text += </span><span class="s4">f&quot;• Success Rate: </span><span class="s7">{</span><span class="s0">ppo_performance[</span><span class="s4">'success_rate'</span><span class="s0">]</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">%</span><span class="s7">\n</span><span class="s4">&quot;</span>
<a name="l2645"><span class="ln">2645 </span></a>    <span class="s0">summary_text += </span><span class="s4">f&quot;• Avg Reward: </span><span class="s7">{</span><span class="s0">ppo_performance[</span><span class="s4">'avg_reward'</span><span class="s0">]</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}\n</span><span class="s4">&quot;</span>
<a name="l2646"><span class="ln">2646 </span></a>    <span class="s0">summary_text += </span><span class="s4">f&quot;• Avg Fuel: </span><span class="s7">{</span><span class="s0">ppo_performance[</span><span class="s4">'avg_fuel'</span><span class="s0">]</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}\n\n</span><span class="s4">&quot;</span>
<a name="l2647"><span class="ln">2647 </span></a>
<a name="l2648"><span class="ln">2648 </span></a><span class="s2">if </span><span class="s4">'rewards' </span><span class="s2">in </span><span class="s0">locals() </span><span class="s2">or </span><span class="s4">'rewards' </span><span class="s2">in </span><span class="s0">globals():</span>
<a name="l2649"><span class="ln">2649 </span></a>    <span class="s0">summary_text += </span><span class="s4">f&quot;Training Episodes:</span><span class="s7">\n</span><span class="s4">&quot;</span>
<a name="l2650"><span class="ln">2650 </span></a>    <span class="s0">summary_text += </span><span class="s4">f&quot;• DQN: </span><span class="s7">{</span><span class="s0">len(rewards)</span><span class="s7">} </span><span class="s4">episodes</span><span class="s7">\n</span><span class="s4">&quot;</span>
<a name="l2651"><span class="ln">2651 </span></a>    <span class="s0">summary_text += </span><span class="s4">f&quot;• Final DQN Reward: </span><span class="s7">{</span><span class="s0">np.mean(rewards[-</span><span class="s6">100</span><span class="s0">:])</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">&quot;</span>
<a name="l2652"><span class="ln">2652 </span></a><span class="s2">else</span><span class="s0">:</span>
<a name="l2653"><span class="ln">2653 </span></a>    <span class="s0">summary_text += </span><span class="s4">f&quot;Training Data:</span><span class="s7">\n</span><span class="s4">&quot;</span>
<a name="l2654"><span class="ln">2654 </span></a>    <span class="s0">summary_text += </span><span class="s4">f&quot;• DQN: Training completed</span><span class="s7">\n</span><span class="s4">&quot;</span>
<a name="l2655"><span class="ln">2655 </span></a>    <span class="s0">summary_text += </span><span class="s4">f&quot;• Models: Saved and loaded&quot;</span>
<a name="l2656"><span class="ln">2656 </span></a>
<a name="l2657"><span class="ln">2657 </span></a><span class="s0">ax9.text(</span><span class="s6">0.1</span><span class="s5">, </span><span class="s6">0.9</span><span class="s5">, </span><span class="s0">summary_text</span><span class="s5">, </span><span class="s0">transform=ax9.transAxes</span><span class="s5">, </span><span class="s0">fontsize=</span><span class="s6">10</span><span class="s5">,</span>
<a name="l2658"><span class="ln">2658 </span></a>         <span class="s0">verticalalignment=</span><span class="s4">'top'</span><span class="s5">, </span><span class="s0">fontfamily=</span><span class="s4">'monospace'</span><span class="s5">,</span>
<a name="l2659"><span class="ln">2659 </span></a>         <span class="s0">bbox=dict(boxstyle=</span><span class="s4">'round'</span><span class="s5">, </span><span class="s0">facecolor=</span><span class="s4">'lightgray'</span><span class="s5">, </span><span class="s0">alpha=</span><span class="s6">0.8</span><span class="s0">))</span>
<a name="l2660"><span class="ln">2660 </span></a>
<a name="l2661"><span class="ln">2661 </span></a><span class="s0">plt.suptitle(</span><span class="s4">'LunarLander-v3 Comprehensive Analysis'</span><span class="s5">, </span><span class="s0">fontsize=</span><span class="s6">16</span><span class="s5">, </span><span class="s0">fontweight=</span><span class="s4">'bold'</span><span class="s0">)</span>
<a name="l2662"><span class="ln">2662 </span></a><span class="s0">plt.show()</span>
<a name="l2663"><span class="ln">2663 </span></a>
<a name="l2664"><span class="ln">2664 </span></a><span class="s0">print(</span><span class="s4">&quot;</span><span class="s7">\n</span><span class="s4">&quot; </span><span class="s0">+ </span><span class="s4">&quot;=&quot; </span><span class="s0">* </span><span class="s6">60</span><span class="s0">)</span>
<a name="l2665"><span class="ln">2665 </span></a><span class="s0">print(</span><span class="s4">&quot;FINAL PERFORMANCE SUMMARY&quot;</span><span class="s0">)</span>
<a name="l2666"><span class="ln">2666 </span></a><span class="s0">print(</span><span class="s4">&quot;=&quot; </span><span class="s0">* </span><span class="s6">60</span><span class="s0">)</span>
<a name="l2667"><span class="ln">2667 </span></a><span class="s2">if </span><span class="s4">'dqn_performance' </span><span class="s2">in </span><span class="s0">locals() </span><span class="s2">and </span><span class="s4">'ppo_performance' </span><span class="s2">in </span><span class="s0">locals():</span>
<a name="l2668"><span class="ln">2668 </span></a>    <span class="s0">summary_data = {</span>
<a name="l2669"><span class="ln">2669 </span></a>        <span class="s4">'Metric'</span><span class="s0">: [</span><span class="s4">'Algorithm Type'</span><span class="s5">, </span><span class="s4">'Success Rate (%)'</span><span class="s5">, </span><span class="s4">'Avg Reward'</span><span class="s5">, </span><span class="s4">'Avg Fuel Usage'</span><span class="s0">]</span><span class="s5">,</span>
<a name="l2670"><span class="ln">2670 </span></a>        <span class="s4">'DQN'</span><span class="s0">: [</span>
<a name="l2671"><span class="ln">2671 </span></a>            <span class="s4">'Value-based (Off-policy)'</span><span class="s5">,</span>
<a name="l2672"><span class="ln">2672 </span></a>            <span class="s4">f&quot;</span><span class="s7">{</span><span class="s0">dqn_performance[</span><span class="s4">'success_rate'</span><span class="s0">]</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s5">,</span>
<a name="l2673"><span class="ln">2673 </span></a>            <span class="s4">f&quot;</span><span class="s7">{</span><span class="s0">dqn_performance[</span><span class="s4">'avg_reward'</span><span class="s0">]</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s5">,</span>
<a name="l2674"><span class="ln">2674 </span></a>            <span class="s4">f&quot;</span><span class="s7">{</span><span class="s0">dqn_performance[</span><span class="s4">'avg_fuel'</span><span class="s0">]</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">&quot;</span>
<a name="l2675"><span class="ln">2675 </span></a>        <span class="s0">]</span><span class="s5">,</span>
<a name="l2676"><span class="ln">2676 </span></a>        <span class="s4">'PPO'</span><span class="s0">: [</span>
<a name="l2677"><span class="ln">2677 </span></a>            <span class="s4">'Policy-based (On-policy)'</span><span class="s5">,</span>
<a name="l2678"><span class="ln">2678 </span></a>            <span class="s4">f&quot;</span><span class="s7">{</span><span class="s0">ppo_performance[</span><span class="s4">'success_rate'</span><span class="s0">]</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s5">,</span>
<a name="l2679"><span class="ln">2679 </span></a>            <span class="s4">f&quot;</span><span class="s7">{</span><span class="s0">ppo_performance[</span><span class="s4">'avg_reward'</span><span class="s0">]</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">&quot;</span><span class="s5">,</span>
<a name="l2680"><span class="ln">2680 </span></a>            <span class="s4">f&quot;</span><span class="s7">{</span><span class="s0">ppo_performance[</span><span class="s4">'avg_fuel'</span><span class="s0">]</span><span class="s7">:</span><span class="s4">.1f</span><span class="s7">}</span><span class="s4">&quot;</span>
<a name="l2681"><span class="ln">2681 </span></a>        <span class="s0">]</span>
<a name="l2682"><span class="ln">2682 </span></a>    <span class="s0">}</span>
<a name="l2683"><span class="ln">2683 </span></a>    <span class="s0">summary_df = pd.DataFrame(summary_data)</span>
<a name="l2684"><span class="ln">2684 </span></a>    <span class="s0">print(summary_df.to_string(index=</span><span class="s2">False</span><span class="s0">))</span>
<a name="l2685"><span class="ln">2685 </span></a><span class="s2">else</span><span class="s0">:</span>
<a name="l2686"><span class="ln">2686 </span></a>    <span class="s0">print(</span><span class="s4">&quot;Performance data available - run evaluation sections first&quot;</span><span class="s0">)</span>
<a name="l2687"><span class="ln">2687 </span></a><span class="s0">print(</span><span class="s4">&quot;=&quot; </span><span class="s0">* </span><span class="s6">60</span><span class="s0">)</span><hr class="ls0"><a name="l2688"><span class="ln">2688 </span></a><span class="s0">#%% md 
<a name="l2689"><span class="ln">2689 </span></a># 11. Conclusion and Final Analysis 
<a name="l2690"><span class="ln">2690 </span></a> 
<a name="l2691"><span class="ln">2691 </span></a>## 11.1 Project Overview and Achievements 
<a name="l2692"><span class="ln">2692 </span></a> 
<a name="l2693"><span class="ln">2693 </span></a>This (pretty) comprehensive study successfully implemented and compared two fundamentally different reinforcement learning approaches for the LunarLander-v3 control task: Double Deep Q-Network (DDQN) with N-step returns and Proximal Policy Optimization (PPO) with vectorized environments. Both algorithms exceeded the environment's 200-point &quot;solved&quot; threshold, demonstrating the viability of deep reinforcement learning for autonomous spacecraft control systems. 
<a name="l2694"><span class="ln">2694 </span></a> 
<a name="l2695"><span class="ln">2695 </span></a>The project encompassed extensive experimental work including hyperparameter analysis, custom reward function implementation, multi-session statistical validation, and comprehensive performance evaluation across multiple metrics. The modular codebase architecture enabled systematic comparison of value-based versus policy-based learning paradigms in a controlled experimental setting. 
<a name="l2696"><span class="ln">2696 </span></a> 
<a name="l2697"><span class="ln">2697 </span></a>## 11.2 Key Implementation Decisions and Technical Contributions 
<a name="l2698"><span class="ln">2698 </span></a> 
<a name="l2699"><span class="ln">2699 </span></a>### DDQN v4 Architecture and Innovations 
<a name="l2700"><span class="ln">2700 </span></a> 
<a name="l2701"><span class="ln">2701 </span></a>**Network Design**: The final DDQN implementation utilized a progressive architecture (256 -&gt; 128 -&gt; 64) with layer normalization for training stability and conservative Xavier initialization (gain=0.1) to prevent gradient instabilities that plagued earlier versions. 
<a name="l2702"><span class="ln">2702 </span></a> 
<a name="l2703"><span class="ln">2703 </span></a>**N-Step Returns Enhancement**: The integration of 3-step returns significantly improved temporal credit assignment, enabling faster propagation of landing rewards through multi-step sequences while maintaining computational efficiency compared to the computationally prohibitive Prioritized Experience Replay (PER) approach. 
<a name="l2704"><span class="ln">2704 </span></a> 
<a name="l2705"><span class="ln">2705 </span></a>**Training Stabilization**: A comprehensive stabilization framework including reward normalization, gradient clipping (max norm 1.0), and a strategic warmup phase (300 episodes of forced exploration) ensured robust convergence without the catastrophic gradient explosions experienced in preliminary implementations. 
<a name="l2706"><span class="ln">2706 </span></a> 
<a name="l2707"><span class="ln">2707 </span></a>### PPO Implementation and Optimization 
<a name="l2708"><span class="ln">2708 </span></a> 
<a name="l2709"><span class="ln">2709 </span></a>**Vectorized Architecture**: The use of 4 parallel environments with Stable-Baselines3's MlpPolicy enabled efficient sample collection and reduced gradient variance, demonstrating the effectiveness of distributed training for policy-based methods. 
<a name="l2710"><span class="ln">2710 </span></a> 
<a name="l2711"><span class="ln">2711 </span></a>**Conservative Hyperparameters**: The selection of clip_range=0.2, learning_rate=3e-4, and GAE_lambda=0.95 reflected a deliberate emphasis on training stability over aggressive optimization, resulting in consistent performance across evaluation sessions. 
<a name="l2712"><span class="ln">2712 </span></a> 
<a name="l2713"><span class="ln">2713 </span></a>## 11.3 Performance Analysis and Statistical Validation 
<a name="l2714"><span class="ln">2714 </span></a> 
<a name="l2715"><span class="ln">2715 </span></a>### Multi-Session Statistical Results 
<a name="l2716"><span class="ln">2716 </span></a> 
<a name="l2717"><span class="ln">2717 </span></a>The rigorous 10-session evaluation protocol (1,000 episodes per algorithm) revealed significant performance differences: 
<a name="l2718"><span class="ln">2718 </span></a> 
<a name="l2719"><span class="ln">2719 </span></a>**Success Rate Performance**: PPO achieved superior reliability with 92.0% ± 2.4% success rate compared to DDQN's 79.9% ± 4.6%, representing a 15.2% relative improvement in landing consistency. PPO's lower variance (2.4% vs 4.6%) indicates more predictable deployment performance. 
<a name="l2720"><span class="ln">2720 </span></a> 
<a name="l2721"><span class="ln">2721 </span></a>**Fuel Efficiency Paradigm**: DDQN demonstrated exceptional resource efficiency, consuming only 104.47 ± 3.12 fuel units compared to PPO's 210.23 ± 1.02 units—a remarkable 50.3% reduction. This efficiency advantage stems from DDQN's value-based learning approach that inherently optimizes the fuel-reward trade-off. 
<a name="l2722"><span class="ln">2722 </span></a> 
<a name="l2723"><span class="ln">2723 </span></a>**Precision and Consistency**: PPO achieved marginally better landing precision (0.947 vs 0.901) and demonstrated superior consistency across all metrics, with standard deviations approximately 50% lower than DDQN across reward and success rate measurements. 
<a name="l2724"><span class="ln">2724 </span></a> 
<a name="l2725"><span class="ln">2725 </span></a>### Reward Structure Analysis 
<a name="l2726"><span class="ln">2726 </span></a> 
<a name="l2727"><span class="ln">2727 </span></a>Despite lower success rates, DDQN achieved marginally higher average rewards (245.54 vs 242.33), indicating that successful DDQN landings generate higher individual episode rewards, likely due to substantial fuel efficiency bonuses. This paradox highlights the complex relationship between task completion and reward optimization in reinforcement learning. 
<a name="l2728"><span class="ln">2728 </span></a> 
<a name="l2729"><span class="ln">2729 </span></a>## 11.4 Algorithmic Behavioral Differences and Trade-offs 
<a name="l2730"><span class="ln">2730 </span></a> 
<a name="l2731"><span class="ln">2731 </span></a>### DDQN's Conservative Value-Based Strategy 
<a name="l2732"><span class="ln">2732 </span></a> 
<a name="l2733"><span class="ln">2733 </span></a>DDQN's 50% fuel efficiency advantage suggests the emergence of a conservative, ballistic-trajectory approach that minimizes engine usage while accepting lower success rates. This behavior pattern aligns with value-based learning's natural tendency to optimize long-term cumulative rewards, resulting in risk-averse policies that prioritize resource conservation. 
<a name="l2734"><span class="ln">2734 </span></a> 
<a name="l2735"><span class="ln">2735 </span></a>**Exploration-Exploitation Balance**: The extended warmup phase (300 episodes) and gradual epsilon decay (0.998) enabled comprehensive state space exploration before transitioning to exploitation, resulting in well-informed but cautious policies. 
<a name="l2736"><span class="ln">2736 </span></a> 
<a name="l2737"><span class="ln">2737 </span></a>**N-Step Learning Benefits**: The 3-step lookahead mechanism accelerated learning of landing sequences while avoiding the computational overhead of more complex temporal credit assignment methods. 
<a name="l2738"><span class="ln">2738 </span></a> 
<a name="l2739"><span class="ln">2739 </span></a>### PPO's Aggressive Policy-Based Approach 
<a name="l2740"><span class="ln">2740 </span></a> 
<a name="l2741"><span class="ln">2741 </span></a>PPO's higher fuel consumption paired with 92% success rate indicates a more aggressive control strategy that prioritizes landing success over resource conservation. This reflects policy-based learning's direct optimization of action selection for task completion rather than value maximization. 
<a name="l2742"><span class="ln">2742 </span></a> 
<a name="l2743"><span class="ln">2743 </span></a>**Stochastic Policy Advantages**: PPO's inherent stochasticity enabled more robust exploration of the action space, leading to discovery of highly successful but resource-intensive control strategies. 
<a name="l2744"><span class="ln">2744 </span></a> 
<a name="l2745"><span class="ln">2745 </span></a>**Vectorized Training Benefits**: The 4-environment parallel collection mechanism provided 4x sample efficiency per timestep, enabling rapid policy refinement through diverse experience accumulation. 
<a name="l2746"><span class="ln">2746 </span></a> 
<a name="l2747"><span class="ln">2747 </span></a>## 11.5 Practical Deployment Implications 
<a name="l2748"><span class="ln">2748 </span></a> 
<a name="l2749"><span class="ln">2749 </span></a>### Mission-Specific Algorithm Selection 
<a name="l2750"><span class="ln">2750 </span></a> 
<a name="l2751"><span class="ln">2751 </span></a>**Fuel-Critical Applications**: DDQN's exceptional resource efficiency makes it ideal for scenarios where fuel conservation is paramount, such as deep space missions or multi-landing sequences where each drop of propellant matters. 
<a name="l2752"><span class="ln">2752 </span></a> 
<a name="l2753"><span class="ln">2753 </span></a>**Reliability-Critical Applications**: PPO's 92% success rate and low variance make it preferable for high-stakes missions where landing failure would be catastrophic, such as crewed missions or expensive scientific payloads. 
<a name="l2754"><span class="ln">2754 </span></a> 
<a name="l2755"><span class="ln">2755 </span></a>**Computational Constraints**: DDQN's simpler architecture and lower computational requirements during inference make it suitable for embedded systems with limited processing power. 
<a name="l2756"><span class="ln">2756 </span></a> 
<a name="l2757"><span class="ln">2757 </span></a>## 11.6 Technical Lessons and Methodological Insights 
<a name="l2758"><span class="ln">2758 </span></a> 
<a name="l2759"><span class="ln">2759 </span></a>### Training Stability Discoveries 
<a name="l2760"><span class="ln">2760 </span></a> 
<a name="l2761"><span class="ln">2761 </span></a>The project revealed critical details during the implementation process: 
<a name="l2762"><span class="ln">2762 </span></a> 
<a name="l2763"><span class="ln">2763 </span></a>**Tensor Creation Optimization**: The discovery that `torch.FloatTensor([list_of_numpy_arrays])` caused catastrophic training slowdowns (90 minutes vs normal speeds) highlights the importance of efficient tensor operations in practical implementations. 
<a name="l2764"><span class="ln">2764 </span></a> 
<a name="l2765"><span class="ln">2765 </span></a>**Gradient Explosion Prevention**: The necessity of conservative initialization and gradient clipping was demonstrated through the failure of alternative approaches, providing concrete evidence for these stabilization techniques. 
<a name="l2766"><span class="ln">2766 </span></a> 
<a name="l2767"><span class="ln">2767 </span></a>**Hyperparameter Sensitivity**: The systematic learning rate and discount factor experiments confirmed that DDQN performance is highly sensitive to these parameters, with lr=0.0001 and γ=0.99 emerging as optimal through empirical validation. 
<a name="l2768"><span class="ln">2768 </span></a> 
<a name="l2769"><span class="ln">2769 </span></a>### Experimental Design Insights 
<a name="l2770"><span class="ln">2770 </span></a> 
<a name="l2771"><span class="ln">2771 </span></a>**Multi-Session Validation**: Single-session evaluations can be misleading; the 10-session protocol revealed important variance characteristics that inform deployment reliability estimates. 
<a name="l2772"><span class="ln">2772 </span></a> 
<a name="l2773"><span class="ln">2773 </span></a>**Video Generation Value**: The visual analysis capability proved invaluable for qualitative policy assessment, enabling identification of behavioral patterns not captured by numerical metrics. 
<a name="l2774"><span class="ln">2774 </span></a> 
<a name="l2775"><span class="ln">2775 </span></a>**Custom Reward Function Impact**: The fuel efficiency and precision bonus experiments demonstrated that reward shaping can significantly influence learned behaviors, providing a tool for biasing algorithms toward specific objectives. 
<a name="l2776"><span class="ln">2776 </span></a> 
<a name="l2777"><span class="ln">2777 </span></a>## 11.7 Limitations and Future Directions 
<a name="l2778"><span class="ln">2778 </span></a> 
<a name="l2779"><span class="ln">2779 </span></a>### Current Limitations 
<a name="l2780"><span class="ln">2780 </span></a> 
<a name="l2781"><span class="ln">2781 </span></a>**Environment Specificity**: Results are specific to the LunarLander-v3 discrete action space; continuous control variants may favor different algorithmic approaches. 
<a name="l2782"><span class="ln">2782 </span></a> 
<a name="l2783"><span class="ln">2783 </span></a>**Computational Resources**: Training required substantial GPU resources; deployment on resource-constrained spacecraft computers would require model compression techniques. 
<a name="l2784"><span class="ln">2784 </span></a> 
<a name="l2785"><span class="ln">2785 </span></a>**Environmental Variability**: The standardized environment lacks the unpredictability of real-world lunar surface conditions, wind effects, and hardware variations. 
<a name="l2786"><span class="ln">2786 </span></a> 
<a name="l2787"><span class="ln">2787 </span></a>### Future Work 
<a name="l2788"><span class="ln">2788 </span></a> 
<a name="l2789"><span class="ln">2789 </span></a>**Hybrid Approaches**: Investigate combining DDQN's fuel efficiency with PPO's reliability through ensemble methods or hierarchical control architectures. 
<a name="l2790"><span class="ln">2790 </span></a> 
<a name="l2791"><span class="ln">2791 </span></a>**Domain Randomization**: Implement environmental variations (gravity fluctuations, sensor noise, actuator delays) to improve real-world transferability. 
<a name="l2792"><span class="ln">2792 </span></a> 
<a name="l2793"><span class="ln">2793 </span></a>**Model Compression**: Develop quantization and pruning techniques to enable deployment on spacecraft computing systems. 
<a name="l2794"><span class="ln">2794 </span></a> 
<a name="l2795"><span class="ln">2795 </span></a>**Continuous Control**: Extend the comparison to continuous action spaces using algorithms like SAC (Soft Actor-Critic) or TD3 (Twin Delayed DDPG). 
<a name="l2796"><span class="ln">2796 </span></a> 
<a name="l2797"><span class="ln">2797 </span></a>## 11.8 Final Assessment and Contributions 
<a name="l2798"><span class="ln">2798 </span></a> 
<a name="l2799"><span class="ln">2799 </span></a>This project makes several important contributions to the reinforcement learning community: 
<a name="l2800"><span class="ln">2800 </span></a> 
<a name="l2801"><span class="ln">2801 </span></a>1. **Comprehensive Algorithm Comparison**: Provides rigorous statistical comparison of value-based vs policy-based approaches with practical deployment considerations. 
<a name="l2802"><span class="ln">2802 </span></a>2. **Implementation Best Practices**: Documents critical implementation details and optimization techniques often omitted from academic papers. 
<a name="l2803"><span class="ln">2803 </span></a>3. **Multi-Metric Evaluation Framework**: Establishes a comprehensive evaluation protocol incorporating success rate, fuel efficiency, precision, and statistical robustness. 
<a name="l2804"><span class="ln">2804 </span></a>4. **Practical Decision Framework**: Offers concrete guidance for algorithm selection based on mission requirements and operational constraints. 
<a name="l2805"><span class="ln">2805 </span></a> 
<a name="l2806"><span class="ln">2806 </span></a>The project successfully demonstrates that both DDQN and PPO can solve the lunar landing problem effectively, but with fundamentally different approaches and trade-offs. DDQN excels in resource-constrained scenarios requiring fuel efficiency, while PPO dominates when mission success rates are paramount. This comprehensive analysis provides valuable insights for practitioners implementing reinforcement learning solutions in aerospace applications and similar control domains. 
<a name="l2807"><span class="ln">2807 </span></a> 
<a name="l2808"><span class="ln">2808 </span></a>*For real-world lunar landing applications, a dual-algorithm approach leveraging DDQN for nominal operations (fuel efficiency) and PPO for contingency scenarios (maximum reliability) would provide optimal operational flexibility across diverse mission requirements. Maybe with a bit of luck too, it's kinda necessary those days.* 
<a name="l2809"><span class="ln">2809 </span></a> 
<a name="l2810"><span class="ln">2810 </span></a>### Thank you for being patient with my learning process throughout this project! 
<a name="l2811"><span class="ln">2811 </span></a>### Working with DDQN and PPO has taught me so much about reinforcement learning, and I know there's still plenty more for me to learn. 
<a name="l2812"><span class="ln">2812 </span></a>### I appreciate your time and any feedback you might have 🤗 <hr class="ls0"><a name="l2813"><span class="ln">2813 </span></a>#%% md 
<a name="l2814"><span class="ln">2814 </span></a>this cell left intentionally empty</span></pre>
</body>
</html>